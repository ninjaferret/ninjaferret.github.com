---
layout: default
title: A/B Testing Service Implementations
summary: So you have a service-oriented architecture of lightweight, tested components; your architecture is complete and there's no reason to worry, right? How did you build the services? Did you try different approaches or just the first one that came to mind? Is that the best implementation? Is it the most scalable? Is it the most understandable? What happens when those who wrote it have gone?
categories:
- Architecture
- Change
- Disruptive Change
---
<h3>TL;DR</h3>

<p>Our code start our new, clean, fresh but then the world changes beneath them. The people who wrote them move on, the technologies/platforms they are based on them change and advance, but often the code stays the same, our code ages and rots. By building small, well-tested, services and committing to periodically re-writing them you can mitigate risk of stagnation, spread the domain knowledge across your developers, learn about new technologies and you have then a pre-defined migration strategy to a new platform.</p>


<h3>What is wrong with what we are doing now?</h3>

<p><strong>What is wrong with what we are doing now?</strong> Nothing... perhaps a bit of arrogance that the first implementation we tend to write is a readable, maintainable and even efficient and will never become <em>legacy</em></p>
<p><strong>We have tests so that can't be legacy code, can it?</strong> Tests make code safer but they don't prevent it from being legacy. Readability, technology stack, the domain knowledge of your developers are all key too.</p>
<p><strong>How can we change?</strong> Build the ability to run two parallel implementations of same service, A/B test them, compare them, choose a winner. Get into the practice of regularly rebuilding your services.</p>
<p><strong>Isn't that costly?</strong> Tell that to companies with code-bases in COBOL still, how much more does it cost them to hire developers and maintain the code? How much would a re-write of their monoliths cost now? Perhaps a process of continually refreshing the code-base in a safe way would reduce that cost and the risk.</p>


<h3>Risky business</h3>

<p>
For most software projects we can identify a certain number of risks that could undermine the success of the project, e.g.
</p>
<ul>
	<li>People - what happens if we lose Hannah? she's the only person who knows the X service</li>
	<li>Infrastructure - what happens if the our cloud host goes down? or our own server goes down? What about our developer machines?</li>
	<li>Technology - are we on unsupported, old or even dead frameworks? Languages?</li>
	<li>Code - do people still understand the code? how quickly can you make a change? </li>
</ul>
<p>
We are gradually evolving a number of techniques for dealing with the people, requiring documentation, requiring knowledge transfer, we pair program or perform code reviews to help with these processes. Techniques like <em>Domain Driven Design</em> or specification by example are there to help the communication of the intent of the code over the specific technology. Rarely would we have one person who has sole responsibility for a huge area of domain knowledge (and we wouldn't be comfortable if we do), how would the company cope while that person is on holiday?
</p>
<p>
When it comes to infrastructure, companies often replace hardware on a regular schedule as we know that hardware wears out so many companies put into place procedures to handle these eventualities. With server/cloud infrastructure companies will tend to go for larger providers with more redundancy and even geo-located servers, many companies also have disaster recovery sites which can take over in the event of an emergency. 
</p>
<p>
What about code and infrastructure, I mean the actually coding of our domain logic, our websites etc. we have unit testing, code reviews, pairing, design patterns and a whole arsenal of tools to help us at our disposal to make the code clean and maintainable; though I have often found that the even with all those things the domain gets lost, the less disciplined teams (of which there are plenty) end up producing unintelligible tests and code so the code is still scary, it is still <em>legacy</em>. All of this is without the problem that we depend on specific frameworks that may be popular now but may become obsolete, what if we want to change server arhitecture (e.g. moving from Windows to Linux)? There are so many questions that we don't really answer until we need to but I believe that there are practices we can evolve to help us manage change at these levels.
</p>



<h3>Services</h3>

<p>
In my post <a href="/2013/01/03/Isolating-yourself-from-change.html">Isolating yourself from change</a> I talked about how a service-oriented architecture with well-defined, tested, technology-agnostic interfaces can isolate you from the risks involved in writing software. <em>Microservices</em> are becoming popular but as a term there are still too many definitions about what constitutes a <em>microservice</em> and what size they should be. 
</p>

<p>
I am not going to use the term <em>microservice</em> to describe what I mean, instead I want to put forward a few simple ideas for what I think a service should be:
</p>

<ul>
	<li>Represent a single <strong>Business Capability</strong> (or <strong>Bounded Context</strong> in <em>Domain Driven Design</em> terms) - this keeps the interface "sufficiently small" based on the domain. This is almost like the <em>Single Responsibility Principle</em> for services and their interfaces.</li>
	<li>Have a technology <strong>agnostic</strong> interface i.e. can be implemented in a number of languages on a number of platforms. This is the equivalent of the <em>Liskov Substitution Principle</em></li>
	<li>Be sufficiently <strong>tested</strong> for the behaviour of the interface running directly against the interface</li>
</ul>

<p>
As my grandad used to say <em>good fences make good neighbours</em> and I think that the same is true for services. A well-defined boundary with good points of entry make good neighbours out of our services, only communicate through these interfaces (no dipping your hands into someone else's databases etc) and you have the isolation that we so crave. Once a domain is correctly separated into an isolated service its neighbours can change without impacting them, unless there is an interface change.
</p>


<h3>Early decisions have a big impact</h3>

<h4>How often do we really prototype?</h4>
<p>
Imagine you are given a new greenfield project, with no legacy and no constraints; feels good, doesn't it? So, what's the first step?
</p>
<ul>
	<li>
		<em>Start coding the real thing!</em> Well, that's what a lot of people do. You know a technology or two, you have an idea what stack you're going to use and just code. Yet, before you know the domain, before you really understand what you are developing you are committing to an infrastructure, this is something that could come to haunt you much later. One example being the Twitter and Rails where Rails helped them to rapidly grow but eventually the ActiveRecord patterns that gave them the speed became a hinderance. 
	</li>
	<li>
		<em>Make a prototype!</em> OK, you admit you don't know everything so you just get something up and running quickly in a stack you know, but you're going to throw this code away, aren't you? This is where reality comes crashing down on a lot of prototypes, the business need this live NOW! So your "prototype" ends up in production, but you'll get the chance to re-write it fairly quickly, won't you? More features keep taking priority over a re-write and effectively you're in the same situation as if you'd started coding it live anyway.
	</li>
	<li>
		<em>Multiple teams each making their own prototype!</em> Wow, I think you're in some kind of programmer heaven if you have the time and funds to manage this. The ability to create and compare multiple prototypes, in multiple stacks, really provides value as you can triangulate features and really understand the domain and then properly compare the implementations rather than just guessing which one will prove the best at the start. You're probably not afraid of throwing away code either and don't see the extra man hours as wasted effort but as a learning opportunity.
	</li>
</ul>
<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ninja Ferret</title>
  <link href="http://ninjaferret.github.com/feed.atom" rel="self"/>
  <link href="http://ninjaferret.github.com/"/>
  <updated>2015-05-07T18:13:37+01:00</updated>
  <id>http://ninjaferret.github.com</id>
  <author>
    <name>Ian Johnson</name>
    <email>ij2030+blog@gmail.com</email>
  </author>
  
    <entry>
      <title>A/B Testing Service Implementations</title>
      <link href="http://ninjaferret.github.com/2015/04/23/A-B-Test-Your-Implementation.html"/>
      <updated>2015-04-23T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2015/04/23/A-B-Test-Your-Implementation</id>
      <content type="html">&lt;h3&gt;TL;DR&lt;/h3&gt;

&lt;p&gt;Our code start our new, clean, fresh but then the world changes beneath them. The people who wrote them move on, the technologies/platforms they are based on them change and advance, but often the code stays the same, our code ages and rots. By building small, well-tested, services and committing to periodically re-writing them you can mitigate risk of stagnation, spread the domain knowledge across your developers, learn about new technologies and you have then a pre-defined migration strategy to a new platform.&lt;/p&gt;


&lt;h3&gt;What is wrong with what we are doing now?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;What is wrong with what we are doing now?&lt;/strong&gt; Nothing... perhaps a bit of arrogance that the first implementation we tend to write is a readable, maintainable and even efficient and will never become &lt;em&gt;legacy&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We have tests so that can&#39;t be legacy code, can it?&lt;/strong&gt; Tests make code safer but they don&#39;t prevent it from being legacy. Readability, technology stack, the domain knowledge of your developers are all key too.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How can we change?&lt;/strong&gt; Build the ability to run two parallel implementations of same service, A/B test them, compare them, choose a winner. Get into the practice of regularly rebuilding your services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isn&#39;t that costly?&lt;/strong&gt; Tell that to companies with code-bases in COBOL still, how much more does it cost them to hire developers and maintain the code? How much would a re-write of their monoliths cost now? Perhaps a process of continually refreshing the code-base in a safe way would reduce that cost and the risk.&lt;/p&gt;


&lt;h3&gt;Risky business&lt;/h3&gt;

&lt;p&gt;
For most software projects we can identify a certain number of risks that could undermine the success of the project, e.g.
&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;People - what happens if we lose Hannah? she&#39;s the only person who knows the X service&lt;/li&gt;
	&lt;li&gt;Infrastructure - what happens if the our cloud host goes down? or our own server goes down? What about our developer machines?&lt;/li&gt;
	&lt;li&gt;Technology - are we on unsupported, old or even dead frameworks? Languages?&lt;/li&gt;
	&lt;li&gt;Code - do people still understand the code? how quickly can you make a change? &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
We are gradually evolving a number of techniques for dealing with the people, requiring documentation, requiring knowledge transfer, we pair program or perform code reviews to help with these processes. Techniques like &lt;em&gt;Domain Driven Design&lt;/em&gt; or specification by example are there to help the communication of the intent of the code over the specific technology. Rarely would we have one person who has sole responsibility for a huge area of domain knowledge (and we wouldn&#39;t be comfortable if we do), how would the company cope while that person is on holiday?
&lt;/p&gt;
&lt;p&gt;
When it comes to infrastructure, companies often replace hardware on a regular schedule as we know that hardware wears out so many companies put into place procedures to handle these eventualities. With server/cloud infrastructure companies will tend to go for larger providers with more redundancy and even geo-located servers, many companies also have disaster recovery sites which can take over in the event of an emergency. 
&lt;/p&gt;
&lt;p&gt;
What about code and infrastructure, I mean the actually coding of our domain logic, our websites etc. we have unit testing, code reviews, pairing, design patterns and a whole arsenal of tools to help us at our disposal to make the code clean and maintainable; though I have often found that the even with all those things the domain gets lost, the less disciplined teams (of which there are plenty) end up producing unintelligible tests and code so the code is still scary, it is still &lt;em&gt;legacy&lt;/em&gt;. All of this is without the problem that we depend on specific frameworks that may be popular now but may become obsolete, what if we want to change server arhitecture (e.g. moving from Windows to Linux)? There are so many questions that we don&#39;t really answer until we need to but I believe that there are practices we can evolve to help us manage change at these levels.
&lt;/p&gt;



&lt;h3&gt;Services&lt;/h3&gt;

&lt;p&gt;
In my post &lt;a href=&quot;/2013/01/03/Isolating-yourself-from-change.html&quot;&gt;Isolating yourself from change&lt;/a&gt; I talked about how a service-oriented architecture with well-defined, tested, technology-agnostic interfaces can isolate you from the risks involved in writing software. &lt;em&gt;Microservices&lt;/em&gt; are becoming popular but as a term there are still too many definitions about what constitutes a &lt;em&gt;microservice&lt;/em&gt; and what size they should be. 
&lt;/p&gt;

&lt;p&gt;
I am not going to use the term &lt;em&gt;microservice&lt;/em&gt; to describe what I mean, instead I want to put forward a few simple ideas for what I think a service should be:
&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Represent a single &lt;strong&gt;Business Capability&lt;/strong&gt; (or &lt;strong&gt;Bounded Context&lt;/strong&gt; in &lt;em&gt;Domain Driven Design&lt;/em&gt; terms) - this keeps the interface &quot;sufficiently small&quot; based on the domain. This is almost like the &lt;em&gt;Single Responsibility Principle&lt;/em&gt; for services and their interfaces.&lt;/li&gt;
	&lt;li&gt;Have a technology &lt;strong&gt;agnostic&lt;/strong&gt; interface i.e. can be implemented in a number of languages on a number of platforms. This is the equivalent of the &lt;em&gt;Liskov Substitution Principle&lt;/em&gt;&lt;/li&gt;
	&lt;li&gt;Be sufficiently &lt;strong&gt;tested&lt;/strong&gt; for the behaviour of the interface running directly against the interface&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
As my grandad used to say &lt;em&gt;good fences make good neighbours&lt;/em&gt; and I think that the same is true for services. A well-defined boundary with good points of entry make good neighbours out of our services, only communicate through these interfaces (no dipping your hands into someone else&#39;s databases etc) and you have the isolation that we so crave. Once a domain is correctly separated into an isolated service its neighbours can change without impacting them, unless there is an interface change.
&lt;/p&gt;


&lt;h3&gt;Early decisions have a big impact&lt;/h3&gt;

&lt;h4&gt;How often do we really prototype?&lt;/h4&gt;
&lt;p&gt;
Imagine you are given a new greenfield project, with no legacy and no constraints; feels good, doesn&#39;t it? So, what&#39;s the first step?
&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;
		&lt;em&gt;Start coding the real thing!&lt;/em&gt; Well, that&#39;s what a lot of people do. You know a technology or two, you have an idea what stack you&#39;re going to use and just code. Yet, before you know the domain, before you really understand what you are developing you are committing to an infrastructure, this is something that could come to haunt you much later. One example being the Twitter and Rails where Rails helped them to rapidly grow but eventually the ActiveRecord patterns that gave them the speed became a hinderance. 
	&lt;/li&gt;
	&lt;li&gt;
		&lt;em&gt;Make a prototype!&lt;/em&gt; OK, you admit you don&#39;t know everything so you just get something up and running quickly in a stack you know, but you&#39;re going to throw this code away, aren&#39;t you? This is where reality comes crashing down on a lot of prototypes, the business need this live NOW! So your &quot;prototype&quot; ends up in production, but you&#39;ll get the chance to re-write it fairly quickly, won&#39;t you? More features keep taking priority over a re-write and effectively you&#39;re in the same situation as if you&#39;d started coding it live anyway.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;em&gt;Multiple teams each making their own prototype!&lt;/em&gt; Wow, I think you&#39;re in some kind of programmer heaven if you have the time and funds to manage this. The ability to create and compare multiple prototypes, in multiple stacks, really provides value as you can triangulate features and really understand the domain and then properly compare the implementations rather than just guessing which one will prove the best at the start. You&#39;re probably not afraid of throwing away code either and don&#39;t see the extra man hours as wasted effort but as a learning opportunity.
	&lt;/li&gt;
&lt;/ul&gt;</content>
    </entry>
  
    <entry>
      <title>Isolating yourself from change</title>
      <link href="http://ninjaferret.github.com/2013/01/03/Isolating-yourself-from-change.html"/>
      <updated>2013-01-03T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2013/01/03/Isolating-yourself-from-change</id>
      <content type="html">&lt;p&gt;
	One of the guys at the round table discussion proposed the question (can&#39;t remember exactly) &lt;em&gt;What technology should I 	choose to ensure that my system doesn&#39;t have to change for the next 10 years?&lt;/em&gt; Our initial response was to ask a bit	more about his application, it was a line of business application that was currently running on the windows desktop	that did not have to process high loads and did not have a high volume of users. 
&lt;/p&gt;
&lt;p&gt;
	Our discussions went from trying to answer the original question to delving deeper into why he needed the change. We discussed what advice we would give for choosing a technology since &quot;Microsoft is dead&quot; (according to some) but then we were looking more towards designing and architecting your system to cope with change better.
&lt;/p&gt;

&lt;h3&gt;
	Choosing the right technology (1)
&lt;/h3&gt;

&lt;p&gt;
	It depends! Well that was easy enough, wasn&#39;t it, there aren&#39;t many questions around technology that can&#39;t be answered by that. Well, to be a little bit more specific then it depends on the nature of the solution that you are planning. For	example, if you are still focussed on building desktop applications on windows then you are going to have to consider .NET alongside Java or maybe native, if you are going for the new Windows Store applications then you have the choice of C++, .NET, HTML and CSS. If you are going web-based then you can choose from Java, Ruby, Scala, .NET, PHP, Python and many more than I cannot possibly name here. 
&lt;/p&gt;
&lt;p&gt;
	Does the application that you are writing need to work on multiple different environmnets? Does it need to be distributed	so that multiple people are accessing the same system at the same time from different environments? Then we are probably	talking either web stack or a cross platform tool.
&lt;/p&gt;
&lt;p&gt;
	Are we talking moving into mobile applications? Then actually the .NET runtime may be possibility as thanks to the Mono for Android	and MonoTouch allows us to write most of the logic once and then just create a new UI per platform. 
&lt;/p&gt;
&lt;p&gt;
	So which tech? If you are wanting stability for the next ten years that I do not think that you would want to go bleeding	edge, there is a lot of risk that this technology will not take off and get the adoption required, it is likely to change	a lot during the early days and case you some serious problems. However, conversely, you do not want to be using something	that is dying, though the technology is well understood it will get increasingly hard to find people who know the	technology to maintain your product. We are aiming therefore for something fairly popular with a large enough base of 	developers to ensure that in the next ten years there will be enough people who understand it, also there is a sufficient 	community who have already encountered and solved many of your problems. 
&lt;/p&gt;
&lt;p&gt;
	&lt;em&gt;What about coolness?&lt;/em&gt; Developers like the shiny new languages and frameworks, we like to see the new cool things	and as technologies die we are less tempted to take a job that uses these technologies. There is the concept of the  &lt;a href=&quot;http://en.wikipedia.org/wiki/Hype_cycle&quot;&gt;Gartner Hype Cycle&lt;/a&gt; which shows how technologies are adopted and  their level of maturity. I think we should be looking for a technology on the &lt;em&gt;slope of productivity&lt;/em&gt;. It is still fairly new and cool but people are being more realistic about what the technology can do. 
&lt;/p&gt;
&lt;p&gt;
	Another suggestion about choosing the right technology was to look at the &lt;a href=&quot;http://www.thoughtworks.com/radar&quot;&gt;Thoughtworks Tech Radar&lt;/a&gt;. This is an opinion blog from Thoughtworks but it shows their current thinking about which technologies should be adopted and trialed but it is a fair representation of what is happening in the world. 
&lt;/p&gt;
&lt;h3&gt;
	Change happens 
&lt;/h3&gt;
&lt;p&gt;
	To quote Robert Burns&#39; poem &lt;em&gt;To a Mouse&lt;/em&gt;:
&lt;/p&gt;
&lt;blockquote&gt;
	...&lt;br&gt;
	But little Mouse, you are not alone,&lt;br&gt;
	In proving foresight may be vain:&lt;br&gt;
	The best laid schemes of mice and men&lt;br&gt;
	Go often awry,&lt;br&gt;
	And leave us nothing but grief and pain,&lt;br&gt;
	For promised joy!&lt;br&gt;
	...
&lt;/blockquote&gt;
&lt;p&gt;
	We think, we plan, we forge ahead with our plans because they are our best guess as to what will be useful to us in the future but we do not have the gift of foresight, we cannot guarantee that our choices will be the right ones. Life changes around use all of the time.
&lt;/p&gt;
&lt;p&gt;
	If you believe in &lt;em&gt;Agile Development&lt;/em&gt; then we are told that change will happen. The focus tends to be on change as a consequence of the business changing their minds, by the world changing and the business needing to change what they require from the system. However, we rarely think of this as the platform, language and frameworks that we are using changing from underneath us. How can we isolate ourselves from these changes?
&lt;/p&gt;
&lt;h3&gt;
	Architecting for change
&lt;/h3&gt;
&lt;p&gt;
	A typical situation that we could find ourselves in is that our entire system is built as one massive project, with highly coupled components and has very brief encounters with other languages. This means that any change to the platform is going to cost us severely. We have to change everything. So what can we do?
&lt;/p&gt;
&lt;h4&gt;
	1. Build a system of loosely coupled components
&lt;/h4&gt;
&lt;p&gt;
	We need to think of our system as a set of loosely coupled components that interact through well defined interfaces. This allows any change to the platform that may impact only a single component to be isolated within that component. It does not mean that we are safe from the change but it reduces the cost of fixing that component.
&lt;/p&gt;
&lt;h4&gt;
	2. Define clear APIs
&lt;/h4&gt;
&lt;p&gt;
	If we are to achieve loose coupling then we are going to have to create the interfaces that I spoke of above. These interfaces form an API around the component, they are the only ways that anyone else can interact with the component. Each API should have a clear design and each call should have a clear purpose. Our APIs should not really expose the inner workings of the component, they should be an abstraction layer (a &lt;em&gt;anti-corruption layer&lt;/em&gt; in &lt;em&gt;Doman Driven Design&lt;/em&gt; terms) on top of our component&#39;s logic. 
&lt;/p&gt;
&lt;p&gt;
	These APIs could be in the form of a RESTful or SOAP based web service if we are developing a distributed application, or a framework library that only publicly exposes the API to the rest of the world. It does not matter how we do it but the client should only depend on the API (and maybe even then build it&#39;s own abstraction layer around the API).
&lt;/p&gt;
&lt;p&gt;
	One way to ensure that people do not &quot;do the wrong thing&quot; and dig into the internal structure of the component is to develop the component in isolation from everythying else. The component is developed and published in an independent work space/solution to the consuming components.
&lt;/p&gt;
&lt;h4&gt;
	3. Test your APIs
&lt;/h4&gt;
&lt;p&gt;
	A key principle that is being widely adopted is that we should test our systems. A lot of developers are now seeing the advantage of unit testing and people in the &lt;em&gt;Agile Testing&lt;/em&gt; world have been pushing &lt;em&gt;Specification by Example&lt;/em&gt;, &lt;em&gt;Executable Specifications&lt;/em&gt; and &lt;em&gt;Behaviour Driven Development&lt;/em&gt; to focus is more on testing our systems to ensure that they meet the functional demands. If we have an API, we should test it thoroughly to ensure that the behaviour, that way we already have the tests should we need to change a component.
&lt;/p&gt;
&lt;p&gt;
	By testing, I do mean that we need to test at the unit level but also more importantly testing the APIs in a &lt;em&gt;live like&lt;/em&gt; environment, it is no use just testing the code, we need to be able to test the actual implentation, exactly what the clients would talk to.
&lt;/p&gt;
&lt;p&gt;
	As an example, assume we have a RESTful API for managing a shopping basket. We write an automated test suite for the API that runs against a web server, let&#39;s say for the moment that we developed it in C# and it was running on IIS. If we wanted to move to a Java service then the tests are already there, we can re-implement it safely because we know the behaviour it is supposed to implement. We implement the API in Java and then we switch from the original implementation to the new one. We do so safe in the knowledge that the consumer does not know anything about the underlying implementation and that the new implementation mirrors exactly the original yet we are now free from the original platform.
&lt;/p&gt;
&lt;h4&gt;
	4. Version your APIs
&lt;/h4&gt;
&lt;p&gt;
	Versioning the APIs is a key part of being able to loosely couple your components. If you do not version these components the systems become highly coupled through the API when it comes to deployment time. You cannot deploy either the client or the component without the client breaking. With a versioned API you publish the server first and then the clients can make changes as time goes by. Usually the older versions are adapters that change the older message formats into the current versions and the newer results into the older version.
&lt;/p&gt;
&lt;p&gt;
	Once again, this does not isolate us from change directly but again reduces the impact of any one specific change. 
&lt;/p&gt; 
&lt;h4&gt;
	5. Automate
&lt;/h4&gt;
&lt;p&gt;
	I mentioned testing the APIs in a &quot;realistic environment&quot; above. Yet, if we are left to do this manually then we will ignore it when we are busy or we will leave it for weeks and do a lot of testing in one go. This not only leaves us open to &quot;human&quot; errors and mistakes passing incorrect functionality, or simply not having time and not testing everything but it also increases our feedback time. If I am updating an API and I break an earlier version, then I need to know as soon as possible so I am more able to find the breaking change. 
&lt;/p&gt;
&lt;p&gt;
	Each component should be able to be deployed automatically, each component should be deployed quickly to a test environment that is similar to the real one and it should be able to run the suite of tests that we have against it. If we have been conscientious and written the tests (or at least at the time of writing) so we know we have a comprehensive suite of tests a nightly build will let me know if I have broken anything.
&lt;/p&gt;
&lt;p&gt;
	Althought this does not directly isolate us from change it enables us to automatically verify that our systems work, and even integrate with each other. If we are wanting to develop and test a new implementation of a component then we should learn how to deploy it, we front-load the risk of learning how to deploy a new technology, we grow our understanding before we are there, on the &quot;go-live&quot; date panicking that we have not worked out how to correctly deploy our application, we have already automated everything.
&lt;/p&gt;

&lt;h3&gt;
	Choosing the right technology (2)
&lt;/h3&gt;
&lt;p&gt;
	Given what I have said above I would now ask you the question...
&lt;/p&gt;
&lt;blockquote&gt;Is there a single &quot;right technology&quot; for your application?&lt;/blockquote&gt;
&lt;p&gt;
	I think my answer would probably be &lt;em&gt;no&lt;/em&gt;. There are several possible technologies that could implement each component and I have to then say that we have to look at a &lt;em&gt;good&lt;/em&gt; technology in which to implement each component. We do not have to have a single technology stack any more.
&lt;/p&gt;
&lt;p&gt;
	We are moving towards having a polyglot of languages, frameworks and paradigms in our systems but each one suited to the individual component(s) that it supports. Does this mean that it will be harder to find developers? Perhaps, but good developers can understand multiple languages and may even be more interested in working in a variety of languages.
&lt;/p&gt;
&lt;p&gt;
	Loosely coupled, well tested components, give us the freedom the choose a good technology for each component. It allows us to migrate a component without significantly disturbing any of its clients with reduced effort. Should a platform that several of our components depend upon change it is rarely a sudden and immediate change, we have time to move to a new platform but with this system we can change one component at a time, learning about our new platform as we go.
&lt;/p&gt;
&lt;h3&gt;
	Final thoughts
&lt;/h3&gt;
&lt;p&gt;
	These are my thoughts about what we discussed in the meeting but I think they portray what we discussed but probably in a little more detail concerning the techniques. I think the discussion was interesting it cemented my views around designing a system around a set of APIs. This gives us the time to play, to try out new languages, new platforms, new frameworks and minimise the risk to our system, it is a technique we can use for prototyping with new technologies and evolving our system as technology moves on.
&lt;/p&gt;
</content>
    </entry>
  
    <entry>
      <title>DDD eXchange - Restful Objects - Dan Haywood</title>
      <link href="http://ninjaferret.github.com/2012/06/25/DDDX-restful-objects.html"/>
      <updated>2012-06-25T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2012/06/25/DDDX-restful-objects</id>
      <content type="html">&lt;p&gt;
	The second talk at &lt;a href=&quot;http://skillsmatter.com&quot;&gt;Skillsmatter&#39;s&lt;/a&gt; DDD Exchange was Dan Haywood 
	&lt;a href=&quot;http://skillsmatter.com/podcast/design-architecture/restful-objects&quot;&gt;talking about RESTful objects&lt;/a&gt; and
	there was a lot to consider in this talk.
&lt;/p&gt;

&lt;h3&gt;
	A Brief Guide to RESTful Objects
&lt;/h3&gt;

&lt;p&gt;
	The &lt;a href=&quot;http://restfulobjects.org/&quot;&gt;RESTful Objects&lt;/a&gt; specification comes out of the &lt;a href=&quot;http://www.nakedobjects.org/&quot;&gt;Naked Objects&lt;/a&gt; movement. It is a specification that
	allows you to create a domain model and expose it through a RESTful API using the &lt;em&gt;Hypermedia As The Engine Of Application State&lt;/em&gt; or &lt;em&gt;HATEOAS&lt;/em&gt; 
	for short.
&lt;/p&gt;

&lt;p&gt;
	As you would expect for the &lt;em&gt;REST&lt;/em&gt; based system the domain models are exposed as resources and 
	client applications interact with these through their &lt;em&gt;URI&lt;/em&gt;, the standard &lt;em&gt;HTTP Verbs&lt;/em&gt; and also
	the use of both standard and custom &lt;em&gt;HTTP Headers&lt;/em&gt;.
&lt;/p&gt;

&lt;p&gt;
	It is possible to expose:
&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;The &lt;em&gt;Domain Object&lt;/em&gt; as a resource at the &lt;em&gt;URI&lt;/em&gt;&lt;/li&gt;
	&lt;li&gt;The properties of  the &lt;em&gt;Domain Object&lt;/em&gt; as their own child resource&lt;/li&gt;
	&lt;li&gt;The actions/commands that can be performed on the &lt;em&gt;Domain Object&lt;/em&gt; as their own child resource&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
	For each of the &lt;em&gt;Resource&lt;/em&gt;, which can be any of the above, the specification allows you to define how the API should interact
	with that resource:
&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;
		Just to retrieve the &lt;em&gt;Resource&lt;/em&gt; you can perform a &lt;strong&gt;GET&lt;/strong&gt; this will be an idempotent operation that
		will not change the state of the object
	&lt;/li&gt;
	&lt;li&gt;
		To change the state of a &lt;em&gt;Resource&lt;/em&gt; you can perform a &lt;strong&gt;PUT&lt;/strong&gt; and pass through the new representation
		of that &lt;em&gt;Resource&lt;/em&gt;. Again, this is an idempotent operations as you can put the same data as many times as you like and it
		will retain the same details.
	&lt;/li&gt;
	&lt;li&gt;
		To delete the &lt;em&gt;Resource&lt;/em&gt; you can perform a &lt;strong&gt;DELETE&lt;/strong&gt;.
	&lt;/li&gt;
	&lt;li&gt;
		If we have a collection as a &lt;em&gt;Resource&lt;/em&gt; and you want to add something to that collection then it is a standard approach is 
		to &lt;strong&gt;POST&lt;/strong&gt; the new item to the root of the collection and you will be given back the address at which you can find
		the new &lt;em&gt;Resource&lt;/em&gt; as a &lt;strong&gt;201 Created&lt;/strong&gt;. This is not an idempotent operation as repeated &lt;strong&gt;POSTS&lt;/strong&gt;
		may generate multiple new objects.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
	There is currently both a Java and .NET implementation of the &lt;em&gt;RESTful Objects&lt;/em&gt; specification.
&lt;/p&gt;

&lt;h3&gt;
	Why do I have a problem with it?
&lt;/h3&gt;

&lt;p&gt;
	I have many concerns about using frameworks like this to expose my domain model to the outside world, at the moment I find 
	developers tend to rush towards a framework that &quot;solves&quot; their problems without first understanding the nature of the problem
	they are trying to solve or understand the constraints that the framework will place upon them in the future. 
&lt;/p&gt;

&lt;h4&gt;
	1. My Domain Model is Private
&lt;/h4&gt;

&lt;p&gt;
	The &lt;em&gt;Domain Model&lt;/em&gt; is the value of the functionality that we are providing, it reflects the ubiquitous language that we use to communicate
	internally about the domain and I do not feel that we should be exposing that for the outside world to consume. In &lt;em&gt;Domain Driven Design&lt;/em&gt;
	Eric Evans talks about the &lt;em&gt;Bounded Contexts&lt;/em&gt; and also the techniques that we use to ensure that the domain is protected from the 
	outside world, like the use of &lt;em&gt;Anti-Corruption Layers&lt;/em&gt;. I think that we need to design our &lt;em&gt;APIs&lt;/em&gt; as the 
	&lt;em&gt;Anti-corruption layer&lt;/em&gt; for our domain and not build our &lt;em&gt;API&lt;/em&gt; out of the domain. 
&lt;/p&gt;

&lt;h4&gt;
	2. My Domain Model Changes
&lt;/h4&gt;

&lt;p&gt;
	As we work on a project, as we discuss the &lt;em&gt;Domain&lt;/em&gt;, we are constantly getting a deeper understanding of the &lt;em&gt;Domain&lt;/em&gt;
	and we are changing our &lt;em&gt;Ubiquitous Language&lt;/em&gt; (and hence the &lt;em&gt;Domain Model&lt;/em&gt; to reflect those changes). However,
	when we have exposed our &lt;em&gt;Domain Model&lt;/em&gt; to the outside world we have built in &lt;strong&gt;rigidity&lt;/strong&gt; into our systems, if
	we change the nature of the domain too much we can no longer expose the same objects over the old &lt;em&gt;API&lt;/em&gt; and the existing clients break.
&lt;/p&gt;

&lt;h4&gt;
	3. Domain Modelling != Service Design
&lt;/h4&gt;

&lt;p&gt;
	I think that it is important to make the distinction between &lt;em&gt;Domain Driven Design&lt;/em&gt; and the &lt;em&gt;Service Design/Architecture&lt;/em&gt;
	because the two are not exactly the same thing. They are related concepts, without knowing what operations you need to expose on your domain
	you cannot know how to build your service, or if you don&#39;t understand what operations you need to expose to the clients then you don&#39;t know 
	what impact this will have on the domain. However, I think that the domain, in many cases, is richer than the nature of the services that you
	expose to the world, there may be internal features that you need to have access to in the background that you do not wish to expose over services
	and you want to keep your service definitions isolated from the domain changes. 
&lt;/p&gt;

&lt;p&gt;
	I think that when looking at service design we are looking for what we need to expose to the clients and how we are going to expose it, what
	language we use is important as often the deep understanding that we have of our domain has led to a more abstract &lt;em&gt;Ubiquitous
	Language&lt;/em&gt; and the concepts will make no sense. 
&lt;/p&gt;

&lt;h4&gt;
	4. Exposing Properties...
&lt;/h4&gt;

&lt;p&gt;
	Part of the specification allows the properties of the domain to be exposed to the client including the ability to change the values. This
	does not sit well for me in a &lt;em&gt;DDD&lt;/em&gt; context as we are constantly talking about the design that is reflecting the actions that
	people take, it feels like there is an element of just exposing a POCO or POJO object as an RPC endpoint rather than exposing something
	the actions and the intentions of the client. If we have this freedom, it will be misused.
&lt;/p&gt;

&lt;h3&gt;
	Conclusion
&lt;/h3&gt;

&lt;p&gt;
	I think that the specification is very thorough and has a number of points that I need to dig into in much more detail about how they are
	structuring their URLs for actions and commands but I think that there are some fundamental flaws in using this approach to marry together
	the buzzwords of &lt;em&gt;REST&lt;/em&gt; and &lt;em&gt;DDD&lt;/em&gt;. 
&lt;/p&gt;

&lt;p&gt;
	This specification may be useful to us in the case where you control both the client and the server, and you can deploy everything in
	one big go (but this is creating a rigid structure). Paraphrasing what Jeff Bezos told people at Amazon &quot;you will write APIs as
	if they are going to be made public&quot; because he knew that if they did that they would be able to make them public if there was a 
	commercial need for them and look how successful that has been.
&lt;/p&gt;
</content>
    </entry>
  
    <entry>
      <title>DDD eXchange - Functional DDD - Greg Young</title>
      <link href="http://ninjaferret.github.com/2012/06/24/DDDX-functional-programming-with-ddd.html"/>
      <updated>2012-06-24T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2012/06/24/DDDX-functional-programming-with-ddd</id>
      <content type="html">&lt;p&gt;
	Greg Young was the first speaker at &lt;a href=&quot;http://skillsmatter.com&quot;&gt;Skillsmatter&#39;s&lt;/a&gt; DDD eXchange and was talking to 
	us about how functional programming and &lt;em&gt;Domain Driven Design&lt;/em&gt; can work together peacefully.
&lt;/p&gt;
&lt;p&gt;
	The origins of &lt;em&gt;DDD&lt;/em&gt; lay firmly in the Object Oriented world. The &lt;em&gt;Domain Models&lt;/em&gt; in our code are linked
	directly to the &lt;em&gt;Ubiquitous Language&lt;/em&gt;, the code and the language we use to describe the domain are as one. Therefore
	in the object world the objects are the nouns and the methods are the verbs. So how does this work in a functional context where there
	are only functions? I think that there are still a few issues that Greg has not fully answered in this but I&#39;m hoping to play with the ideas
	in the future to really understand my own doubts
&lt;/p&gt;
&lt;h3&gt;
	Refactoring the Domain
&lt;/h3&gt;
&lt;p&gt;
	So, assuming that we have an domain around eCommerce we may well have the concept of a &lt;strong&gt;InventoryItem&lt;/strong&gt;
	which is something you can sell. Assume someone wants to deactivate the item:
&lt;/p&gt;
&lt;code&gt;
	&lt;pre class=&quot;brush: csharp&quot;&gt;
	public class InventoryItem
	{
		...
		public void Deactivate(string reason)
		{
			if (!_active) throw new AlreadyDeactivatedException(_id);
			_active = false;
		}
		...
	}
	&lt;/pre&gt;
&lt;/code&gt;
&lt;p&gt;
	At the moment that this is not even &lt;em&gt;event sourced&lt;/em&gt;, but we also want to think about splitting apart the logic that decides whether
	the state change can occur from the actual state change.
&lt;/p&gt;
&lt;code&gt;
	&lt;pre class=&quot;brush: csharp&quot;&gt;
	public class InventoryItem
	{
		...
		public void Deactivate(string reason)
		{
			if (!_active) throw new AlreadyDeactivatedException(_id);
			DoDeactivate(reason);
		}
		
		private void DoDeactivate(string reason)
		{
			_active = false;
		}
		...
	}
	&lt;/pre&gt;
&lt;/code&gt;
&lt;h3&gt;
	Making it Event Sourced
&lt;/h3&gt;
&lt;p&gt;
	It is easy to make things &lt;em&gt;Event Sourced&lt;/em&gt;  from this position, we refactor further to extract out the parameters
	of the method into a class that represents the state change:
&lt;/p&gt;
&lt;code&gt;
	&lt;pre class=&quot;brush: csharp&quot;&gt;
	public class InventoryItem
	{
		...
		public void Deactivate(string reason)
		{
			if (!_active) throw new AlreadyDeactivatedException(_id);
			DoDeactivate(new InventoryItemDeactivated(_id, reason));
		}
		
		private void DoDeactivate(InventoryItemDeactivated event)
		{
			_active = false;
		}
		...
	}
	
	public class InventoryItemDeactivated : Event
	{
		public InventoryItemDeactivated(string reason)
		{
			...
		}
	}
	&lt;/pre&gt;
&lt;/code&gt;
&lt;p&gt;
	When we have separated the code out in this  manner we can re-build the state of the object by applying methods in sequence:
&lt;p&gt;
&lt;code&gt;
	&lt;pre class=&quot;brush: csharp&quot;&gt;
	var item = new InventoryItem();
	item.Created(...);
	item.PriceUpdated(...);
	item.Deactivated(...);
	&lt;/pre&gt;
&lt;/code&gt;
&lt;p&gt;
	Also, we are now independent of any logic that may be imposed, any calculations. We are simply involved in state change
	but what that state change means can vary over time.
&lt;/p&gt;
&lt;h3&gt;
	Event Sourcing is Functional
&lt;/h3&gt;
&lt;p&gt;
	Greg argues that when you have made your model &lt;em&gt;Event Sourced&lt;/em&gt; then the &lt;em&gt;Events&lt;/em&gt; are 
	representations of method calls, they are the serialized function calls of the &lt;strong&gt;DoXXX&lt;/strong&gt; methods that change state
	not the public methods that decide whether or not to allow the state the change. 
&lt;/p&gt;
&lt;p&gt;
	Let&#39;s make another change or two, assume that the &lt;strong&gt;InventoryItem&lt;/strong&gt; is an immutable...
&lt;/p&gt;
&lt;code&gt;
	&lt;pre class=&quot;brush: csharp&quot;&gt;
	public class InventoryItem
	{
		...
		public void Deactivate(string reason)
		{
			if (!_active) throw new AlreadyDeactivatedException(_id);
			DoDeactivate(new InventoryItemDeactivated(_id, reason));
		}
		
		public InventoryItem DoDeactivate(InventoryItemDeactivated event)
		{
			return new InventoryItem(this, false);
		}
		...
	}
	
	public class InventoryItemDeactivated : Event
	{
		public InventoryItemDeactivated(string reason)
		{
			...
		}
	}
	&lt;/pre&gt;
&lt;/code&gt;
&lt;p&gt;
	One advantage of this is that now we can re-build the state of the object at any time by chaining together method calls...
&lt;/p&gt;
&lt;code&gt;
	&lt;pre class=&quot;brush: csharp&quot;&gt;
	new InventoryItem()
		.Created(inventoryItemCreated)
		.PriceUpdated(inventoryItemPriceUpdated)
		.Deactivated(inventoryItemDeactivated);
	&lt;/pre&gt;
&lt;/code&gt;
&lt;p&gt;
	I would apply a simple refactoring here and given that the context of the method call is given in the event then we can call the methods 
	&lt;strong&gt;Apply&lt;/strong&gt;:
&lt;/p&gt;
&lt;code&gt;
	&lt;pre class=&quot;brush: csharp&quot;&gt;
	new InventoryItem()
		.Apply(inventoryItemCreated)
		.Apply(inventoryItemPriceUpdated)
		.Apply(inventoryItemDeactivated)
	&lt;/pre&gt;
&lt;/code&gt;
&lt;p&gt;
	So, what if we each function returns a current state of the &lt;strong&gt;InventoryItem&lt;/strong&gt; then we can remove the need
	for the objects and we can, in a functional language, simply chain calls together to get to the current state of the item:
&lt;/p&gt;
&lt;code&gt;
	&lt;pre class=&quot;brush: csharp&quot;&gt;
	Apply(
		Apply(
			Apply(inventoryItemCreated),
			,inventoryItemPriceUpdated),
		inventoryItemDeactivated);
	&lt;/pre&gt;
&lt;/code&gt;
&lt;p&gt;
	So, I&#39;ve be focussed on re-hydration of the current state from &lt;strong&gt;Events&lt;/strong&gt; but now what happens when 
	we issue a command? Greg explained that these methods would return the event to represent the state change...
&lt;/p&gt;
&lt;code&gt;
	&lt;pre class=&quot;brush: csharp&quot;&gt;
	evt = Reactivate(
			Apply(
				Apply(
					Apply(inventoryItemCreated), 
					inventoryItemPriceUpdated), 
				inventoryItemDeactivated))
	currentState = Apply(
				Apply(
					Apply(
						Apply(inventoryItemCreated), 
						inventoryItemPriceUpdated), 
					inventoryItemDeactivated), 
				evt)
	&lt;/pre&gt;
&lt;/code&gt;
&lt;h3&gt;
	Conclusion
&lt;/h3&gt;
&lt;p&gt;
	Greg has a habit of killing my illusions, and I don&#39;t think that it is just me who takes a look at his work and thinks &quot;hey, is it
	really that simple?&quot; but I think he just looks at the problems from a different perspective. There are things that I have yet to
	be convinced about in terms of functional &lt;em&gt;DDD&lt;/em&gt;:
&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;
		Cohesion: one thing the object model gives you is strong cohesion of the functions, even if we are doing the 
		&lt;em&gt;immutable&lt;/em&gt; domain object then all of the state change methods are bound together in the object
		and can be hidden inside the object from the outside world, I could envisage a 
		&lt;strong&gt;BuildFromHistory(IEnumerable&amp;lt;Event&amp;gt; history)&lt;/strong&gt; method on the object that will
		return it&#39;s immutable state.
	&lt;/li&gt;
	&lt;li&gt;
		Where is the model that we are working on? Where is the &lt;em&gt;ubiquitous language&lt;/em&gt;?
		I can see that we can still model the verbs as methods, as event representations, but where do the nouns fit
		into this? Do we lose the nouns now?
	&lt;/li&gt;
&lt;/ul&gt;
	
	
	</content>
    </entry>
  
    <entry>
      <title>Make Use of Your Subconscious Mind</title>
      <link href="http://ninjaferret.github.com/2012/05/17/make-use-of-your-subconscious-mind.html"/>
      <updated>2012-05-17T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2012/05/17/make-use-of-your-subconscious-mind</id>
      <content type="html">&lt;p&gt;
	I was talking previously about putting yourself in an Oasis of Calm where we are in a relaxed mood and are more creative 
	but quite often I find that one session is not enough, at least not enough to gets us into the depth of a complex problem.  How 
	many of us find a solution to a complex problem when we are away from our desks? In the shower? With friends? What
	powers this sudden insight? For me, the answer is our subconscious mind. We rarely make use of the immense power of our 
	subconscious, rarely fuel it or give it time to solve our problems, it has an amazing power for problem solving and pattern
	matching and we need to use play to help it along the way.
&lt;/p&gt;
&lt;p&gt;
	As a software developer, I spend a lot of my time dealing with complicated issues and complex problem spaces but 
	I have noticed that there is often a pattern to how I have solved problems and talking with other people in the industry 
	I don&#39;t think that I am alone in this. When I am at work and there is a particular problem I am trying to solve it while 
	in the Closed Mode, there is usually pressure on me to fix something and I&#39;m probably stressed so the answers just 
	don&#39;t come, which only makes me even more stressed and less capable of finding a solution. However, throwing my 
	hands up in despair,  I&#39;ll storm off and go do something else completely unrelated, chatting to friends, swimming or 
	doing almost anything else and suddenly I have the answer, my subconscious mind has been freed to comprehend
	the nature of the problem and has provided the answer, or at the very least a clue. 
&lt;/p&gt;
&lt;p&gt;
	Playing around a subject, delving into the possibilities, laughing out it, exploring it all helps us to understand a subject 
	and may open up even more areas for our subconscious to explore. We may not find a solution in one play session, 
	instead that play has planted seeds in our subconscious, fed and watered them, we just have to let them grow. Suddenly 
	our subconscious mind forms a link that our conscious mind could never have done, a link between two concepts that 
	at a shallow level are unconnected but the subconscious mind finds the connection. 
&lt;p&gt;
&lt;p&gt;
	We can then go into another play session and play around with these new connections, that may solve our problem 
	or once again we play and let our subconscious attack the problem until the next play session. If we don&#39;t put 
	pressure on our play to come up with the solution in one session we are more likely, over time, to find the solution because
	the instant we apply that pressure to ourselves, the more likely we are to be stressed and hence &lt;strong&gt;Closed&lt;/strong&gt;.
&lt;/p&gt;
&lt;p&gt;
	One key to having this confidence is to know when you need to make a decision by, to know that you can delay
	critical decisions until they need to be made to give yourself maximum play time and maximum &lt;em&gt;subconscious
	thinking time&lt;/em&gt;. This is not being indecisive, there are times you will need to make instant decisions, but know
	when those times are and know when you can delay the decision. In my own field of expertise there is a phrase
	I keep coming across from the &lt;em&gt;agile development&lt;/em&gt; community, &lt;em&gt;delay decisions until the last
	responsible moment&lt;/em&gt; (which should not be confused with the last possible moment). For as long as is 
	responsibly possible delay your decisions and stay &lt;strong&gt;Open&lt;/strong&gt; to new possibilities.
&lt;/p&gt;
&lt;p&gt;
	I have heard &lt;a href=&quot;http://dannorth.net/&quot;&gt;Dan North&lt;/a&gt; and &lt;a href=&quot;http://lizkeogh.com/&quot;&gt;Liz Keogh&lt;/a&gt;
	talk several times about &lt;em&gt;Real Options&lt;/em&gt;. When you are looking to solve a problem then you should 
	explore what &lt;em&gt;options&lt;/em&gt; are available to you, you can explore them through play, let your subconscious hint to new
	&lt;em&gt;options&lt;/em&gt; the more we know about the options that are available the more we know about when we need to make
	decisions. &lt;em&gt;Options&lt;/em&gt; have a &lt;strong&gt;value&lt;/strong&gt;,  maybe not a quantifiable one (as in trading in options)
	 but the value is in able to delay decision making, but &lt;em&gt;Options&lt;/em&gt; also have an &lt;strong&gt;expiry&lt;/strong&gt;, 
	 the point beyond which the option becomes unviable, i.e. the &lt;em&gt;last responsible moment&lt;/em&gt;. Knowing when
	 each option expires gives us points at which we need to make a decision, should we take this option? Should we 
	 give up on this option and delay the decision until close to the next option expiry?
&lt;/p&gt;
&lt;p&gt;
	So, what can we learn from all of this? We have to give ourselves time to come up with solutions to the problems
	that we face, we can allocate ourselves the time and space we need to play, to explore the options and to feed our
	subconscious mind. Finally, when we have our options, when we understand them, when we understand their expiry
	and we can use play to evaluate the different options then we can delay the decisions until the responsible moment.
&lt;/p&gt;
</content>
    </entry>
  
    <entry>
      <title>Creating Your Oasis Of Calm</title>
      <link href="http://ninjaferret.github.com/2012/05/15/creating-your-oasis-of-calm.html"/>
      <updated>2012-05-15T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2012/05/15/creating-your-oasis-of-calm</id>
      <content type="html">&lt;p&gt;
	I have recently been blogging a bit about creativity and I would like to continue on that theme, to talk 
	more about how we can get ourselves into a position where we are in the &lt;strong&gt;Open Mode&lt;/strong&gt;.
&lt;/p&gt;
&lt;p&gt;
	In &lt;a href=&quot;http://www.brainpickings.org/index.php/2012/04/12/john-cleese-on-creativity-1991/&quot;&gt;John Cleese&#39;s talk about creativity&lt;/a&gt; he uses the phrase &quot;Oasis of Calm&quot; to refer to both a physical space and 
	a time in which we are free from the distractions of the world and free to relax, to become open, to become 
	creative. But what is this &lt;strong&gt;Oasis of Calm&lt;/strong&gt;? How can we build it? 
&lt;/p&gt;
&lt;p&gt;
	The first thing is to find a space in which you feel relaxed, where you can shut yourself away from the distractions 
	of your daily life where you won&#39;t be interrupted and where you will be able to play safely. I often find that in my 
	flat I have a million distractions that I cannot escape from, paperwork that needs doing, cleaning that needs doing, 
	a million chances to procrastinate by watching tv, gaming, or simply using the Internet and making use of my broadband 
	connection. I have found that personally I need to take myself away from that and have often taken myself to a coffee 
	shop where, although there are people around to distract me, I find I am more capable of focusing on the task at hand.
&lt;/p&gt;
&lt;p&gt;
	This may not be the most conducive place for some groups to relax and play together, if the play is noisy or where 
	the play is involving physical activity but for me when I am playing with my imagination it works. I&#39;ve used this 
	technique for preparing my blog posts, writing software and even doing some poetry and in each case it has proved 
	invaluable to my levels of concentration. This entire blog post was conceived planned and written in this way.
&lt;/p&gt;
&lt;p&gt;
	It was only after I had watched the John Cleese talk that I finally understood what it was that I was doing, I was 
	putting myself into the &lt;strong&gt;Open Mode&lt;/strong&gt; by placing myself physically in an &lt;strong&gt;Oasis of Calm&lt;/strong&gt;.
	Like many people out there I had found something that works for me and it had not occurred to me that 
	it could be useful technique to other people, or even just me writing about what I do could be thought-proviking to 
	other people.
&lt;/p&gt;
&lt;p&gt;
	Physical space is only one aspect of the &lt;strong&gt;Oasis of Calm&lt;/strong&gt; as we need to also think about the 
	time we spend in this physical location. Very few of us have lives in which we can be completely free of the 
	real world, it is always there lurking in the background to ruin our day, distract us, put us into the &lt;strong&gt;Closed Mode&lt;/strong&gt;.
	So, in order to be able to relax we should dedicate a specific amount of time to our play, I will call this our 
	&lt;em&gt;Bounded Time&lt;/em&gt; which is a very specific period of time where we will be in the comfortable 
	physical space, for that time we can relax because we know that the cares of the world can be dealt with 
	when that period ends. It is important to have a start and end time, don&#39;t wing it, be disciplined but also the 
	amount of time is highly important. 
&lt;/p&gt;
&lt;p&gt;
	If you are like me, or John Cleese, then once you are in your &lt;strong&gt;Oasis of Calm&lt;/strong&gt; you will 
	find it difficult to switch off from the real world, you will start to relax for a few minutes, start to play with 
	the problem you were struggling with and the &quot;oh... Did I send Dave that e-mail?&quot; or some similar thought 
	pops into your head. A lot of people struggle to get fully relaxed, to shut out their daily lives, but the trick is 
	to relax, let the thoughts come and go, tell yourself you can deal with that later as very few things in life 
	are really that urgent you need to respond now (especially if it is something you already have put off). 
	You may become a bit anxious but ride it out, the feelings will pass, maybe the more creative people are just 
	those who can surf this wave of anxiety on their way to becoming relaxed and open. So, the simple conclusion 
	that we can learn from this is don&#39;t allocate yourself a &lt;em&gt;Bounded Time&lt;/em&gt; of less than 30 minutes as it may take 
	you that long to fully relax and you will only just have started to play.
&lt;/p&gt;
&lt;p&gt;
	So, why do we need create an upper bound? Well, even as children our parents impose upon us a limit to the 
	amount of play we can do, simply because their are real world concerns that have to be taken into consideration 
	(as children this may be meal times, school work, bed time etc) but it is the same for us adults. As I have said 
	earlier, we are isolating ourselves from the real world but we d need to deal with it at some point, the upper 
	bound gives us a point in time at which the real world becomes real to us once more. Also, we will get tired, when 
	people&#39;s bodies get tired they get restless, they start to be stressed and it is easy to lose the moment and become 
	&lt;strong&gt;Closed&lt;/strong&gt;. Probably the upper bound to our play should be around one and a half hours for a 
	single session, especially with group play where it only takes a single person to disrupt the group&#39;s play and actually 
	someone is going to need a comfort break and disrupt the flow of the play.
&lt;/p&gt;
&lt;p&gt;
	I will go into more detail in a later post but it is far better to have a series of one and a half hour play sessions over 
	several weeks than to have a day/morning/afternoon session.
&lt;/p&gt;
&lt;p&gt;
	In conclusion, to feel free and &lt;strong&gt;Open&lt;/strong&gt; you should look to find yourself an &lt;strong&gt;Oasis
	of Calm&lt;/strong&gt; that isolates you from your daily concerns. Don&#39;t be constrained by reminders of the real world
	and give your self &lt;em&gt;Bounded Times&lt;/em&gt; where you can be in your isolated physical space. Once there, relax,
	ride the wave of anxiety that may come and allow yourself to become &lt;strong&gt;Open&lt;/strong&gt;.
&lt;/p&gt;


</content>
    </entry>
  
    <entry>
      <title>Creative Play</title>
      <link href="http://ninjaferret.github.com/2012/04/28/Creative-Play.html"/>
      <updated>2012-04-28T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2012/04/28/Creative-Play</id>
      <content type="html">&lt;p&gt;
	In my &lt;a href=&quot;http://blog.ninjaferret.co.uk/2012/04/25/creativity.html&quot;&gt;blog post about creativity&lt;/a&gt; I talked mentioned
	that we could use &lt;em&gt;Play&lt;/em&gt; to utilise our creative potential and I want to delve a bit deeper into my reasoning behind this.
&lt;/p&gt;

&lt;p&gt;
	When we are children we explore the world through play. Our parents provide a safe environment for us to relax and play without
	feeling too constrained, but without significant risk of us injuring ourselves, but we push against those boundaries, our minds are 
	forming a model of the world and testing each and every boundary. Are we innately born with a knowledge of right and wrong?
	I don&#39;t think so, so we are taught that and we find that as we explore certain things open up to us but other boundaries stay closed, so
	when we are playing we are learning and our understanding of the world is driven by our imagination, which for a child is almost
	unbounded.
&lt;/p&gt;
&lt;p&gt;
	We see this all over the animal kingdom, when we see lion cubs playing and mock-fighting they are learning crucial skills about
	how to defend their territory and when they are chasing small animals they are learning about hunting, they don&#39;t know what to
	do if they caught their playmate nor do they often catch anything but they are learning all of the time. Yet, the rest of the pride are
	there to keep them safe and to provide some boundaries, mum will always make sure they don&#39;t put themselves in danger. 
&lt;/p&gt;
&lt;p&gt;
	As we grow older, I believe we tend to lose this playfulness and I think that this is a sad thing. I don&#39;t expect teenagers and adults to get 
	out the Barbie and Action Man to play but the playful spirit, the curiousity the willingness to try new things somehow disappears
	as the pressures of the world increase. We move from a life in the &lt;em&gt;Open Mode&lt;/em&gt; to a live in the &lt;em&gt;Closed Mode&lt;/em&gt;
	and in many ways we lose the power to access our imagination to the same level. Do we feel playful at school? Do we get 
	inspired to be curious? Or is school the place where we learn to give up on play? Are we under pressure to achieve grades? Are we 
	bored when in lesson? Is there pressure on us from our peers to appear cool and disinterested? I think that the education system 
	is focussed on knowledge more than it on actually developing the capacity to get into the &lt;em&gt;Open Mode&lt;/em&gt; or developing
	the key skills to allow people to learn. Quite often in life it is not what you know that is crucially important, but how easily you can
	find and learn the knowledge, or skills, that are now required of you, how adaptable you are and I think that the &lt;em&gt;Open Mode&lt;/em&gt;
	will help you become more adaptable. 
&lt;/p&gt;
&lt;p&gt;
	In the corporate world, &lt;em&gt;Play&lt;/em&gt; is almost taboo. It implies a sense of frivolity, lack of focus, lack of seriousness and
	solemntiy that you need in order to run a successful business, or does it? Well, it depends on the type of business that you are in.
	The area of your business that deals with mass production, automated repetitive tasks, a production line or simple things like
	a warehouse where the computer tells you to put Box A onto Shelf B then the people working in that area do not really need to
	play, they can remain in the &lt;em&gt;Closed Mode&lt;/em&gt; and get on with their work. However, product design, business process
	design, systems design, software development and many other fields require a certain amount of time to be spent in the 
	&lt;em&gt;Open Mode&lt;/em&gt; and these areas can make significant use of &lt;em&gt;Play&lt;/em&gt;.
&lt;/p&gt;
&lt;p&gt;
	Play stimulates the imagination, stimulates creativity and allows us to explore our curiousity. Just as when we are children our
	companies should act like our parents and provide a safe environment for us to play, a place where we can be free to explore the
	world in which we work, our play cannot ever be highly focussed but it can be centred around the needs of the organisation.
&lt;/p&gt;
&lt;p&gt;
	What games could we play? What if we could get everyone who is involved in a business process in a room? What if routine scenarios
	were acted out? Perhaps a chosen facilitator could write down the processes, discuss what they wrote down, try out different processes
	for the different scenarios and understand each other&#39;s roles and how the process works, or doesn&#39;t for them. Could we take a problem
	and make people draw out how their understanding of the problem? Would everyone agree? A picture is easier to understand, makes you
	focus on the key points and it is a bit fun, you can use pretty colours, but they are merely talking points, they are parts of the game and 
	we use them to gain a deeper understanding. If we communicate with each other, if we gain that understanding, then a solution may
	present itself. 
&lt;/p&gt;
&lt;p&gt;
	So, it is my firm belief that we should try to find ourselves more time to play in order to make use of the amazing power of our 
	imagination and our subconscious to explore the world around us.
&lt;/p&gt;</content>
    </entry>
  
    <entry>
      <title>Creativity</title>
      <link href="http://ninjaferret.github.com/2012/04/25/creativity.html"/>
      <updated>2012-04-25T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2012/04/25/creativity</id>
      <content type="html">&lt;p&gt;
	Recently, I was inspired to think more about the creative process after watching a
	&lt;a href=&quot;http://www.brainpickings.org/index.php/2012/04/12/john-cleese-on-creativity-1991/&quot;&gt;John Clease
	talk on creativity&lt;/a&gt; and also &lt;a href=&quot;http://www.bbc.co.uk/programmes/b01g99j3&quot;&gt;the BBC documentary
	&lt;cite&gt;Beautiful Minds: Professor Andre Geim&lt;/cite&gt;&lt;/a&gt;, the discoverer of Graphene. Both of them
	gave me some insights into what makes us creative and potentially how we can become creative.
&lt;/p&gt;

&lt;p&gt;
	There was a time when I longed to be more creative but there was an odd connection that my mind had formed
	and I came to realise that I was longing to be more artistic, to be a poet, artist or musician but that creativity does
	not solely exist in these art forms. Creativity, the ability to create something new, is at the centre of scientific
	discovery, the centre of engineering, of architecture and of software development. Each of these fields requires us
	to delve deeply into often complex problems and find solutions, to solve a problem that no one has solved before and 
	to make connections that perhaps other people cannot make. If we contrast working as a scientist compared to working
	on a production line we can see many of the differences, the production line requires us to perform the same repetitive
	where as the scientist will often perform the same experiments again and again but also spends a significant amount
	of time devising the experiments, analysing the data and coming up with a model of the world that fits the results.
&lt;/p&gt;

&lt;p&gt;
	However, even within these professions, even within these complex spheres of science and engineering there are
	those who are considered to be creative by their peers, to stand out among the crowd. Are these people actually any
	more intelligent, any more talented, any more creative than those around them or is it something else that makes them
	stand out?
&lt;/p&gt;

&lt;p&gt;
	John Cleese pointed out the work of the psychologist Donald W. MacKinnon who is a significant figure in the research
	into the creative mind. MacKinnon did a study of architects, some who were considered highly creative and looked
	into their creativity but what he found was that creativity was not a talent, the people who were considered more creative
	were the people who were more easily able to get into what he calls the &lt;em&gt;Open Mode&lt;/em&gt; rather than the
	&lt;em&gt;Closed Mode&lt;/em&gt;.
&lt;/p&gt;


&lt;h2&gt;
	The Closed Mode
&lt;/h2&gt;

&lt;p&gt;
	So, as I said above, thew &lt;em&gt;Closed Mode&lt;/em&gt; is where most people live most of our lives. We go to work, 
	there is usually far too much to do and far too little time to do it, we are doing household chores, we are performing
	repetitive tasks, we are responding to e-mail, telephone calls, texts, twitter, facebook and there are many other 
	distractions. Most people will know the complete frustration when you are focussed on solving a difficult problem
	and someone turns up and goes &amp;ldquo;do you have a moment?&amp;rdquo; and the instant they say that you lose
	your train of thought and the moment is gone... you are not disrupted and you might as well answer their question. 
&lt;/p&gt;

&lt;p&gt;
	When we are in the &lt;em&gt;Closed Mode&lt;/em&gt; we are more likely to find ourselves stressed, under pressure and
	anxious about something. We may also be quite nervous, we are trying to impress, or we are afraid of saying the 
	wrong thing, or looking stupid in front of someone important. The simple fact is that when we are feeling these
	negative emotions we do not really feel creative. 
&lt;/p&gt;

&lt;p&gt;
	What happens when someone says to you  &amp;ldquo;Go on, say something funny!&amp;rdquo;? I certainly lose all inspiration 
	about anything funny to say (though there is a debate to be had about whether I actually	have anything funny to say). 
	The reason for this is that, in my opinion, spontaneity and humour are both creative things, the pressure that is put on
	someone to act in that way stifles the process and puts us into the &lt;em&gt;Closed Mode&lt;/em&gt;. 
&lt;/p&gt;

&lt;p&gt;
	Too often we get stuck in the &lt;em&gt;Closed Mode&lt;/em&gt; and too often we find that we are just ploughing on regardless
	and becoming tunnel visioned, maybe even too goal oriented. By focussing solely on the destination and by focussing
	on a single way to get there, we sometimes isolate ourselves from new, exciting and even better ways to achieve our
	goal but we do not take the time to step back and take a wider view. 
&lt;/p&gt;

&lt;p&gt;
	However, the &lt;em&gt;Closed Mode&lt;/em&gt; does have its uses. The &lt;em&gt;Closed Mode&lt;/em&gt; is an active mode, we are more able
	to just &amp;ldquo;get stuff done&amp;rdquo; that does not require any creative input, i.e. fairly linear/repetative tasks, or tasks that
	simply apply following specific rules, in fact the creative mind would impede this process. A creative mind is more
	likely to say &amp;ldquo;I wonder what would happen if...?&amp;rdquo; or &amp;ldquo;Surely this can&#39;t be right...&amp;rdquo; and 
	goes off on wild tangents and gets distracted by creativity. 
&lt;/p&gt;

&lt;h2&gt;
	The Open Mode
&lt;/h2&gt;

&lt;p&gt;
	In order for us to be creative we need to be in the &lt;em&gt;Open Mode&lt;/em&gt;. When we are in the open mode we
	are free from the distractions of our daily lives, we are free to concentrate on a particular problem space, we are 
	free from the pressures of all of the work that must get done and therefore we are far more relaxed. I think that when
	we are dealing with complexity, with a problem, we should be aiming to spend some of our time in the &lt;em&gt;Open Mode&lt;/em&gt;
	to allow ourselves to become more creative and to come up with more innovative solutions to that problem.
&lt;/p&gt;

&lt;p&gt;
	As Andre Geim said in the documentary:
&lt;/p&gt;

&lt;blockquote&gt;
	When you are travelling on rails it is very difficult to find something new. When you make forays into new areas you
	increase the chance of finding something new.
&lt;/blockquote&gt;

&lt;p&gt;
	What Andre is saying here is really that while we are in the &lt;em&gt;Closed Mode&lt;/em&gt; we are very narrowly focussed
	and unlikely to see the wider view. When we are in the &lt;em&gt;Open Mode&lt;/em&gt; we are much more likely to engage
	in what I will call &lt;em&gt;Play&lt;/em&gt; where we will try new and unexpected things, we will increase the chances of finding
	a connection that we did not know before, we are open to new discoveries, we are being creative. 
&lt;/p&gt;

&lt;p&gt;
	If we &lt;em&gt;play&lt;/em&gt; around an idea we have to have the confidence, the freedom, the relaxation to be able to make
	mistakes, to be able to fail and to try out even random things, try out impossibilities and use them to discover entirely new
	possibilities. In &lt;em&gt;play&lt;/em&gt; we are curious, relaxed and confident, we may be playing with a particular problem
	space but we are not tunnel visioned.
&lt;/p&gt;

&lt;h2&gt;
	Final Thoughts...
&lt;/h2&gt;

&lt;p&gt;
	When we have a problem I think that often, especially as adults, we rush headlong into the problem and grind away until
	we have managed to make it go away and we do this is us in the &lt;em&gt;Closed Mode&lt;/em&gt;. However, to find a creative
	solution to the problem we have to relax, we have to get ourselves into the &lt;em&gt;Open Mode&lt;/em&gt; and then we can
	explore the possibilites that are available to us and come up with a solution to our problem. Once we have a solution, we should
	go back into the &lt;em&gt;Closed Mode&lt;/em&gt; and implement that solution away from the creative doubts that the &lt;em&gt;Open
	Mode&lt;/em&gt; would undoubtedly bring.
&lt;/p&gt;

&lt;p&gt;
	Over the next few weeks I hope to be releasing some more blog posts around this subject delving a bit deeper into the 
	techniques and tools that we can use to help us become more creative... stay tuned.
&lt;/p&gt;
</content>
    </entry>
  
    <entry>
      <title>A Journey into MSBuild</title>
      <link href="http://ninjaferret.github.com/2011/04/13/My-MSBuild-Experiences.html"/>
      <updated>2011-04-13T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2011/04/13/My-MSBuild-Experiences</id>
      <content type="html">&lt;p&gt;
For the majority of the time I have avoided looking too deeply into the depths of our &lt;strong&gt;MSBuild&lt;/strong&gt; scripts allows TeamCity to build our projects, run our tests, prepare our artefacts and even run the deployment to our development and test servers but I am starting a new project and looking at using MSBuild to co-ordinate all of these activities.
&lt;/p&gt;

&lt;p&gt;
However, the current project files in existing projects seem to be large and overly complex, there are a lot of properties and it is difficult to see what needs to change when adding new components into the system. What I want to do is to simplify the process and reduce the amount of thought that you need in order to create a new project.
&lt;/p&gt;

&lt;h3&gt;The Master Project File&lt;/h3&gt;

&lt;p&gt;
The standard way I approach this is that we have a &lt;strong&gt;teamcity-master.proj&lt;/strong&gt; file that has the responsibility for loading any projects and MSBuild tasks that are required as well as providing a series of high-level targets:
&lt;/p&gt;
&lt;dl&gt;
	&lt;dt&gt;Build&lt;/dt&gt;
	&lt;dd&gt;This is solely responsible for building the solution, however, it will defer to an actual task in the &lt;strong&gt;teamcity-build.proj&lt;/strong&gt; that knows how to build your project.&lt;/dd&gt;
	&lt;dt&gt;UnitTest&lt;/dt&gt;
	&lt;dd&gt;This requires that the &lt;em&gt;Build&lt;/em&gt; target is run and is solely responsible for running all of the unit tests that you have written, however, it will defer to the actual unit test tasks in the &lt;strong&gt;teamcity-unit-tests.proj&lt;/strong&gt; file.&lt;/dd&gt;
	&lt;dt&gt;Package&lt;/dt&gt;
	&lt;dd&gt;This requires that the &lt;em&gt;UnitTest&lt;/em&gt; target is run and is then solely responsible for packaging the artefacts for deployment to a staging area. Once again this defers to a separate &lt;strong&gt;teamcity-package.proj&lt;/strong&gt; file that manages the complexity of preparing the artefacts.&lt;/dd&gt;
	&lt;dt&gt;Deploy&lt;/dt&gt;
	&lt;dd&gt;This will take the latest set of deployment artefacts from the staging area and deploy them to the test deployment environment and is controlled by the contents of the &lt;strong&gt;teamcity-deploy.proj&lt;/strong&gt; project file for details.&lt;/dd&gt;
	&lt;dt&gt;IntegrationTest&lt;/dt&gt;
	&lt;dd&gt;Whereas the &lt;em&gt;UnitTest&lt;/em&gt; project may have run &quot;acceptance tests&quot; that involve mocking out external dependencies this will drive the automated integration-level tests that require the ability to drive the user interface etc. These tests are again controlled directly from the &lt;strong&gt;teamcity-integration-tests.proj&lt;/strong&gt; file.
&lt;/dl&gt;

As you can see, the master project file is solely there for orchestrating the builds, providing the standard targets for different teamcity builds to run depending on their job, e.g. you may have a simple build and unit test project to run after every check-in but nightly run a full build that builds, runs unit tests, packages, deploys and runs the integration tests (or it may be an on demand project). Details are kept away from the master file allowing each independent section to be controlled in finer detail.

&lt;h3&gt;Build (teamcity-build.proj)&lt;/h3&gt;

</content>
    </entry>
  
    <entry>
      <title>Fundamentals of a Successful Project</title>
      <link href="http://ninjaferret.github.com/2011/03/12/Fundamentals-Of-A-Successful-Project.html"/>
      <updated>2011-03-12T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2011/03/12/Fundamentals-Of-A-Successful-Project</id>
      <content type="html">&lt;p&gt;Over the last few years I have grown increasingly passionate about the agile/lean movements and truly believe that for the majority of the projects I have encountered these methodologies would actually provide a better way to manage their complexity. What I aim to write in this post is some of the key learnings, they are simply re-iterations of what has been previously said about agile and lean but I think that there are some core concepts that can be taken across to any project.&lt;/p&gt;

&lt;h3&gt;Get &quot;Buy in&quot; to the Process&lt;/h3&gt;

&lt;p&gt;Everyone involved in a software project from the business to the developers/testers need to understand the development process but they also need to buy into that process. No single process is applicable to all situations and there must be a carefully considered decision about which methodology to use and to get people to understand why this process has been chosen.&lt;/p&gt;

&lt;p&gt;I have been on a number of &quot;agile&quot; projects where significant stakeholders just wanted to write some vague requirements at the start and then don&#39;t want to be bothered, where testers don&#39;t really want to know about the project until it is delivered to them with weeks before the project is needed live; on other projects, managers have demanded that everything be written down, again on an &quot;agile&quot; project, and e-mailed to all concerned parties who then take ages to respond. These project are incredibly difficult to run and manage because the development team are looking for quick feedback on both requirements and from testing but when this does not come the team ends up reverting back to a waterfall approach and spend a lot of effort &quot;protecting themselves&quot; from any potential comeback.&lt;/p&gt;

&lt;p&gt;One thought is that on these projects were actually waterfall but the developers were trying to go more agile but either way, where there is this conflict there is going to be uncertainty and risk.&lt;/p&gt;

&lt;h3&gt;Communication&lt;/h3&gt;

&lt;p&gt;Regardless of whether you are working in an agile manner, a waterfall manner or even in a post-agile manner, the key to a successful project is communication. I believe that the business, architects, developers and testers should be talking constantly about the project to highlight and tackle issues sooner rather than later.&lt;/p&gt;

&lt;p&gt;Good communication will help to build trust between the business and the development team, a regular show of progress can help to allay fears and to also help communicate the amount of effort that the developers are putting into the project.&lt;/p&gt;

&lt;p&gt;When things are faltering communication becomes even more important, letting people know that there are issues allows everyone involved to prioritise them, tackle the issues immediately or even just plan to deal with them. The sooner you know there is an issue, the more options you have available and therefore the more likely you are to find a workable solution.&lt;/p&gt;

&lt;h3&gt;Feedback&lt;/h3&gt;

&lt;p&gt;This really is a sub-set of communication but it is something that I consider vitally important. Businesses should constantly be focussed on the risks to their projects and one of the greatest risks that I can see is concerned with the timescales of feedback.&lt;/p&gt;

&lt;p&gt;In a most waterfall projects of any significant size (6 months or bigger) that I have been a part of the feedback loop between the developers and the testers is huge. Several months of work before the testers even start to look at the system means that bugs entered into the system at the very start do not surface until close to the end. The sooner we can start testing, the sooner these bugs will surface and, once again, the more options the development team will have to fix the bug.&lt;/p&gt;

&lt;p&gt;Also, we need to be looking at the feedback loop between the business and the developers/testers. The business define their requirements at the very start, usually with the analysts, and will sign-off the design but if they are not involved in the day-to-day development issues will arise that the customers will have no control over and decisions will be made that may end up not delivering what is actually needed but can be interpreted as meeting the requirements. The business will end up seeing the final system when they get to perform UAT but the developers have not received the feedback that what they have developed is correct until it is too late to make any significant changes.&lt;/p&gt;

&lt;p&gt;This is quite an agile concept, about reducing the feedback loops to improve quality and importantly to keep the project on course. It does not mean that you have to give up the up-front-design or even give up on having the analysis, design, build and test stages of a project but at least regularly show your work and get that feedback.&lt;/p&gt;

&lt;h3&gt;Slack&lt;/h3&gt;

&lt;p&gt;Quite often we expect teams to commit all their time to something, either to the current project, or to support, or to the design/analysis of the next project and there is never any slack built into the system. When you have the slack you can burst when there are issues that need to be addressed immediately but most of the time you can spend your time learning, improving your skills or even tackling technical debt issues that are normally left because there is no budget.&lt;/p&gt;

&lt;p&gt;I believe that development teams should be constantly working, constantly developing new software, but at a predictable pace, if you have them running at 100% you will lose that predictability as problems have a proportionally larger effect than if you are working at 75% where the problems can be absorbed by the slack in the system.&lt;/p&gt;

&lt;p&gt;One major advantage of building deliberate slack into the system (and having the discipline to maintain it under most circumstances) is that developers will have time to research into new techniques or APIs and become better developers.&lt;/p&gt;

&lt;h3&gt;Adopt a Team Centric Approach&lt;/h3&gt;

&lt;p&gt;I have realised that I much prefer working as part of a consistent team, or at least working as a team. I want the team members to be passionate developers who won&#39;t let me get away with making stupid decisions or not thinking about the impact of a decision, people who will question me and force me to justify my thought processes but will also accept when a decision has been made.&lt;/p&gt;

&lt;p&gt;I find that if I work alone then I will be less productive and should I fall ill, or even take a holiday, the knowledge of the software has left the company and there is no one who understands what has been done or how it has been done.&lt;/p&gt;

&lt;p&gt;Also, perhaps one that most people don&#39;t consider important, is that I find I will work harder because of the people in my team. It is often much harder to feel a level of loyalty to a lot of companies but if you are working with a good team, if you feel part of that team you will feel loyalty to that team and you will work harder for them.&lt;/p&gt;

&lt;h3&gt;Empowerment/Ownership&lt;/h3&gt;

&lt;p&gt;I think finally I would like to say that quite often projects seem to flounder despite the lack of strong leadership and I think that is partially due to the fact that the team do not fully feel that the project belongs to them. The project belongs to all of the stakeholders, including the users, analysts, developers and testers. If each of these people has a personal stake in the project, if they feel that they are empowered to raise issues, to guide and make decisions based on the ultimate goals of the project then they will strive harder to make that work. People who feel that they are simply doing something for someone tend to approach that task with less vigour.&lt;/p&gt;

&lt;h3&gt;Summary&lt;/h3&gt;

&lt;p&gt;In summary, I think that regardless of whether you are doing projects in an agile manner or a waterfall manner there are a number things that can be done to drive that project forward more successfully, the teams will feel more involved and will build a culture of success.&lt;/p&gt;</content>
    </entry>
  
    <entry>
      <title>Tagging on GitHub Pages</title>
      <link href="http://ninjaferret.github.com/2011/02/12/tagging-on-github-pages.html"/>
      <updated>2011-02-12T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2011/02/12/tagging-on-github-pages</id>
      <content type="html">&lt;p&gt;I now have my blog on &lt;a href=&quot;http://pages.github.com&quot;&gt;GitHub Pages&lt;/a&gt; but there are certain features that I would kind of like that I was unsure that I would be able to do with a static file and one of these was tagging or categories.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/mojombo/jekyll&quot;&gt;Jekyll&lt;/a&gt; static site generator is blog aware and they weren&#39;t going to miss out on blog tags/categories feature.&lt;/p&gt;

&lt;h3&gt;Adding Categories to Your Blog Posts&lt;/h3&gt;

&lt;p&gt;The categories are defined by in the meta data that you place at the top of your files, just add them as a yaml list:&lt;/p&gt;

&lt;code&gt;&lt;pre class=&quot;brush: plain&quot;&gt;---
layout: default
title: Tagging on GitHub Pages
categories:
- blogging
- github
- tags
---&lt;/pre&gt;&lt;/code&gt;

&lt;h3&gt;Create a Tags Page&lt;/h3&gt;

&lt;p&gt;We have our pages now with a list of categories and the next thing to do is to produce a list of tags and every page that has that tag. The way I approached this was to create a new page called &lt;strong&gt;tags.html&lt;/strong&gt;:&lt;/p&gt;

&lt;code&gt;&lt;pre class=&quot;brush: xml&quot;&gt;---
layout: default
title: Tags
---
&amp;lt;ul&amp;gt;
&amp;#123;% for category in site.categories %&amp;#125;
  &amp;lt;li&amp;gt;&amp;lt;a href=&quot;#&amp;#123;&amp;#123; category | first &amp;#125;&amp;#125;&quot;&amp;gt;&amp;#123;&amp;#123; category | first &amp;#125;&amp;#125;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
&amp;#123;% endfor %&amp;#125;
&amp;lt;/ul&amp;gt;

&amp;lt;h2&amp;gt;Articles by tag&amp;lt;/h2&amp;gt;

&amp;lt;ul&amp;gt;
&amp;#123;% for category in site.categories %&amp;#125;
  &amp;lt;li&amp;gt;&amp;lt;a name=&quot;&amp;#123;&amp;#123; category | first &amp;#125;&amp;#125;&quot;&amp;gt;&amp;#123;&amp;#123; category | first &amp;#125;&amp;#125;&amp;lt;/a&amp;gt;
    &amp;lt;ul&amp;gt;
    &amp;#123;% for posts in category offset: 1 %&amp;#125;
      &amp;#123;% for post in posts %&amp;#125;
        &amp;lt;li&amp;gt;&amp;lt;a href=&quot;&amp;#123;&amp;#123; post.url &amp;#125;&amp;#125;&quot;&amp;gt;&amp;#123;&amp;#123; post.title &amp;#125;&amp;#125;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
      &amp;#123;% endfor %&amp;#125;
    &amp;#123;% endfor %&amp;#125;
    &amp;lt;/ul&amp;gt;
  &amp;lt;/li&amp;gt;
&amp;#123;% endfor %&amp;#125;
&amp;lt;/ul&amp;gt;&lt;/pre&gt;&lt;/code&gt;

&lt;p&gt;At the moment it is not the prettiest thing but it does the job. This is built using the &lt;a href=&quot;http://www.liquidmarkup.org/&quot;&gt;Liquid Templating language&lt;/a&gt; and there were a number of things to be aware of:&lt;/p&gt;

&lt;p&gt;All of the categories in all of the pages are contained in the &lt;strong&gt;site.categories&lt;/strong&gt; and each element is an array that contains all of the posts for that category but there is a problem... The first element of the array is actually the category name and the subsequent elements are the posts, therefore to get the name we can use &lt;strong&gt;&amp;#123;&amp;#123; category | first &amp;#125;&amp;#125;&lt;/strong&gt; which is a special filter that gets the first element.&lt;/p&gt;

&lt;p&gt;So, there is a problem with the displaying of the pages if we have to ignore the first element, therefore we have to skip the first element using the offset filter &lt;strong&gt;&amp;#123;% for posts in category offset: 1 %&amp;#125;&lt;/strong&gt; which skips the first element of the category array.&lt;/p&gt;

&lt;h3&gt;Creating a basic tag list&lt;/h3&gt;

&lt;p&gt;I also want to get the list of tags for the site into my side-bar along with the twitter feed, so people can jump to other interesting articles. The tag list will link right back to the tags page I made earlier to get the list of related posts.&lt;/p&gt;

&lt;p&gt;The mechanism that I used to get the list was extremely similar to the way I generated the tags page but I make use of the &lt;strong&gt;&amp;#123;&amp;#123; category | size &amp;#125;&amp;#125;&lt;/strong&gt; filter to get the size of the array, but remember the first element is just the name, not a post, therefore there is an extra filter that I added on get the last element of the array which is the list of posts &lt;strong&gt;&amp;#123;&amp;#123; category | last | size &amp;#125;&amp;#125;&lt;/strong&gt;:&lt;/p&gt;

&lt;code&gt;&lt;pre class=&quot;brush: xml&quot;&gt;&amp;lt;html&amp;gt;
...
&amp;lt;div id=&quot;tags&quot;&amp;gt;
	&amp;lt;h3&amp;gt;Tags&amp;lt;/h3&amp;gt;
	&amp;lt;ul&amp;gt;
	&amp;#123;% for category in site.categories %&amp;#125;
	  	&amp;lt;li&amp;gt;&amp;lt;a href=&quot;/tags.html#&amp;#123;&amp;#123;category | first&amp;#125;&amp;#125;&quot;&amp;gt;
	    &amp;#123;&amp;#123;category | first&amp;#125;&amp;#125;&amp;lt;/a&amp;gt; 
	    (&amp;#123;&amp;#123;category | last | size&amp;#125;&amp;#125;)&amp;lt;/li&amp;gt;
	&amp;#123;% endfor %&amp;#125;
&amp;lt;/div&amp;gt;
...
&amp;lt;html&amp;gt;&lt;/pre&gt;&lt;/code&gt;

&lt;h3&gt;Summary&lt;/h3&gt;

&lt;p&gt;This gives me now a starting point to start making a tag cloud for my blog, this is the simple implementation that I can think of but it didn&#39;t take me too long. I hope you find this useful if you are thinking of creating your own GitHub Pages blog.&lt;/p&gt;</content>
    </entry>
  
    <entry>
      <title>Creating an Atom Feed on GitHub Pages</title>
      <link href="http://ninjaferret.github.com/2011/02/12/github-pages-atom-feed.html"/>
      <updated>2011-02-12T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2011/02/12/github-pages-atom-feed</id>
      <content type="html">&lt;p&gt;Like any good blog I need an feed so that the many people who are addicted to my random ramblings can follow me using their news readers. So, how would I do that?&lt;/p&gt;

&lt;p&gt;The atom feed is simply an XML document and the Liquid Templating engine can generate anything that you want so I searched on-line and found some atom samples and used them to generate this &lt;a href=&quot;/feed.atom&quot;&gt;Atom feed&lt;/a&gt; file:&lt;/p&gt;

&lt;code&gt;&lt;pre class=&quot;brush: xml&quot;&gt;---
layout: null
---
&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;
&amp;lt;feed xmlns=&quot;http://www.w3.org/2005/Atom&quot;&amp;gt;
  &amp;lt;title&amp;gt;Ninja Ferret&amp;lt;/title&amp;gt;
  &amp;lt;link href=&quot;http://ninjaferret.github.com/feed.atom&quot; rel=&quot;self&quot;/&amp;gt;
  &amp;lt;link href=&quot;http://ninjaferret.github.com/&quot;/&amp;gt;
  &amp;lt;updated&amp;gt;&amp;#123;&amp;#123; site.time | date_to_xmlschema &amp;#125;&amp;#125;&amp;lt;/updated&amp;gt;
  &amp;lt;id&amp;gt;http://ninjaferret.github.com&amp;lt;/id&amp;gt;
  &amp;lt;author&amp;gt;
    &amp;lt;name&amp;gt;Ian Johnson&amp;lt;/name&amp;gt;
    &amp;lt;email&amp;gt;ij2030+blog@gmail.com&amp;lt;/email&amp;gt;
  &amp;lt;/author&amp;gt;
  &amp;#123;% for post in site.posts %&amp;#125;
    &amp;lt;entry&amp;gt;
      &amp;lt;title&amp;gt;&amp;#123;&amp;#123; post.title &amp;#125;&amp;#125;&amp;lt;/title&amp;gt;
      &amp;lt;link href=&quot;http://ninjaferret.github.com&amp;#123;&amp;#123; post.url &amp;#125;&amp;#125;&quot;/&amp;gt;
      &amp;lt;updated&amp;gt;&amp;#123;&amp;#123; post.date | date_to_xmlschema &amp;#125;&amp;#125;&amp;lt;/updated&amp;gt;
      &amp;lt;id&amp;gt;http://ninjaferret.github.com&amp;#123;&amp;#123; post.id &amp;#125;&amp;#125;&amp;lt;/id&amp;gt;
      &amp;lt;content type=&quot;html&quot;&amp;gt;&amp;#123;&amp;#123; post.content | xml_escape &amp;#125;&amp;#125;&amp;lt;/content&amp;gt;
    &amp;lt;/entry&amp;gt;
  &amp;#123;% endfor %&amp;#125;
&amp;lt;/feed&amp;gt;&lt;/pre&gt;&lt;/code&gt;

&lt;p&gt;And there is it, it&#39;s a simple file to create that will build an atom feed for your site.&lt;/p&gt;</content>
    </entry>
  
    <entry>
      <title>Blogging with GitHub Pages</title>
      <link href="http://ninjaferret.github.com/2011/02/12/blogging-with-github-pages.html"/>
      <updated>2011-02-12T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2011/02/12/blogging-with-github-pages</id>
      <content type="html">&lt;p&gt;
	For a while I have been using WordPress to host my blog but recently I have just become a bit 
	disappointed with the way in which I felt I had less control over the page than I would like. I
	know that there is a lot of flexibility and power within WordPress but it was just something
	else that I had to learn just to maintain a blog. So, I decided that I would look at
	&lt;a href=&quot;http://pages.github.com&quot;&gt;GitHub Pages&lt;/a&gt; which will give me the ability to have more 
	control and also improve my web design skills.
&lt;/p&gt;

&lt;p&gt;
	One of the advantages that I see about using github is that you then have your content in source control rather than just in a database on another server and I always have copies on my computers so there are multiple copies produced to keep the data safe.
&lt;/p&gt;

&lt;h3&gt;Create your github repository&lt;/h3&gt;

&lt;p&gt;
	If you don&#39;t have an account already then you need to do that first. After that simply create a repository in the format &lt;strong&gt;&lt;em&gt;yourusername&lt;/em&gt;.github.com&lt;/strong&gt; which tells github that you want to use this as a github pages repository. 
&lt;/p&gt;

&lt;h2&gt;Generate some content&lt;/h2&gt;


&lt;p&gt;So on my computer I should really clone the git repository:&lt;/p&gt;

&lt;code&gt;
	&lt;pre class=&quot;brush: plain&quot;&gt;$ cd github
$ git clone git@github.com:yourusername/yourusername.github.com.git	&lt;/pre&gt;
&lt;/code&gt;

&lt;p&gt;Let&#39;s add some content - so the first thing is to create an &lt;em&gt;index.html&lt;/em&gt; file:&lt;/p&gt;

&lt;code&gt;
	&lt;pre class=&quot;brush: xml&quot;&gt;&amp;lt;html&amp;gt;
  &amp;lt;body&amp;gt;
  &amp;lt;p&amp;gt;Hello world!&amp;lt;/p&amp;gt;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;&lt;/pre&gt;
&lt;/code&gt; 

&lt;p&gt;And now let&#39;s push that file to the server:&lt;/p&gt;

&lt;code&gt;
	&lt;pre class=&quot;brush: plain&quot;&gt;$ git add index.html
$ git commit -m &#39;First commit - creating index.html&#39;
$ git push origin master&lt;/pre&gt;
&lt;/code&gt;

&lt;p&gt;
	In my experience the first commit takes a little while for it to appear (~10 mins) so be patient and then go to &lt;strong&gt;http://&lt;em&gt;yourusername&lt;/em&gt;.github.com&lt;/strong&gt;. There you go, you have your first page ready to go.
&lt;/p&gt;

&lt;h3&gt;Building your first template&lt;/h3&gt;

&lt;p&gt;
	&quot;Is that all there is?&quot; I hear you say, &quot;just uploading static HTML pages to github?&quot;
&lt;/p&gt;

&lt;p&gt;
	Well that is not all there is because GitHub generate your site using &lt;a href=&quot;https://github.com/mojombo/jekyll&quot;&gt;Jekyll&lt;/a&gt; (at the time of writing this is version &lt;strong&gt;0.10.0&lt;/strong&gt;). Jekyll allows you to use a templating mechanism for standardising the format and layout of your site so let&#39;s take a look at our first template.
&lt;/p&gt;

&lt;p&gt;Create a new folder in your repository called &lt;strong&gt;_layouts&lt;/strong&gt; and create a new file within there called &lt;strong&gt;default.html&lt;/strong&gt;:&lt;/p&gt;

&lt;code&gt;
	&lt;pre class=&quot;brush: xml&quot;&gt;&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;
    &amp;lt;title&amp;gt;&amp;#123;&amp;#123;page.title&amp;#125;&amp;#125;&amp;lt;/title&amp;gt;
  &amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
  &amp;lt;h1&amp;gt;&amp;#123;&amp;#123;page.title&amp;#125;&amp;#125;&amp;lt;/h1&amp;gt;
  &amp;#123;&amp;#123;page.content&amp;#125;&amp;#125;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;&lt;/pre&gt;
&lt;/code&gt;

&lt;p&gt;We can now go back to our &lt;strong&gt;index.html&lt;/strong&gt; file and modify it to use the template:&lt;/p&gt;

&lt;code&gt;
	&lt;pre class=&quot;brush: xml&quot;&gt;---
layout: default
title: Hello World!
---
&amp;lt;Welcome to my new blog, this will be the best blog ever!&amp;gt;&lt;/pre&gt;
&lt;/code&gt;

&lt;p&gt;All that is left is to push the changes back up to GitHub for the files to re-compile.&lt;/p&gt;

&lt;code&gt;
	&lt;pre class=&quot;brush: plain&quot;&gt;$ git add index.html
$ git add _layouts/default.html
$ git commit -m &#39;Adding in layout support&#39;
$ git push origin master&lt;/pre&gt;
&lt;/code&gt;

&lt;h3&gt;What about posts?&lt;/h3&gt;

&lt;p&gt;
	&lt;a href=&quot;https://github.com/mojombo/jekyll&quot;&gt;Jekyll&lt;/a&gt; is a blog aware rendering engine therefore does have post support. All you need to do is to create a new directory called &lt;strong&gt;_posts&lt;/strong&gt;. By default then create a post with the name &lt;strong&gt;&lt;em&gt;year&lt;/em&gt;-&lt;em&gt;month&lt;/em&gt;-&lt;em&gt;day&lt;/em&gt;-&lt;em&gt;title&lt;/em&gt;.html&lt;/strong&gt; (e.g. for this very page &lt;strong&gt;2011-02-12-blogging-with-github-pages.html&lt;/strong&gt;):
&lt;/p&gt;

&lt;code&gt;
	&lt;pre class=&quot;brush: xml&quot;&gt;---
layout: default
title: My First Blog Post
---
&amp;lt;This is my first blog post, isn&#39;t it amazing?&amp;gt;&lt;/pre&gt;
&lt;/code&gt;

&lt;p&gt;Push the new page onto the server and wait for it to recompile:&lt;/p&gt;

&lt;code&gt;
	&lt;pre class=&quot;brush: plain&quot;&gt;$ git add _posts/2011-02-12-My-First-Blog-Post.html
$ git commit -m &#39;Adding in layout support&#39;
$ git push origin master&lt;/pre&gt;
&lt;/code&gt;

&lt;h3&gt;Summary&lt;/h3&gt;

&lt;p&gt;
	So there it is, you are now set up to build your blog, there is a lot more to play with but the basics are there. I will be taking this further in a number of posts to come but this is enough to show you how simple the basics are and how easy things are to get going.
&lt;/p&gt;</content>
    </entry>
  
    <entry>
      <title>Adding A Twitter Feed</title>
      <link href="http://ninjaferret.github.com/2011/02/12/Adding-Twitter-Feed.html"/>
      <updated>2011-02-12T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2011/02/12/Adding-Twitter-Feed</id>
      <content type="html">&lt;p&gt;
In my post &lt;a href=&quot;/2011/02/12/blogging-with-github-pages.html&quot;&gt;blogging with github pages&lt;/a&gt; I focussed on the initial steps of creating a new blog using &lt;a href=&quot;http://pages.github.com&quot;&gt;GitHub Pages&lt;/a&gt; but there is more to a blog than simply a series of HTML pages. One of the things that I was looking for was to be able to display some of my more recent tweets to the outside world.
&lt;/p&gt;
&lt;p&gt;
I decided that I would maintain the tweets in a side-bar and then I added in the references to the twitter api into my default layout:
&lt;/p&gt;

&lt;code&gt;&lt;pre class=&quot;brush: xml&quot;&gt;&amp;lt;html&amp;gt;
	...
	&amp;lt;body&amp;gt;
	   	...
	   	&amp;lt;div id=&quot;twitter&quot;&amp;gt;
			&amp;lt;h3&amp;gt;Twitter: (&amp;lt;a href=&quot;http://twitter.com/ijohnson_tnf&quot;&amp;gt;@IJohnson_TNF&amp;lt;/a&amp;gt;)&amp;lt;/h3&amp;gt;
 			&amp;lt;ul id=&quot;twitter_update_list&quot;&amp;gt;&amp;lt;/ul&amp;gt;
		&amp;lt;/div&amp;gt;
		...
	&amp;lt;/body&amp;gt;
	&amp;lt;script src=&quot;http://twitter.com/javascripts/blogger.js&quot; 
	  	type=&quot;text/javascript&quot;&amp;gt;&amp;lt;/script&amp;gt;
	&amp;lt;script
		src=&quot;http://twitter.com/statuses/user_timeline/ijohnson_tnf.json?callback=twitterCallback2&amp;count=5&quot;
	 	type=&quot;text/javascript&quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;/html&amp;gt;&lt;/pre&gt;&lt;/code&gt;

&lt;p&gt;This uses the browser to render a list of tweets limited by the number specified in the scripts (currently 5) which means that you don&#39;t have to generate that list server-side.&lt;/p&gt;

&lt;p&gt;There is something that I did not like about the contents of the twitter javascript file (&lt;a href=&quot;http://twitter.com/javascripts/blogger.js&quot;&gt;blogger.js&lt;/a&gt;) which was that there was specific formatting embedded in the generated HTML that was overriding my javascript, namely the size of certain links was set at 85%. In general, I would recommend that you take a look at using this file as a template for your own tweet rendering javascript file because then you are in control of how your tweets look. This is my &lt;a href=&quot;https://github.com/ninjaferret/ninjaferret.github.com/blob/master/assets/scripts/rendertweets.js&quot;&gt;javascript code for rendering tweets&lt;/a&gt;.  

&lt;p&gt;It&#39;s as easy as that, in no time at all you have a twitter feed on your blog.&lt;/p&gt;</content>
    </entry>
  
    <entry>
      <title>DDD9</title>
      <link href="http://ninjaferret.github.com/2011/02/01/ddd9.html"/>
      <updated>2011-02-01T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2011/02/01/ddd9</id>
      <content type="html">Firstly I have to say massive thanks to the organisers of &lt;a href=&quot;http://developerdeveloperdeveloper.com/ddd9/&quot;&gt;DDD9&lt;/a&gt; who, once again, have put on a fantastic free conference for us and thanks to the speakers who put their time and effort into the sessions. 

So what did I go to see:

&lt;h3&gt;Functional Programming in C#&lt;/h3&gt;
by Oliver Sturm

Oliver talked us through some of the basic principles of the functional paradigm, he talked about &lt;strong&gt;Closures&lt;/strong&gt;, &lt;strong&gt;Functional composition&lt;/strong&gt; and even &lt;strong&gt;Currying&lt;/strong&gt;. The focus of the talk was on how we can apply the functional principles within C# and how many functional concepts have been brought into the language (LINQ being the most obvious).

The shape of the code that we would write would be significantly different if we approached C# in function manner and there is one thing that I personally would hate which is the proliferation of generic parameters:

Func&lt;T1, T2, T3, T4&gt; func = (a, b, c) =&gt; (a * b) + c;
Func&lt;T1, T2, T3, T4&gt; curriedFunc = a =&gt; b =&gt; c =&gt; f(a, b, c); 

This is why I much prefer F#, because it has been designed to be functional and dynamically typed rather than designed to be a static language but Oliver really did explain a lot of the features of the functional paradigm that will make me think about how I approach my C# development. 

&lt;h3&gt;CQRS, Fad or Future?&lt;/h3&gt;
by Ian Cooper

I have been a fan of the &lt;strong&gt;CQRS&lt;/strong&gt; pattern for some time but I thought that it would be good to hear someone else&#39;s opinions and see if Ian was going to give us a pure, simplest definition, and not make the mistake of over-complicating it. Thankfully, Ian takes the same approach to &lt;a href=&quot;http://blog.ninjaferret.co.uk/?p=184&quot;&gt;CQRS&lt;/a&gt; as I do, that is a fundamentally simple pattern separate your &lt;strong&gt;reads&lt;/strong&gt; from your &lt;strong&gt;writes&lt;/strong&gt;.

For me this should be how we split the interfaces, we have two interfaces into a domain that split these two fundamentally different aspects of the domain. They are then free to use simple models that are focussed on either commands, or focussed on the queries that are needed within the system. Each interface can be then scaled and secured independently. 

Ian told us what CQRS is definitely not, it is definitely not &lt;strong&gt;Event Sourcing&lt;/strong&gt; and nor is it &lt;a href=&quot;http://blog.ninjaferret.co.uk/?p=221&quot;&gt;Domain events&lt;/a&gt; though he touched very briefly on these topics. In my opinion the Domain events are the way in which domains interact with each other, they are almost integration level commands, whereas the Commands we normally talk about are externally driven.

&lt;h3&gt;HTML5 Boilerplate&lt;/h3&gt;
by Dan Maharry

The &lt;a href=&quot;http://html5boilerplate.com/&quot;&gt;HTML5 Boilerplate&lt;/a&gt; is the collective work of a number of web developers who have built a template that will provide a basic structure for almost any site. I think that this is a fantastic tool that we should hook into because it already provides standards for handling cross-browser Javascript and CSS support to your tools.

&lt;h3&gt;Grok Talks&lt;/h3&gt;

&lt;h4&gt;Sharepoint and Visio 2010&lt;/h4&gt;
by Dave McMahon

This is a talk that is a shortened version that I have seen at &lt;a href=&quot;http://nxtgenug.net&quot;&gt;nxtgen user group&lt;/a&gt; and it concerns the ability to expose the data bound visio diagrams through sharepoint onto the web using Silverlight controls. 

&lt;h4&gt;CUDA&lt;/h4&gt;
by Rob Ashton

CUDA is a framework for running additional processing on the GPU. It seems like a very cool framework with a very odd syntax, lots of angle brackets, I won&#39;t go into details but &lt;strong&gt;Add&lt;&lt;&lt;1,1&gt;&gt;&gt;(4, 6, output) &lt;/strong&gt; is a little excessive.

&lt;h4&gt;IronRuby&lt;/h4&gt;

IronRuby is a ruby implementation based on the Dynamic Language Runtime and we were given a brief but very interesting demonstration of IronRuby running on the Windows Phone 7, being dynamically changed on the phone and then running. This is a fantastic way to update your apps without having to submit to regular reviews on the app store, just download new IronRuby files and you&#39;re done.

&lt;h3&gt;Is Your Code Solid?&lt;/h3&gt;

Once again this was a demonstration of the SOLID principles that every developer should be aware of and the following design smells that probably mean that our code isn&#39;t solid:

&lt;ul&gt;
  &lt;li&gt;Rigidity - Every change causes everything else in the system to change&lt;/li&gt;
  &lt;li&gt;Fragility - Changing X will break Y&lt;/li&gt;
  &lt;li&gt;Immobility - I can see a way to re-use X but can&#39;t re-use it because it would break Y =&gt; copy and paste&lt;/li&gt;
  &lt;li&gt;Software Viscosity - it takes too much time to &quot;to the right thing&quot;, the hack is cheaper&lt;/li&gt;
  &lt;li&gt;Environmental Viscosity - external forces stop us &quot;doing the right thing&quot;&lt;/li&gt;
  &lt;li&gt;Needless complexity - this really hides the intent of the code&lt;/li&gt;
  &lt;li&gt;Needless repetition - copied code is everywhere&lt;/li&gt;
  &lt;li&gt;Opacity - this is linked to needless complexity but the code is difficult to maintain and understand&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;From .NET to Rails&lt;/h3&gt;
by Colin Gemmell

This was a great talk that took us through Colin&#39;s journey into becoming a ruby developer. Starting from using IDEs that try to emulate the eclipse/visual studio environment including intellisense, which is extremely difficult for a language such as Ruby where to work out what methods are available you have to execute the code, to a basic overview of monkey patching and the problems that it holds. 

Colin took us through the SOLID principles and which ones were applicable to ruby programs. 

This was a refreshingly honest talk, there were problems with ruby but there were advantages to ruby such as the fact that the conventions involved in Rails development allowed for extremely rapid development but they then often caused other problems when it came to scaling of systems and the Active Record pattern. Be sure that you know how many database calls you are making when you are writing your ruby classes, you may be surprised.

We also got a brief look at deployment with Ruby, which, once again really does put the .NET world to shame but this is something that we could learn from and do properly, find an architectural pattern and build templates, packages etc that will allow us to rapidly build our applications.

&lt;h3&gt;Continuous Integration&lt;/h3&gt;

The final talk I attended as about continuous integration. The purpose of continuous integration is to assist the developers in producing better software, it was a good talk that focussed on a practical demonstration of TeamCity. 

There were a number of points that I picked up on including the idea that the build environment is something that should be clean, that should be a blank slate and that we should make sure that our systems build on a blank slate... checkout and build... 

The ultimate goal would be to end up in a place where we have sufficient automated testing, sufficiently frequent buildings and sufficiently short iterations that we could build and deploy our code from our build server.

&lt;h3&gt;And finally...&lt;/h3&gt;

I would heartily recommend these events to anyone who is interested in development, it is an amazing conference with amazing people and once more many thanks to the organisers, speakers and attendees for making it an education and a pleasure. 
</content>
    </entry>
  
    <entry>
      <title>What have you learned today?</title>
      <link href="http://ninjaferret.github.com/2010/11/27/what-have-you-learned-today-.html"/>
      <updated>2010-11-27T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2010/11/27/what-have-you-learned-today-</id>
      <content type="html">&lt;h3&gt;Software development is about learning/discovery&lt;/h3&gt;

Most people think that software development is about producing a piece of code to fullfil a task, but is it really that simple? I am increasingly agreeing with the BDD community that software development is about learning, how can developers develop something to solve a business problem without understanding that problem? 

Mary Poppendieck in &lt;a href=&quot;http://www.amazon.co.uk/Lean-Software-Development-Agile-Toolkit/dp/0321150783&quot;&gt; Lean Software Development&lt;/a&gt; states:

&lt;blockquote&gt;
Development is quite different than production. Think of development as creating a recipe and production as following the recipe... In fact, the whole idea of developing a recipe is to try many variations on a theme and discover the best dish.
&lt;/blockquote&gt;

I think that this is an interesting analogy because the software is actually the thing that facilitates the task that the user wishes to do so you could say it gives them the recipe to be efficient at their task. When it comes to software the &quot;production&quot; is actually using the software not building it, the building of the software is the development. Going back onto the last few words of her quote Mary is putting the learning at the heart of the development process.

I actually think that it is not just the developers who learn, if you are talking regularly with the customer then they are seeing what you are developing and they will change their minds, what they thought would work well may not work well at all when you come to develop it and they are learning about themselves, their needs and perhaps the needs of their customers. 

&lt;h3&gt;How do we learn?&lt;/h3&gt;

While we can learn through reading up on a subject and thinking more around it but the majority of discovery and learning comes through practice. You can teach me a subject and I will pick up on the key points but when it comes down it apply it is when I truly start grasping the concepts. 

Some of the greatest discoveries in science are not done through someone planning out all of the experiments that they are going to do and knowing what they will discover, people try something, measure it and then go &quot;Oh... that wasn&#39;t what I expected to happen&quot; then they figure out why. If Alexander Fleming had not looked at his &quot;ruined&quot; experiment and gone &quot;why is it that those areas are clear of bacteria?&quot; then he would not have discovered penicillin but that certainly did not come from a well thought out plan.

When things go wrong is when we can learn the most so perhaps the best way to learn is just to let things go wrong, inspect and adapt. Our learning cycle should really be plan, do, learn and adapt but the key is to reduce the time of that cycle, the shorter the period of learning the better.



&lt;h3&gt;Solving problems not providing solutions&lt;/h3&gt;

So, with the idea that we are constantly learning and that the software we develop is an evolving tool that reflects our current learning and assists further learning then what do we do with it?

We use this learning not to just implement what the customer has asked for, we use the tools that we have in our armoury to get down to the root problem we are trying to solve. Quite often people will come to me with specific solutions to a perceived problem but actually the problem is very different. 

I have recently worked on a project where I was asked to modify an integration system so that it would store two extra dates that would mark the cut-off point for certain activities, we would need to default them when importing data from a new system, provide a user interface for the users to be able to set the date. These dates would be sent to a third party who would act on them accordingly but could only be sent once so the user interface would have to then disable the fields once the data had been sent. Rather than just implementing the solution I talked to them about the problem and we dug only slightly deeper and the root problem was that currently they set up calendar entries and someone manually would go through turning off the features in the third party product on the appropriate date, which as time consuming. After discussions we realised that the defaults would be more than enough because they worked for over 90% of the cases and it would be a much smaller task to deal with the exceptional cases manually. The user interface disappeared, we reduced the amount of work the users had to do (they did not have to set these dates) and it kept the domain from leaking into the integration layer, everyone is happy.

&lt;h3&gt;Ignorance and product delays&lt;/h3&gt;

It is very rare to find a project that runs smoothly, where there has not been a single constraint impacting on scope, delivery or technology. There are always problems in any suitably complex software system but the problem is no matter how hard we think of the problem, no matter how much we have learned, we will be ignorant of the things that we do not know (obvious isn&#39;t it?).

&lt;h4&gt;This time we will know better&lt;/h4&gt;

Well, you&#39;ve worked on a project in this domain before and you&#39;ve had a few problems in the previous projects but by now you&#39;re up to speed on the solution aren&#39;t you? You now know all of the problems that you are likely to encounter, don&#39;t you?

We will always assume that we do so you say &quot;This time it will be different&quot; and &quot;this time we will come in on time and on budget&quot;, what could possibly go wrong?

We are pre-programmed to think in this way to assume that we are now completely prepared for something and that anything that subsequently goes wrong is &quot;bad luck&quot; but yet anyone else failing we often say that they were &quot;unprepared&quot; for the task. 

More than likely the project will go wrong due to some constraint that we have not previously encountered and mitigated in our processes. 

&lt;h4&gt;Ignorance reduces in steps&lt;/h4&gt;

What happens in a project when we encounter a problem? What is the first thought? &lt;strong&gt;Oh!!!&lt;/strong&gt; usually followed by a lot of swearing because we have just encountered a constraint that has invalidated out thinking. We delve deeper into the constraint and eventually we find a solution to that constraint. A bit later we get one of those &lt;strong&gt;Oh!!!&lt;/strong&gt; moments again.

So, even though we don&#39;t know what is going to trip us up how can we actively seek out those constraints? Probably the fastest way is to actually try, develop something to solve the problem and see what arises, learn from the problems that you face and adapt.

&lt;h3&gt;Deliberate Discovery&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#deliberatediscovery&quot;&gt;Deliberate Discovery&lt;/a&gt; how to reduce the ignorance of your team and deliver better software&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a name=&quot;deliberatediscovery&quot;&gt;Deliberate Discovery - Dan North&lt;/a&gt;&lt;/h3&gt;

Why do projects always seem to drag on behind schedule? There always seems to be something to trip us up and delay us. Dan argues that it is not anything overtly technical that causes the constraint but ignorance, basically we cannot know what we cannot know.

We are in general ignorant of:

&lt;ul&gt;
  &lt;li&gt;The domain we are working in (we aren&#39;t domain experts after-all)&lt;/li&gt;
  &lt;li&gt;The nature of the problem we are trying to solve (often people come to us with their concept of a solution rather than with the real problem)&lt;li&gt;
  &lt;li&gt;The current software, rarely are we in a position to have written all of the software we need to change&lt;/li&gt;
  &lt;li&gt;New technologies that may solve the problems as the world changes so rapidly these days&lt;/li&gt;
  &lt;li&gt;Organisational constraints that cause blockages&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Accidental Discovery&lt;/h4&gt;

As good as we are at software development, or as good as we are at analysis, there is the problem that we cannot know which of the constraints listed above (or even constraints that I haven&#39;t mentioned here) we are going to encounter, we do not know the details so we cannot plan for them. 

These constraints stalk us throughout a project and at the appropriate time they pounce and impede us. At that point the nice project plan might as well be thrown out of the window as we have to now abandon the path we were currently taking, or spend days/weeks working around a new impediment. This is accidental discovery, we discover too late the problems that we are facing.

&lt;h3&gt;Ignorance&lt;/h3&gt;

So, assuming we have finished our first project late and over-budget but we learned a lot aout 





</content>
    </entry>
  
    <entry>
      <title>Barcamp London 8</title>
      <link href="http://ninjaferret.github.com/2010/11/17/barcamp-london-8.html"/>
      <updated>2010-11-17T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2010/11/17/barcamp-london-8</id>
      <content type="html">Last weekend I got to attend the delayed &lt;a href=&quot;http://eight.barcamplondon.org/&quot;&gt;BarCamp London 8&lt;/a&gt; and had a fantastic time meeting people from a variety of backgrounds and people from across Europe. About 250 people descended on London for BarCamp bringing with them their thoughts, ideas, topics for sessions, computers, guitars, amps etc

I have to say a massive thank you to all of the organisers and crew of BarCamp for a fantastic weekend, they arranged the venue, got loads of sponsors, tidied up after us, arranged our meals and generally ran around constantly to keep the event running smoothly. Huge thanks has to go to the City of London University for providing us one of their buildings for several days, letting us stay overnight and trusting us not to destroy everything, it was a great venue for a conference as there were plenty of lecture rooms for presentations with all the equipment anyone could need. Also, thanks to the sponsors who made the conference possible, the event was totally free and we were provided with all our meals, coffee, drink, our personalised t-shirts from &lt;a href=&quot;http://spreadshirt.co.uk&quot;&gt;spreadshirt&lt;/a&gt; and other freebies. Finally, I have to thank everyone who attended, an unconference only works because of those who attend, who give sessions and contribute in every way.

For those of you who do not know what Bar Camp is then you might already have got the idea by now, a horde of people descend on a venue for a weekend&#39;s unconference and by that I mean that there is no set agenda, you do not know what topics will be discussed until you are there. The day is broken up into segments of 20 to 30 minutes and people use the advanced technology of pieces of paper with their ideas written on it that they just stick in a particular time slot in a particular room (which is interesting to see how popular people think their session will be by how large the room is that they have chosen). The topics, therefore, are particularly random and vary from how electronic guitars work, The Secret Life of Bees, British Sign Language, Cupcakes for Geeks to more technical talks on HTML 5, Security and the new &lt;a href=&quot;http://openwrap.org&quot;&gt;OpenWrap package management system&lt;/a&gt;.

Here is a very brief, and incomplete, list of everything I remember:

&lt;h3&gt;1. &lt;a href=&quot;http://lanyrd.com/2010/barcamp-london-8/srxg/&quot;&gt;Windows Phone 7 Development&lt;/a&gt;&lt;/h3&gt;

This talk was given by Jess Meats (hope I got the name right) about how easy and simple it is to develop for Windows Phone 7. She admits that she is not a developer but she has developed a &quot;build your own adventure&quot; game for Windows Phone 7 and talked us through how easy she found it. 

The tools are free to download and provide you with a simple mechanism for writing apps in Silverlight and writing graphical games in XNA.

There were a number of people who had experienced problems with registering for the APP HUB, the process of registering with GeoTrust seems to be unexplained and confusing to people with some people needing a notary to sign their documents (other people don&#39;t seem to have needed that so far) and in general the Terms and Conditions changing on XBox Live confused a few people as you needed to visit that site to make anything work. 

One other really good hint for people who run Windows on a VM on Mac - Parallels will not allow you to run the emulator because it is a VM inside a VM but somehow VMware Fusion will let you.

&lt;h3&gt;2. &lt;a href=&quot;http://lanyrd.com/2010/barcamp-london-8/srtm/&quot;&gt;Simple.Data&lt;/a&gt;&lt;/h3&gt;

&lt;a href=&quot;http://twitter.com/markrendle&quot;&gt;Mark Rendle&lt;/a&gt; presented his take on how to make data access easy. Microsoft have released their &quot;Microsoft.Data&quot; assemblies to try to make data access simpler to attract PHP developers onto the Microsoft platform, however, most .NET developers really do not like seeing SQL queries embedded in their web page. 

Mark&#39;s elegant solution is &lt;a href=&quot;https://github.com/markrendle/Simple.Data&quot;&gt;Simple.Data&lt;/a&gt; which is a dynamic library that given a connection string will find the meta data for the database and give you a fluent (and safe) interface onto the data so rather than:

&lt;blockquote&gt;database.Execute(&quot;SELECT * FROM BlogPosts WHERE Id = 12&quot;)&lt;/blockquote&gt;

You can do:

&lt;blockquote&gt;database.BlogPosts.FindById(12)&lt;/blockquote&gt;

This looks very cool and intuitive, it is missing intellisense so you need to know your database schema but is a nice light and extensible framework for building data access. Someone could easily build a twitter data source for this framework.

&lt;h3&gt;3. &lt;a href=&quot;http://lanyrd.com/2010/barcamp-london-8/srwf/&quot;&gt;How to work well with techies&lt;/a&gt;&lt;/h3&gt;

&lt;a href=&quot;http://twitter.com/uniquejosh&quot;&gt;Josh&lt;/a&gt; started what was my first &quot;discussion session&quot; where he wanted to find out from people how, as a business orientated person, he could better work with techies. How does a business person get the best out of their technical staff?

One of the key concepts that was emerging was the fact that there needs to be trust:

&lt;ul&gt;
  &lt;li&gt;The business need to have trust in their developers/techies, trust that things will get done, trust that they will ask rather than assuming&lt;/li&gt;
  &lt;li&gt;The developers/techies need to trust that the business have an idea about what they want, that they will be available when questions arise, that they will understand when we say something is difficult etc&lt;/li&gt;
&lt;/ul&gt;

How you build that trust is through good and regular communication, it is a team after-all and there should be collective responsibility. It is also important to recognise that not every developer has a uniform set of skills, strong points and weak points, some of the most amazing developers would never be able to hold a business-level conversation so then you need to have a broad range of skills on your team. &quot;Techies are people too&quot; was a phrase I came out with, we are all individuals and as such have different needs and different things to offer, getting to know your techies will allow you to understand more about how to work with them.

&lt;h3&gt;4. &lt;a href=&quot;http://lanyrd.com/2010/barcamp-london-8/srwz/&quot;&gt;Books for freaks&lt;/a&gt;&lt;/h3&gt;

A great discussion where people talked about the books they would be eager to recommend, people were great at selling the books because they were so passionate about them and there is an &lt;a href=&quot;http://www.amazon.co.uk/lm/R29F5NM5ZPV7QS/&quot;&gt;Amazon list&lt;/a&gt; of the books mentioned, though missing &lt;a href=&quot;http://www.amazon.co.uk/Jasper-Fforde/e/B000APXZAC/ref=sr_tc_2_0?qid=1290015897&amp;sr=8-2-ent&quot;&gt;Jasper Fforde&lt;/a&gt;.

I now have way too much to read.


&lt;h3&gt;5. &lt;a href=&quot;http://lanyrd.com/2010/barcamp-london-8/sryx/&quot;&gt;British Sign Language&lt;/a&gt;&lt;/h3&gt;

&lt;a href=&quot;http://twitter.com/lallyd&quot;&gt;Lalita D&#39;Cruze&lt;/a&gt; ran what was, in my opinion, the best session that I attended. It was a fantastic introduction to BSL and I came away from a 20 minute talk able to say &quot;Hello, my name is Ian. I work as a computer engineer&quot;, had a greater understanding of deaf etiquette and also had a fun time with some of the anecdotes that Lalita was passing on as she taught us. 

It was a session that left me buzzing and has been the session that I probably talked about most with my friends over the last few days so a big thank you to Lalita. I may even take up some lessons.

&lt;h3&gt;6. Evening...&lt;/h3&gt;

So with the sessions over for the first day we settled down to an evening of pasta, talking, quizzes, werewolf and most importantly drink. Sometime early Sunday I was involved in the first ever recording of the song Zombie Dawn to the &quot;Battle Hymn of the Republic&quot;  followed by watching the Hangover, some karaoke and then finally sleep for about 3 hours before giving up and deciding to stay awake.

&lt;h3&gt;7. &lt;a href=&quot;http://lanyrd.com/2010/barcamp-london-8/stcc/&quot;&gt;From Faraday to Fender: the Physics of the Electric Guitar&lt;/a&gt;&lt;/h3&gt;

This was another great talk about a subject I would never have gone into much detail about in my daily life. &lt;a href=&quot;@dylanbeattie&quot;&gt;Dylan&lt;/a&gt; took us through an entertaining journey from how guitar strings vibrate to produce their distinctive sound to the pick-ups to how the amps work and the history of the power chord. I don&#39;t think that I&#39;ll ever be in a mind to wind my own pickup but apparently 4 days of fun with a drill slowly winding a very fine thread can be a bit tedious and it&#39;s much less effort to buy them.

&lt;h3&gt;8. &lt;a href=&quot;http://lanyrd.com/2010/barcamp-london-8/stdf/&quot;&gt;OpenWrap: .NET package management&lt;/a&gt;&lt;/h3&gt;

&lt;a href=&quot;http://twitter.com/danielirvine&quot;&gt;Daniel&lt;/a&gt; took us thorough introduction to the &lt;a href=&quot;http://openwrap.org&quot;&gt;Open Wrap Package Management&lt;/a&gt; system for .NET written by a mutual friend &lt;a href=&quot;http://twitter.com/serialseb&quot;&gt;Sebastian Lambla&lt;/a&gt;. Taking your existing libraries and making a &lt;strong&gt;wrap&lt;/strong&gt; is as simple as:

&lt;blockquote&gt;
o init-wrap -all
&lt;/blockquote&gt;

And consuming &quot;wraps&quot; for other frameworks is also extremely simple. You now don&#39;t even need to do &quot;Add Reference&quot; within Visual Studio as OpenWrap handles all of this for you and the system will provide your tools with the dependencies they need. 

I am really excited about this and can see it being used not just for open source projects but for internal frameworks and potentially as part of a deployment solution. Since seeing this talk I have downloaded OpenWrap and am now looking at integrating this at work and for my own open source projects.

&lt;h3&gt;What did I present?&lt;/h3&gt;

Well I ran a session on &lt;a href=&quot;http://lanyrd.com/2010/barcamp-london-8/sryp/&quot;&gt;Kanban&lt;/a&gt; which seems to have gone down well with the people who were attending, I certainly had a few people talking to me throughout the day on the topics and it sparked some good discussions even if I wasn&#39;t as prepared as I should have been. 

I also co-hosted a session on &lt;a href=&quot;http://lanyrd.com/2010/barcamp-london-8/stdt/&quot;&gt;Agile! Dos and Don&#39;ts&lt;/a&gt; which was really an open forum discussion where people could get together and share their experiences, the good and the bad. It was a lively debate and it was very interesting to see just how much emphasis people are placing on trust throughout the agile process and how things can break apart when that trust is not there.

&lt;h3&gt;Summary&lt;/h3&gt;

I had a really good time at BarCamp and will be booking a place for the next one. I would recommend attending to anyone reading this (though get your tickets once I&#39;ve got mine) and I would also say staying the evening is well worth it, even if you are in London. Once more thank you to the organisers, sponsors and those who attended for making it such a fun (but tiring) weekend.
</content>
    </entry>
  
    <entry>
      <title>Domain Events</title>
      <link href="http://ninjaferret.github.com/2010/11/10/domain-events.html"/>
      <updated>2010-11-10T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2010/11/10/domain-events</id>
      <content type="html">&lt;h3&gt;Architecture Series Overview&lt;/h3&gt;

I have recently become increasingly interested in how we, as developers and architects, structure the systems that we are creating. The demands on on-line systems are significantly different to the demands on desktop only software. Scalability, robustness, performance, resilience are all important factors when thinking about on-line services and there has been a lot of debate within the software world about how we should architect these systems. These posts are my views of interesting architectural patterns and practices that I have been looking at:

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.ninjaferret.co.uk/?p=184&quot;&gt;CQRS: Keeping it simple&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Domain Events&lt;/strong&gt; is right here&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Domain Events Introduction&lt;/h3&gt;

In my previous post on software architecture on &lt;a href=&quot;http://blog.ninjaferret.co.uk/?p=184&quot;&gt;CQRS&lt;/a&gt; I talked about how we can apply a simple pattern for our domains with the outside world, the separation of the command from the query gives us many advantages but does not solve all of our problems. 

In this post I hope to briefly explain the architecture of how we can use Domain Events, some of the advantages of using them followed by some of the perceived complexities of using them. I have not gone into any technical implementation detail here but I will come onto this in a later post. 

&lt;h3&gt;Multiple Domains&lt;/h3&gt;

In a large enterprise there are going to be a number of domains but these domains do not sit in isolation as business processes sit on top of the domain. For example, in an on-line store you may have a &lt;strong&gt;supplier management domain&lt;/strong&gt;, a &lt;strong&gt;billing domain&lt;/strong&gt;, a &lt;strong&gt;warehouse management domain&lt;/strong&gt; and an &lt;strong&gt;order management domain&lt;/strong&gt; and business processes will cover all of these domains, for example (this is not an optimal workflow and would annoy most customers but bear with me for this example):

&lt;blockquote&gt;
When someone places an order it is sent to the warehouse for processing and the user is billed. In the warehouse, each item needs to be located and added to the package. Once billing and packaging is complete it can be dispatched to the customer. If the item is out of stock then we order more of the item from the supplier but if the supplier tells us that there is a long lead time for the item then we have to notify the customer and let them make a choice about whether to cancel or not, if they choose to cancel then we should refund them for the price of that item.
&lt;/blockquote&gt;

So how do we handle the communication between domains? Should the order domain know about the warehouse domain and issue commands? Well, if that was the only consumer of the order then I guess that would be OK but the billing system will also want to know about the order. Actually, they are both interested that an order has been placed but both will respond to that &lt;em&gt;event&lt;/em&gt; in different ways. 

&lt;h3&gt;Transferring data between domains&lt;/h3&gt;

When we are looking at our domains we have a number of points where something has happened in our domain that has an effect on the processing within other domains so how can we handle this?

&lt;ul&gt;
  &lt;li&gt;Have a single database where every domain is always up to date (though we can run into scalability and performance issues here)&lt;/li&gt;
  &lt;li&gt;Have some form of data transfer between the different databases that then goes away behind the scenes manipulating data in the destination systems&lt;/li&gt;
  &lt;li&gt;Raise an event and the interested domains can consume that event&lt;/li&gt;
&lt;/ul&gt;

So when our order is placed and the &lt;em&gt;orders domain&lt;/em&gt; has done all of the checks that it needs then it will raise an &lt;strong&gt;New Order Event&lt;/strong&gt; and publish that to the outside world. Any domain can then receive and act based upon that event. In my example so far the warehouse management system will compile the list of items, work out how big a package they should go in, check the stock levels and create a work order for someone to process the order and that order goes on to the queue. At the same time the billing system is retrieving the customer&#39;s details and charging them for the total amount. 

So what events would be needed to satisfy the example above?

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Order Placed Event&lt;/strong&gt; from the &lt;em&gt;orders domain&lt;/em&gt; - consumed by the &lt;em&gt;billing domain&lt;/em&gt; and the &lt;em&gt;warehouse domain&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Payment Completed Event&lt;/strong&gt; from the &lt;em&gt;billing domain&lt;/em&gt; - consumed by the &lt;em&gt;warehouse domain&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Missing Stock Event&lt;/strong&gt; from the &lt;em&gt;warehouse domain&lt;/em&gt; - consumed by the &lt;em&gt;supplier management domain&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stock Estimated Arrival Event&lt;/strong&gt;t from the &lt;em&gt;supplier management domain&lt;/em&gt; - consumed by the &lt;em&gt;orders domain&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cancel Order Item Event&lt;/strong&gt; from the &lt;em&gt;orders domain&lt;/em&gt; - consumed by the &lt;em&gt;warehouse domain&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

I think that there are other events, even if they are not currently consumed in the process, that may need to be raised (e.g. when a payment fails) which we may need to trap. Perhaps the greatest difficulty in this pattern is knowing what events people are going to need to isolate yourself from needing to change when a new system comes along.

&lt;h3&gt;What are the benefits?&lt;/h3&gt;

There are many of you out there at the moment who are going &quot;why don&#39;t you just have an ETL task to populate related databases with the appropriate information?&quot; and that is a valid pattern to use, so the question becomes why use Domain Events?

&lt;h4&gt;1. Extensibility&lt;/h4&gt;

Let us say that sometime in the future I want to send a printed invoice to the customer independent of the actual package when an order has been paid, this is a little contrived but bear with me, then using most other techniques I have to write an entire new integration system. With Domain Events I already have all of the integration points that I need because I can subscribe to the &lt;strong&gt;Payment Completed Event&lt;/strong&gt; and  retrieve the order information from the order query service (using &lt;a href=&quot;http://blog.ninjaferret.co.uk/?p=184&quot;&gt;CQRS&lt;/a&gt;) then print out the invoice and send it.

Another approach would be to prepare the invoice when the &lt;strong&gt;Order Placed Event&lt;/strong&gt; is received but only send it when the &lt;strong&gt;Payment Completed Event&lt;/strong&gt; is received but delete the invoice if the &lt;strong&gt;Payment Failed Event&lt;/strong&gt; is received. 

&lt;h4&gt;2. Implementation Isolation&lt;/h4&gt;

Each implementation of your system is completely isolated from every other implementation. Using Domain Events if I choose to implement a new billing system, using a different language or even using a different database I can do without having to change anything but that domain. The inputs to the domain are known, the Commands from &lt;a href=&quot;http://blog.ninjaferret.co.uk/?p=184&quot;&gt;CQRS&lt;/a&gt; and the &lt;strong&gt;Domain Events&lt;/strong&gt; from the other systems. Also my outputs are well defined, my outputs are the &lt;strong&gt;Domain Events&lt;/strong&gt; and the Queries from &lt;a href=&quot;http://blog.ninjaferret.co.uk/?p=184&quot;&gt;CQRS&lt;/a&gt;.

Using ETL etc then we are having to do more work in the background to move data into this new system, into the new data structure, rather than forcing the outside world to conform to the technical implementations of the domain using &lt;strong&gt;Domain Events&lt;/strong&gt; we are forcing ourselves to comply with our own domain.

&lt;h4&gt;3. Logging&lt;/h4&gt;

By definition an event within a domain is something that is extremely important and should contain a lot of important information about the action that has just transpired. Logging at this level means that we are recording all of the important information that is happening within the system and we can do this without actually needing to write much specific logging, we can create a logging component/domain that subscribes to these events and records them. 

&lt;h4&gt;4. Design and Documentation&lt;/h4&gt;

I know, this is something that every developer finds dull and boring, but when you are designing your systems and how they interact you are now thinking in terms of the events that each domain is raising and consuming and this will give us a very simple way to document our business processes at a high level.

&lt;cite&gt;
&lt;strong&gt;ORDERS DOMAIN&lt;/strong&gt;
&amp;nbsp;&lt;em&gt;Raises: Order Placed Event&lt;/em&gt;
&amp;nbsp;&amp;nbsp;Consumed by:
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;- Warehouse Domain
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;- Billing Domain
&amp;nbsp;&lt;em&gt;Raises: Cancel Order Item Event&lt;/em&gt;
&amp;nbsp;&amp;nbsp;Consumed by:
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;- Warehouse Domain
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;- Billing Domain
&amp;nbsp;&amp;nbsp;&lt;em&gt;Consumes: Stock Estimated Arrival Event&lt;/em&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;- Gives user the chance to cancel
&lt;/cite&gt;
&lt;cite&gt;
&lt;strong&gt;BILLING DOMAIN&lt;/strong&gt;
&amp;nbsp;&lt;em&gt;Raises: Payment Completed Event&lt;/em&gt;
&amp;nbsp;&amp;nbsp;Consumed by:
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;- Warehouse Domain
&amp;nbsp;&lt;em&gt;Consumes: Order Placed Event&lt;/em&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;- Bills the user and raises the Payment Completed Event
&amp;nbsp;&lt;em&gt;Consumes: Order Cancelled Event&lt;/em&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;- Issues a refund back to the user
&lt;/cite&gt;

This is not the entire documentation for the process defined but it helps to show how easy it is to document and hopefully how easy it is to follow. We will have to build this documentation anyway as we are designing our systems and there is little there that we could not take directly to the users and discuss with non-technical business experts.

&lt;h3&gt;Areas of concern?&lt;/h3&gt;

It all sounds extremely simple doesn&#39;t it? So where does the complexity lie?

There are a number of different problems that happens when we get into using domain events. The way that we design our systems needs to change and we need to take into account these concepts when we are designing our systems.

&lt;h4&gt;1. Reliable Messaging&lt;/h4&gt;

We are busy raising events in our systems and assuming that they will reach all of the consumers of those events. How can we be sure that this is the case? This is where reliable messaging comes into the frame. 

A reliable messaging system, such as &lt;a href=&quot;http://nservicebus.com/&quot;&gt;NServiceBus&lt;/a&gt;, will ensure that when you publish a message it will ensure that all messages will be delivered to the consumers. The delivery mechanism used by NServiceBus is to use a queuing mechanism allowing the receiver to pull the messages at a speed that they can cope with. One way to guarantee delivery is that taking the item from the queue and the end of the process is maintained in a distributed transaction so that should any component fail the message ends up back on the queue to be retrieved again at a later date.

However, another mechanism that could be applied is for each domain to send a standard acknowledgement of the event and reports if an error has occurred or a message has disappeared from the queue without being acknowledged then the event is replayed into the system. 

&lt;h4&gt;2. Eventual Consistency&lt;/h4&gt;

In a traditional system, where all processing is completed in a single transaction, we are separating out the processing, distributing it between domains so there is a temporal cost to the processing. The data across all domains may not be consistent at the very moment that the data returns to the user, the data is being processed in the background and will eventually be consistent. 

What does this mean? If I am running a website and the user has just submitted some data then when the page refreshes the data may not appear for the customer, a few seconds/minutes later the page may accurately display the current state of the system. One suggestion for this is that the user interface uses a bit of trickery and actively fools the user for a few seconds by making it appear that their command has already been processed. 

&lt;h4&gt;3. How do I handle queries?&lt;/h4&gt;

In looking at the &lt;strong&gt;Domain Events&lt;/strong&gt; they look very similar to the Commands from CQRS, I think that the main difference between them is that the &lt;strong&gt;Domain Event&lt;/strong&gt; is driven by internal forces within the organisation whereas the commands are external, from users or even from third-party applications.

So what happens about queries? Well, the domains should contain enough information within themselves to successfully process the command but the domains are still exposing queries to the world for external systems to communicate with them. There is nothing to stop the generation of new composite services that will gather data from several different domains and present a unified query model.

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;strong&gt;Domain Events&lt;/strong&gt; provide us with a loose coupling mechanism for communication between domains. They allow us to define our interactions, our inputs and out outputs that are focussed on the delivery of the domain, hopefully keeping the domain simple and easy to implement/re-implement as necessary. They work well with CQRS, which says that the commands should be split from the queries, but does not immediately handle how communication should occur between the domains whereas &lt;strong&gt;Domain Events&lt;/strong&gt; focusses us on how the domains interact.
</content>
    </entry>
  
    <entry>
      <title>Registering for Windows Phone 7 Development</title>
      <link href="http://ninjaferret.github.com/2010/11/08/registering-for-windows-phone-7-development.html"/>
      <updated>2010-11-08T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2010/11/08/registering-for-windows-phone-7-development</id>
      <content type="html">This should be a short post about my experiences registering for the Microsoft App Hub so I can develop some applications for Windows Phone 7. I&#39;m working on my first XNA game at the moment and have a few other application ideas that I need to develop over time so I thought that I would start registering in advance as an incentive to actually getting things done.

&lt;h3&gt;Signing-up&lt;/h3&gt;

Signing-up is really quite easy, simply go to http://create.msdn.com/en-US/ and follow the process to join. You will need:

&lt;ol&gt;
  &lt;li&gt;A Windows Live Id - I chose to register with my existing one but you may want to create a specific account just for managing your account&lt;/li&gt;
  &lt;li&gt;£65 to register - though free if you&#39;re a student&lt;/li&gt;
  &lt;li&gt;Details of your/your company&#39;s address etc&lt;/li&gt;
&lt;/ol&gt;

Following the instructions, I thought, that was easy... I received two e-mails:

&lt;ul&gt;
  &lt;li&gt;from the Microsoft App Hub with an e-mail verification link&lt;/li&gt;
  &lt;li&gt;from Microsoft&#39;s verification partner GeoTrust with a link to one of their pages. You have the option now to Approve of Reject your application.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Identity Verfication&lt;/h3&gt;

So that was it, wasn&#39;t it? Well, not really. I now had the ability to access the app hub and even though I was not ready I could submit an application etc though I was constantly being told that I was not verified yet.

It is not explained very clearly what the stages of verification are and I found out more by trawling the &lt;a href=&quot;http://forums.create.msdn.com/forums/&quot;&gt;forums&lt;/a&gt; and seeing that a number of people were as confused as me about the verification procedure. From what I can see you need to submit something before the next stage of verification occurs, even if you aren&#39;t ready to submit your app yet submit something partially and delete it later was the advice.

I tried that and waited for a while for someone to contact me about verification until I got bored (a couple of days) and I went onto the GeoTrust site and used their online support chat to talk to one of them who forwarded me an e-mail containing the next stage.

I had to send back a scan of a form which contained some government issued photo ID, passport or driving licence. Shortly after sending them the e-mail I was then verified and felt a lot calmer about things.

&lt;h3&gt;Bank Accounts&lt;/h3&gt;

I tried registering one of my bank accounts but unless I had a specific tax number the form would not submit. I believe that as a UK citizen I should to go to the US Embassy in London to deal with the IRS office there in order to get the forms appropriately filled in and sent off to the IRS to gain my ITIN (if you go remember to ensure that you take as much documentation as you can as they are probably going to want to verify identity etc).

Microsoft provide a letter that must be attached to the application and there are some UK residents who are saying that the IRS people at the embassy dislike the phrasing of that letter basically saying that you &quot;&lt;strong&gt;maybe&lt;/strong&gt; receiving a payment&quot; in the next financial year, which is true since if no one buys your app then you don&#39;t get paid. However, I think that Microsoft will have discussed this letter with the IRS and therefore the local embassy staff should let this through.

Having said all of this I have not yet done this, my app is still a few weeks away from being live on the market place and even then I am not guaranteed to get any returns on that app. If it looks like I am going to earn a significant enough amount then I will go to the trouble of getting a ITIN.

&lt;h3&gt;The XBox Live Gotcha&lt;/h3&gt;

At one point the APP HUB dashboard stopped working, every link would re-direct me back to the main page. I was getting highly frustrated and so were other people on the forum but then the answer came

I didn&#39;t have an XBox Live account before I got my phone, as strange as that may sound, but I got one when I got the phone. Shortly after that the terms and conditions on XBox Live changed but I had never been to that site so I had not accepted them. This was what was causing the dashboard to fail, I went to the XBox Live site and accepted the terms and conditions then shortly after the APP HUB dashboard started working again. 

&lt;h3&gt;Summary&lt;/h3&gt;

I don&#39;t think that this process has been that hard so far and I can understand all of the requirements and all of the steps that I have had to go through but I found the process fairly frustrating due to the lack of documentation that surrounds the process. I know that I am an impatient person at times and therefore maybe someone more patient would have fewer frustrations as things would naturally happen.

Still, can&#39;t wait to get my app on the market place! I&#39;ll blog about that soon.
</content>
    </entry>
  
    <entry>
      <title>DDD8a - 23rd October 2010</title>
      <link href="http://ninjaferret.github.com/2010/10/24/ddd8a-23rd-october-2010.html"/>
      <updated>2010-10-24T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2010/10/24/ddd8a-23rd-october-2010</id>
      <content type="html">Yesterday I attended a fantastic &lt;a href=&quot;http://developerdeveloperdeveloper.com/ddd8a/&quot;&gt;DeveloperDeveloperDeveloper Modern .NET Day&lt;/a&gt; in Reading, which did mean getting up at 5am to get the train over to Reading. I was at the DDD8 earlier in the year so I was eager to get it booked a few weeks ago when registration opened, which does beg the question why I am writing a blog post to publicise the events making it harder to register next time?

This was smaller than the main event earlier in the year (which is on 29th January next year) running only two tracks but there are so many fantastic speakers in the community that I always end up torn between sessions and yesterday was no different. I can only talk about the sessions that I attended so here goes...

&lt;h3&gt;Session 1: WP7, iPhone, Droid - Oh My! - by Chris Hardy (&lt;a href=&quot;http://twitter.com/chrisntr&quot;&gt;@chrisntr&lt;/a&gt;)&lt;/h3&gt;

There are a number of mobile operating systems out there at the moment and developing for them can be extremely tricky. However, with work that has been done on the development of &lt;a href=&quot;http://monotouch.net&quot;&gt;MonoTouch&lt;/a&gt; and the up-coming &lt;a href=&quot;http://monodroid.net/&quot;&gt;MonoDroid&lt;/a&gt; .NET is becoming a single platform that will allow us to develop cross-platform solutions. 

Chris talked us through the basics behind each of the different development environments. 

Windows Phone 7 is the environment that allows us to develop in the environment that we are generally most familiar with, Visual Studio 2010. This is extremely familiar to the Silverlight developers as developing for Windows Phone 7 is in Silverlight, we can design the interface within the XAML and Silverlight forces us to separate our user interface from the business logic. There are new controls for the phone, the &lt;em&gt;Panorama&lt;/em&gt; control and the &lt;em&gt;Pivot&lt;/em&gt; control that gives the phone its distinctive look and feel.

The next was MonoTouch using .NET to write onto the iPhone, and yes Apple have relented to allow MonoTouch applications to run on their phones. There is a 20MB limit to the size of applications that can be loaded to the AppStore and because libraries cannot be shared between applications each application needs its own copy of the runtime. MonoTouch does not have the full .NET API but has the libraries that people tend to use the most and this tends to be a super-set of the libraries available to Silverlight developers. You have to develop the user interface in Apple&#39;s Interface Builder and then hook your own business logic into the interface. Due to the constraints placed by Apple it is only really possible to develop and debug in Mac OS X as it is tied to the interface builder and the simulator.

A single development licence is $399 which gives you a licence to develop on that platform forever, however, you will need to renew each year in order to get continuing updates of the product.

Finally, MonoDroid is the equivalent of the MonoTouch for Google&#39;s Android Phone and is currently is in the preview (not alpha or beta) mode. The interface building tools for Android are less developed than for the other operating systems but again the interface is designed in XML. The Android development SDK is available to all operating systems and currently it MonoDroid is available for development within Visual Studio but eventually a MonoDevelop plugin will be developed. When it is released expect MonoDroid to have the same pricing model as MonoTouch.

The basics that I took from Chris was that, there may be some problems with a few libraries (Chris created a non-JIT version of the JSON parsing assembly) but each of three platforms separates out the interface from the business logic therefore make sure that when you are developing your projects it should be possible to maintain one set of business logic and have just different presentations of that logic.

&lt;h3&gt;Session 2: Packaging in the .NET World - by Sebastian Lambla (&lt;a href=&quot;http://twitter.com/serialseb&quot;&gt;@serialseb&lt;/a&gt;)&lt;/h3&gt;

Over the past few years the .NET community has been catching up with other areas of the software development world and has started producing its own open source software. Traditionally we either relied on Microsoft to develop everything that we needed or that other third parties would provide frameworks that you would buy in with guaranteed support.

With open source software we are building dependencies into our software on the open source frameworks that we use but these frameworks are constantly changing, constantly evolving and often have complicated dependencies. As Seb pointed out people often download a particular version of a framework and then stick to it regardless of whether a newer, better, version is available, why? They are afraid of breaking changes.

Seb took us though the history of the package management solutions that have been available within the open source world for a long time (e.g. &lt;em&gt;apt-get&lt;/em&gt;) through to the latest offerings from himself (&lt;a href=&quot;http://www.openwrap.org/&quot;&gt;Open Wrap&lt;/a&gt;) and Microsoft (&lt;a href=&quot;http://nupack.codeplex.com/&quot;&gt;NuPack&lt;/a&gt;).

The most recent history between Microsoft and Seb has been an interesting view from the outside. It was known that Seb was busy building a package manager but Microsoft were busy developing something of their own in secret and only when they had completed it did they announce it to the community. 

However, OpenWrap was the primary focus of the talk showing how easy it was for us to create and manage our dependencies. One advantage the NuPack has over OpenWrap at the moment is the integration with the Visual Studio - Seb will argue with me on this point but in order to get through to the standard developer eventually the integration will be required. From the command line I think that we have more power than we would through the Visual Studio interaction and OpenWrap has a distinct advantage over NuPack, it supports the NuPack format.

No longer do you need to download an assembly and do &quot;Add Service Reference&quot; as OpenWrap hooks itself into the build process and automatically references the appropriate packages. It also has a plugin for Resharper that allows Resharper to detect the files even though they are not referenced by the project. 

I think that OpenWrap is a very powerful and useful package manager and I am seriously looking forward to working with it.

&lt;h3&gt;Session 3: Is NoSQL the Future of Data Storage? - by Gary Short (&lt;a href=&quot;http://twitter.com/garyshort&quot;&gt;@garyshort&lt;/a&gt;)&lt;/h3&gt;

Gary was talking about the NoSQL is a movement that has risen over the last few years and is centred around the concept that Relational Databases are not tuned to being easily scalable, or even tuned to the demands of modern scalable systems. Twitter, Facebook and even Google have shunned standard relational databases in favour of other database models that are more tuned to their own needs. Gary started out by explaining that &quot;NoSQL&quot; really did mean &quot;Not Only SQL&quot; as there is never one absolute solution to any problem and relational databases do have their place in the world.

NoSQL databases come in varying shapes and sizes, object databases, graph databases and even document databases.

Gary talked us through why the traditional relational database came into being, normalised data saves space! One of the other concerns was that the data should be in the same place (on the same tape) which means that it could be read efficiently. In the 60s, 70s and 80s the space that was needed to store data was in a premium and therefore having a normalised database means that most data is stored in one place and one place only. However, there is a cost with a normalised database, every query that performs a join has to read data from several tables and combine them into a single results set.

Non-relational databases such as RavenDB work on a completely different model. RavenDB is a document database, which means it stores all the information relating to a document in a single place. This means that we have extremely fast writes (as the transaction is to write to that single place) and also we have extremely fast reads of the data because we have all the data that we require and the server is fairly dumb. For RavenDB we create our indexes in .NET using Linq and effectively create &quot;materialised view&quot; of the required records and can even render different object models using those views, these indexes are &quot;eventually consistent&quot; as when a change is written a background process on the server re-builds the indexes but that could take time. The only problem is the ad-hoc queries you can perform on SQL. The documents are denormalised and you are potentially storing more data but in these days do we care about that? It is so easy to shard the data using RavenDB that if we are stressing a single server whereas in a relational database model sharding the data is not that easy (and yes it is possible).

I think the simple conclusion is that we have to use relational databases for things that need to be consistent, transactional etc but in other cases where we need fast reads/writes, scalable then it is worth checking out the various types of Non-SQL databases out there.

&lt;h3&gt;Lunchtime Grok Talks&lt;/h3&gt;

As always there were some lunchtime Grok talks where people come forward to give a short presentation during lunch to the people attending the conference.

We had Guy Smith-Ferrier (&lt;a href=&quot;http://twitter.com/GuySmithFerrier&quot;&gt;@GuySmithFerrier&lt;/a&gt;) giving a 20/20 talk, which is 20 slides at 20s per slide, on why twitter is so great or rather &quot;Why using twitter will get you more sex&quot; or something like that. It was a very interesting and cool talk but for me there was one reason - so much is now being communicated using twitter.

There was a talk about using F# as a BDD framework with the &lt;a href=&quot;http://tickspec.codeplex.com/&quot;&gt;tickSpec&lt;/a&gt; framework. It is an interesting concept and deserves a further look into.

Finally we had Ross Scott talking about the costs involved with running on the Azure framework. One interesting point that was raised within this talk is that you really need to run two instances of your roles, primarily the web roles, as Microsoft can bring your role down to patch the virtual machine, or move the machine over to another part of the cloud where it is less busy. 

&lt;h3&gt;Session 4: Modern C#: This is not your grand-daddy&#39;s language by Jon Skeet (&lt;a href=&quot;http://twitter.com/jonskeet&quot;&gt;@jonskeet&lt;/a&gt;)&lt;/h3&gt;

Jon turned up to show us how C# has evolved over the years and to force us to think about what we want from our languages and where we could go in the future. Far more importantly he was joined by his trust companion &lt;em&gt;Tony the Pony&lt;/em&gt; who, according to Jon, isn&#39;t a very good developer...

There was a time warp back to the days when C# was new, when C# didn&#39;t have generics, or lambda expressions or linq. Back in those days did we think it was a horrible language to work within? Not really, as Jon pointed out C# is a good language and back in the day it was a nice easy language with a massively powerful framework behind it. Development was actually quite easy.

Along came generics... then came Linq and lambda expressions... did they made the language harder? No, they made it easier. It has become much easier to express intent with C# rather than focussing the implementation. Languages are evolving to focus more on what we are trying to do rather than how to do it and that is not a bad thing. Jon described this in terms of driving, the driver doesn&#39;t actually care about the mechanism that turns the wheels, they only care about their interaction with the steering wheel. 

The things that I will take away from this talk are to learn other languages and understand why they are different and what the advantages and disadvantages of each language is but also to read things that I know that I disagree with and understand why I disagree with it because in doing so I am learning more about my programming language, what I want from my programming languages and myself.

&lt;h3&gt;Session 5: WPF in Modern .NET - Ian Griffiths&lt;/h3&gt;

Ian spent a lot of time focussing on the controversy that has surrounded WPF in recent months. It has been suggested that WPF was dead but there is a massive effort going into WPF at the moment and as Silverlight is a sub-set of WPF (though implemented in a different way) and the new Windows Phone 7 makes use of Silverlight it is likely that WPF is going to be around for a while. 

There was a discussion of the Model View ViewModel pattern and how this can benefit Silverlight.

By this point I think that my mind had actually started to fade at this point and I can&#39;t remember too much about this talk (sorry Ian).

&lt;h3&gt;Geek Dinner&lt;/h3&gt;

So at the end of the day we drifted back into central Reading and went to Revolution bar for a few drinks and more chat. There was a good buzz about the people there and they all seemed to have had a good day. We headed off to Pizza express and continued the discussions there including talking about how the DeveloperDeveloperDeveloper organisers are all eager to encourage new speakers and new people. I think that for January I want to actually present a grok talk and maybe the time after that I&#39;ll have the confidence to sit up and submit an entire session and I would encourage anyone reading this to think about giving a presentation.

Thanks very much to Phil and the organisers of the event and thanks to the speakers for giving us some very interesting talks. 
</content>
    </entry>
  
    <entry>
      <title>CQRS - Keeping it simple</title>
      <link href="http://ninjaferret.github.com/2010/10/22/cqrs-keeping-it-simple.html"/>
      <updated>2010-10-22T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2010/10/22/cqrs-keeping-it-simple</id>
      <content type="html">&lt;h3&gt;Architecture Series Overview&lt;/h3&gt;

I have recently become increasingly interested in how we, as developers and architects, structure the systems that we are creating. The demands on on-line systems are significantly different to the demands on desktop only software. Scalability, robustness, performance, resilience are all important factors when thinking about on-line services and there has been a lot of debate within the software world about how we should architect these systems. These posts are my views of interesting architectural patterns and practices that I have been looking at:

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;CQRS: Keeping it simple&lt;/strong&gt; is right here&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.ninjaferret.co.uk/?p=221&quot;&gt;Domain Events&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;CQRS&lt;/h3&gt;

Command Query Responsibility Segregation, or CQRS from now on, is an architectural pattern developed by Greg Young and Udi Dahan. My aim is to take you through a very brief introduction to the simplicity of CQRS and also show you how it can be used to handle a number of different problems that we face within the software world such as separation of concerns, security and scalability.

CQRS breaks down your interaction with a domain into commands and queries as defined below:

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Command&lt;/strong&gt; - Can change state but returns nothing (i.e. there is no query aspect to the command processing)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Query&lt;/strong&gt; - Can only return values but cannot change state, i.e. has no side-effects&lt;/li&gt;
&lt;/ul&gt;

We keep them separate from each other, implement them on different interfaces/endpoints for the simple reason that it allows us greater flexibility on the architecture of the system. 

In many respects that is all that CQRS says and it seems quite boring which is why so much that is written about CQRS also delves into the realms of &lt;em&gt;Domain events&lt;/em&gt;, &lt;em&gt;Event Sourcing&lt;/em&gt; and even &lt;em&gt;Non-relational databases&lt;/em&gt;. You do not need any of these in order to build a CQRS system, they are useful concepts that I will delve deeper into in later posts but they are not essential in order to get the benefits of CQRS.

I would advise that anyone attempting to do CQRS also look at &lt;em&gt;Domain Driven Design&lt;/em&gt; (I will create a blog post on this later) to identify your domains and create the boundaries and interfaces between them.

&lt;h4&gt;Separation of Concerns&lt;/h4&gt;

Well, the brief summary above says it all really, doesn&#39;t it? How does this really provide any benefit?

Today I&#39;m going to start with a very simple example that I will hopefully expand upon in my later posts. I have chosen to think about what it is I need to do to look at a shop model and think specifically about the domain of the products.

As a store manager I need the ability to be able to add/remove items from the store, I need the ability to change the prices and create discounts and I need the tills to query the product information for the current price based on the barcode of the product (or pick the product from a list in the case of fresh produce).

Traditionally, as a developer I would have a product service that gave us this entire functionality:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
public interface ProductService
{
    ProductList GetAllProducts();
    ProductList SearchProducts(ProductSearchCriteria criteria);
    Product     GetByBarcode(Barcode barcode);
    Product     CreateProduct(Product newProduct);
    Product     UpdateProductDetails(Product newDetails);
    Product     DeleteProduct(Product productDetails);
    Product     SetPrice(Price price);
    Product     AddOffer(Offer offer);
}
&lt;/pre&gt;&lt;/code&gt;

I host this service on a PC in the store and I&#39;m quite happy with what I have. However, what models am I using to represent this? Do I have to submit an entire product model just to change the price for that product? Technically not though keeping everything in a single interface does push the developer towards re-using models throughout the interface. Traditionally the add would return the new product including any internally generated identifier that will be uniquely used to identify the product etc.

So what would I produce in a CQRS design?


&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
public interface ProductQueryService
{
    ProductList GetAllProducts();
    ProductList SearchProducts(ProductSearchCriteria criteria);
    Product     GetByBarcode(Barcode barcode);
}

public interface ProductCommandService
{
    void CreateProduct(NewProductCommand command);
    void UpdateProductDetails(ChangeProductDetailsCommand command);
    void DeleteProduct(DeleteProductCommand command);
    void SetPrice(SetPriceCommand command);
    void AddOffer(CreateOfferCommand command);
}

public interface ProductCommandProcessor
{
    void CreateProduct(NewProductCommand command);
    void UpdateProductDetails(ChangeProductDetailsCommand command);
    void DeleteProduct(DeleteProductCommand command);
    void SetPrice(SetPriceCommand command);
    void AddOffer(CreateOfferCommand command);
}
&lt;/pre&gt;&lt;/code&gt;

So, we now have two interfaces which looks like it increases complexity but it has now separated out the two different concerns of our system. Our queries are using different models than the commands and conceptually thinking about a command will allow you to focus in on the exact details that are needed to process that command.

When we are developing the product &lt;strong&gt;Domain&lt;/strong&gt; we are focussed on the &lt;em&gt;verbs&lt;/em&gt; (the conceptual items that exist within the domain) and &lt;em&gt;nouns&lt;/em&gt; (the actions that can be performed on those items). In a CQRS system the query service is solely concerned with extracting a data structure to represent the conceptual item but the command processor is primarily concerned with hiding the data and applying actions to the concepts. There does not need to be any overlap between the domain of the query (data structures) and the domain of the command (actual objects, using proper OO design).

In the traditional model if I wished to add in one extra piece of data to the Product model, perhaps the date that the price was set (which was already being stored) then I would have to re-compile all of my source code including the parts that solely are issuing commands to the system. In CQRS only the items that query need to be re-compiled.

&lt;h4&gt;Security&lt;/h4&gt;

Given the two models above I want to now create an on-line presence as well as having a number of tills within the shop. I want to host the site within the DMZ and maintain my actual data inside the store for one reason, I don&#39;t want hackers to be able to update the details of any of my products. The immediate solution is to clone the primary database inside the LAN and ensure it is kept up to date and... 

&lt;ul&gt;
  &lt;li&gt;In the traditional model I cannot do this easily, everything is linked in a single interface and therefore I have to think about securing specific methods. The command methods can be secured so that they cannot write to the new database for security but I&#39;m having to do more work to ensure that they do not update anything in the cloned database.&lt;/li&gt;
  &lt;li&gt;In the CQRS model I can just create an instance of the query service within the DMZ and point it at the cloned database to the website. I can continue hosting the same service inside the LAN for my tills etc. Any hacker does not know the structure of my commands, the command processor is only within the LAN so the chances of them damaging either the Live database or the clone is minimised.&lt;/li&gt;
&lt;/ul&gt;

In my opinion it is better to only expose the Query Service to systems that only need to be able to query, you can secure down that interface/endpoint much more easily than you can in a traditional service-based approach. 

&lt;h4&gt;Scalability&lt;/h4&gt;

The website is now taking off, I am getting way more visitors than I was expecting but they are now hitting my servers very hard? What do I do?

I realise that I need to increase the number of services that are available to call and place a load balancer between them. Each of those services could have its own clone of the live product data. However, the queries are expensive as there are a lot of joins etc so I need to create a de-normalised reporting database rather than a clone to get the optimum performance.

&lt;ul&gt;
  &lt;li&gt;In the traditional model I think that I would have to change the structure of the service to always point commands at the LIVE database and queries at this new reporting database. I potentially have to re-deploy the services including everything that is needed to handle the command processing. This also adds new configuration that needs to be tested so that all commands are sent to one database and queries sent to the next.&lt;/li&gt;
  &lt;li&gt;In the CQRS model I can purely change the code for the query service and deploy solely that service. I have removed the need for additional configuration. I am keeping the system simple and scalable.&lt;/li&gt;
&lt;/ul&gt;

That is scaling of the queries but I would also like to take a look at scaling of commands. I have a number of third parties who wants to sell through my site but I know that my single command processor service cannot support that many requests. What can I do?

I want to off-line the processing of the commands, all I need to do is to create a queue and process them off-line, potentially within the LAN not the DMZ to maintain a single point of truth for my product data:

&lt;ul&gt;
  &lt;li&gt;In the traditional model I will struggle with this, the very nature of the service suggests that I will immediately return a response with the exact details that I have saved. I can choose to ignore this at my peril or I can have a mechanism that puts the information onto a queue and only returns to the client when it can find that product in the database (which it achieves by polling). This doesn&#39;t sound too good does it?&lt;/li&gt;
  &lt;li&gt;In the CQRS model I simply re-implement the command interface with a new version that writes to a queue, the fact that an error has not occurred tells the client that we have accepted the command. I can scale this behind a load balancer so I can expose a number of these services. Within the LAN I can provide a background worker process (or processes) that pull data from the queue and writes them to the database.&lt;/li&gt;
&lt;/ul&gt;

There may not be too much difference in how we scale the querying side (bar the configuration issue and the tight coupling between the command and the query) but the command side scales much more efficiently.

&lt;h4&gt;Summary&lt;/h4&gt;

In summary, I am really excited about CQRS because it seems to answer a lot of the challenges that we are facing in the software world. It is applying some of the core principles of software development, the separation of concerns and that just by looking at the interface you know whether a method has an impact on the system or whether it is just querying the system. Just one word of warning, I have shown how CQRS &lt;em&gt;can&lt;/em&gt; be used to solve some of these problems but you only need to solve them when you encounter the problem.

It is not a silver bullet, it does not solve all architectural problems and there are a number of other techniques and patterns that we can apply to a system that I will delve into in later posts. However useful the other methods are none of the other techniques are necessary for CQRS and I would seriously advise that you focus on keeping it simple.

</content>
    </entry>
  
    <entry>
      <title>Architecture series</title>
      <link href="http://ninjaferret.github.com/2010/10/16/architecture-series.html"/>
      <updated>2010-10-16T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2010/10/16/architecture-series</id>
      <content type="html">&lt;h3&gt;Overview&lt;/h3&gt;
Architecture of software systems is vitally important in the development of robust and scalable solutions but also architecture can be expensive and time consuming. However, over the last few decades software developers have been identifying &lt;a href=&quot;http://en.wikipedia.org/wiki/Design_pattern_(computer_science)&quot;&gt;Design Patterns&lt;/a&gt; for the structure of their code to help them develop robust software that is easy to maintain there are a number of patterns and principles that can be applied to the wider architecture of your system. 

In the last decade the nature of the software we are developing has radically changed, in the 90s the desktop ruled the world and people installed applications on their own desktops and the data was stored on their desktops and the web was only beginning it&#39;s journey into a truly interactive medium. Within organisations there was some infrastructure but over the last decade on-line systems started to rule the world, services and web sites started playing a major part of people&#39;s lives and this places a completely new set of demands on how we approach software. Our on-line presence is important now and applications on the internet can suddenly become hugely successful (twitter and facebook are two prime examples) so there is a need to become scalable and robust, there is little scope for bringing down your site every evening to perform maintenance if you are reaching a global audience.

What I want to do in this series of posts is explore some of the concepts that are exciting me in software architecture, explore some of the myths that surround the architectures and eventually start bringing them together for developing greenfield applications and also looking at migration patterns for existing systems. I think that it is important to understand when you need to apply a certain architectural model, not everything needs to be applied to every solution, not every pattern or methodology works for every scenario and as architects and engineers we do have a tendency to &quot;over-engineer&quot; a solution, sometimes it is better to do the simple thing first and get to market quicker.

&lt;h4&gt;A sample application...&lt;/h4&gt;

Perhaps one of the hardest things that I can think of when writing a blog post is whether I should write examples, what examples will they be, how to keep the examples simple and yet informative enough for people to take something valuable away. Within the series of posts I will try to keep a standard example and apply the patterns and principles to that example domain. I think that I am going to start with a simple shop model and focus on different parts of the architecture that will allow the system to function properly.

To give you a brief idea of the areas of the shop that I am thinking of:

&lt;ul&gt;
  &lt;li&gt;Stock management - the shop needs to retain only a certain amount of stock to fullfil the needs of its customers so we need to track how much we are ordering, how fast it is selling etc,&lt;/li&gt;
  &lt;li&gt;Sales management - when a customer has found what they are looking for the products that they are buying need to be processed, the total needs to be calculated and the transaction completed&lt;/li&gt;
  &lt;li&gt;Staff management - the shop is going to require something for the staff to schedule shifts, track hours worked for payment, track sick time and holidays&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Current Architecture posts&lt;/h3&gt;

Over time I will be writing these posts and adding a brief summary below:

&lt;h4&gt;CQRS&lt;/h4&gt;

&lt;strong&gt;CQRS&lt;/strong&gt; &lt;em&gt;(Command Query Responsibility Segregation)&lt;/em&gt; is an increasingly popular topic among the software development community (wel, at least in the .NET community that I am mostly involved in). It applies a simple rule &lt;em&gt;&quot;separate your commands from your queries&quot;&lt;/em&gt; and this post explores the benefits of this extremely simple pattern.
</content>
    </entry>
  
    <entry>
      <title>UK Tech Days - Steve Ballmer, the Cloud and Phones</title>
      <link href="http://ninjaferret.github.com/2010/10/09/uk-tech-days-steve-ballmer-the-cloud-and-phones.html"/>
      <updated>2010-10-09T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2010/10/09/uk-tech-days-steve-ballmer-the-cloud-and-phones</id>
      <content type="html">I was one of those attending the Microsoft UK Tech Days on Tuesday, 5th October 2010. I thought that it would be a good idea to see Steve Ballmer talk about the recent developments within the Microsoft world and hear a bit about Cloud computing and Windows Phone 7.

As always it is fantastic to get to meet so many developers in the UK who are passionate about software development. I have written quite a bit so I thought I&#39;d just provide a quick link to the different talks here:

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#keynote&quot;&gt;Keynote speech by Steve Ballmer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#phone7&quot;&gt;Windows Phone 7&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#azure&quot;&gt;Windows Azure&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ie9&quot;&gt;Internet Explorer 9&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#windows7&quot;&gt;Windows 7&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;keynote&quot;&gt;Keynote - Steve Ballmer&lt;/h3&gt;

Steve was an interesting person to listen to, he is working hard to convey passion about the future of Microsoft. He talked about the imminent release of &lt;em&gt;Windows Phone 7&lt;/em&gt;, he admitted that other smart phone providers had stolen an march on Microsoft and that now Microsoft were trying to play catch-up in the field.

He enthused about IE9, the most standards compliant browser that Microsoft has ever produced and about how they are working with the standards bodies by contributing to the test frameworks.

Perhaps one of the more interesting areas that he was talking about was the Windows Azure platform, a cloud computing service that is Microsoft&#39;s attempt to get into the cloud computing market. It is interesting the cycles that we are seeing in the computer market, at one time there were just mainframe computers and everyone worked on a dumb terminal but eventually the personal computer came along and the day of the mainframe seemed to be coming close to an end. However, as technology grew and systems became more connected it once again drove us back to a fairly powerful server-side layer and rather dumber terminals. This time, however, there is a slight difference Steve believes that people want smarter clients to go along with their cloud services which is a concept that I had not really thought of before. Phones are becoming smarter, web-sites are not demanding that the browser provide the capacity to deliver a rich interface for their customers that goes beyond post backs and links. 

&lt;h4&gt;Microsoft vs Open Source&lt;/h4&gt;
Steve took quite a few questions and there was one by &lt;a href=&quot;http://serialseb.blogspot.com&quot;&gt;Sebastian Lambla&lt;/a&gt; concerning Microsoft&#39;s support for the open source community. There are a number of instances where the Microsoft giant has &quot;re-invented the wheel&quot; when there was a perfectly valid open source project already on the market and relatively few cases where they have thrown their might behind an open source project (&lt;a href=&quot;http://jquery.com&quot;&gt;JQuery&lt;/a&gt; being the most well known example).  

There is a natural tension there, even though Microsoft have created &lt;a href=&quot;http://codeplex.com&quot;&gt;codeplex&lt;/a&gt; for hosting open source projects and a number of their own projects are now hosted on there (they have made their code for &lt;a href=&quot;http://aspnet.codeplex.com/&quot;&gt;ASP.NET&lt;/a&gt; available through the site) there is still a long way to go.

Steve did not immediately have an answer about Microsoft&#39;s strategy concerning the Open Source community though did say he&#39;d take a note on it and would consult with senior management about their open source policy. 

To be fair for a long time there was very little Open Source activity in the .NET world and Microsoft were simply building features, products and libraries that they felt were needed in the community. Now, things are changing, the .NET world is opening itself up to new ideas and concepts and people are writing some excellent open source projects. In reality there are a lot of extremely intelligent developers out there writing new libraries, tools and frameworks on the .NET platform and Microsoft would do well to support that community, not only to leverage the innovative ideas, but to provide support to a growing community that if nurtured will remain on the .NET platform and not drift away disillusioned with Microsoft.

I sincerely hope that Steve goes away and thinks seriously about working with the .NET community.

&lt;h3 id=&quot;phone7&quot;&gt;Windows Phone 7&lt;/h3&gt;

So we now come on to the details of the tech days - once Steve had parted the stage and we&#39;d had a little break it was time to take a look at Windows Phone 7. 

My initial thoughts on hearing about Windows Phone 7 was that Microsoft were coming to the party a little late and that there are a number of other more mature phone OS platforms (iPhone and Android really do spring to mind here). However, it was going to be interesting to see what Microsoft would bring to the party.

Microsoft at the moment are trying to tread a fine line between the Apple model &lt;em&gt;&quot;our hardware, our software and everything is tightly controlled&quot;&lt;/em&gt; and the Google model &lt;em&gt;&quot;any hardware, our software and it&#39;s up to you what you do&quot;&lt;/em&gt; by having very tight constraints on the minimum hardware capabilities for the phone manufacturers and they would provide the software to make full use of that hardware. I&#39;m still unusure about what the implications of the different models are but I see that Apple is maintaining the business model that failed in the 80s and 90s when they bound their hardware and software together in an expensive bundle while Microsoft supported any hardware and therefore more people could afford them.

So, about the operating system itself. Microsoft have been borrowing some ideas from other operating systems with features like displaying social media content for your contacts and I&#39;ve heard some interesting reviews (at least one person has said that they hated the fact that all of their facebook contacts appear in the contacts list when they are trying to find a phone - this wouldn&#39;t generally bother me as almost everyone I know on facebook I have a phone number for). The apparent lack of support for Twitter seems to me to be a bit of an oversight on Microsoft&#39;s part. In terms of other features they have Push notifications that mimics the Apple push notifications but allows people to change the &quot;tile image&quot; for the application that is receiving the message which is a neat little touch. 

Sadly Windows Phone 7 is lacking in multi-tasking abilities beyond their own applications. I understand that Microsoft are probably worried about the impact on the battery life of their phones, as Apple were, but even Apple have realised that having some multi-tasking capability is necessary and relented with iOS 4. Also there was a sad lack of any copy and paste which Microsoft said that this was not something that people want but trust me, the phone comes with a version of office and I want copy and paste when I&#39;m working in office documents. 

Beyond that development for Windows Phone 7 is actually a very nice experience as the applications are either Silverlight based or XNA based so build on the standard set of tools that .NET developers are all going to be familiar with. Seeing how &quot;easy&quot; the development was then I am encouraged to play around with Windows Phone 7 development and see just what is out there.

Will Windows Phone 7 be a hit? I&#39;m not sure, the timescales for development of the phone are very tight and I think that the other operating systems are more mature but Microsoft has an amazing chance to bring something new to the party with later versions of the operating system. They broke in to the lucrative gaming market with great success with the X-box, they have millions of desktop users and therefore they need to bring together a fusion environment where all the devices work together as a phenomenal user experience.

&lt;h3 id=&quot;azure&quot;&gt;Windows Azure&lt;/h3&gt;

The next talk, after a break and a chat to some students who had won a competition to go to this event, was about Windows Azure, the cloud computing service provided by Microsoft. 

Why should we use cloud computing? Well, if you are simply producing a small website then I think that using a cheap hosting package (there are plenty out there for a few pounds a month) would actually be better for you but if you are creating an application that may need to scale up and down as demand waxes and wanes then this is certainly something that you should potentially consider. 

Couldn&#39;t I just get a server? Yes, initially you could just buy a server and then manage and maintain it yourself but that can cost your organisation. Similarly, for a larger organisation a data centre is an expensive thing to have. However, if you have significantly fluctuating demand (perhaps a live sports video feed site that only really is under demand when a sporting event is on) then the servers are sitting there doing nothing but consuming power and costing you money to maintain.

With Azure you can scale back during quiet period and then scale up significantly just before the event ensuring that you have capacity and then scale back when the event is over and you are only paying for those instances for the time that they are running without having to manage the infrastructure yourself. 

Azure works on the idea of a &lt;strong&gt;role&lt;/strong&gt; which can be a web site, web service or a background worker process. Each of these are intuitively easy for existing developers to learn as they are building on the same kinds of concepts that are behind traditional web and service projects. Microsoft will then charge you per compute hour for each instance of each role. 

There are also a number of different storage mechanisms on Azure and it is worth people getting to understand what is available before just deciding to go down the SQL Azure route. 

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Blob Storage&lt;/strong&gt;
  This can store almost anything and is effectively a cloud file system. Fast and reliable&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Queues&lt;/strong&gt;
  Queues are mechanisms for securely communicating between services.
  &lt;li&gt;&lt;strong&gt;Tables&lt;/strong&gt;
  Tables are not SQL Server Tables! In effect they are an large single table that is indexed by &quot;Table Name&quot;, &quot;Partition Key&quot; and &quot;Row&quot; then there are 252 other columns that cannot be indexed (so beware folks, always query your tables with these keys otherwise you&#39;ll be doing table scans). Different rows in the table can be different schemas. If you design your structures correctly this can be a very fast (and very cheap) way to store data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;SQL Azure&lt;/strong&gt;
  This is almost, but not quite, SQL Server in the cloud. There are a number of limitations to this and the most basic package costs $6 per month for up to 1GB of data.&lt;/li&gt;
&lt;/ul&gt;

I am quite interested in development for Windows Azure and I have registered with an account though at the moment I am slightly worried about the costs $0.12 per hour (that is not per CPU hour but per hour that your application is deployed) which means that to run a single compute instance for an entire year is over $1,000 especially when you take into account storage etc. Just as a comparison the &lt;a href=&quot;http://heroku.com&quot;&gt;Heroku&lt;/a&gt; platform for Ruby gives you one web instance free and charges you about half as much for subsequent instances.

&lt;h3 id=&quot;ie9&quot;&gt;IE9&lt;/h3&gt;

Well, IE9 doesn&#39;t really excite me that much. It is the most standards compliant Microsoft browser, they have made it look cleaner and they have leveraged the power of the GPU to make it currently out-perform other browsers (though at some point the other browsers will catch up on that score.

Microsoft at the moment seem to be putting resource into HTML5 and trying to make the platform work and there are a number of little features that integrate your favourite web pages into windows 7 with features like Jumplists that will allow you to pin your site and specific links within your site to the taskbar (for example pin facebook to your taskbar and you&#39;ll have jumplists to go directly to your messages, events etc by just right-clicking on the taskbar icon).

There are some innovative ideas here but nothing that is truly inspiring me at the moment. 

&lt;h3 id=&quot;windows7&quot;&gt;Windows 7 - by Mike Taulty&lt;/h3&gt;

Windows 7 is already here but I think the focus of this talk was primarily around re-thinking how we develop applications and how people expect applications to behave in the modern world. Old-style windows with grey background, text fields etc that are fairly static and boring are no longer really applicable in a world where the web is providing extremely rich and interesting interfaces. 

Using WPF can allow you to develop some interesting and dynamic interfaces (one example of this is the &lt;a href=&quot;http://www.metrotwit.com&quot;&gt;MetroTwit&lt;/a&gt; twitter client that uses a Metro interface) and another example was the Zune interface that Microsoft have developed. People want their desktop apps to feel intuitive, to look sleek and stylish just as they would expect a web-site to look. 

Microsoft now expose ribbon interfaces to replace the old menu and toolbar interfaces and they have provided us with newer dialog boxes that look cleaner and well, generally just more shiny than the traditional boxes. 

Mike also showed us how to handle auto-recovery should your application crash to improve the user experience. 

It is our responsibility, as developers and designers, to provide the users with easy clean and intuitive interfaces and there is a lot of work going on at the moment to change the way in which Windows 7 applications will appear in the future and don&#39;t forget that as touch devices become more popular the nature of a lot of our applications may also have to change, even if Windows 7 is multi-touch compatible our applications have to be as well.

&lt;h3&gt;Summary&lt;/h3&gt;

Sorry for typing so much, all in all the Microsoft Tech Days was a really interesting event and I have been thinking a lot about it over the last few days. I enjoy conferences in general as they are a way to recharge my batteries and learn something that is not part of my daily life, spending time with other passionate developers during (and in the pub after) the conferences. Thanks to the UK Tech Days team for organising this and I look forward to the other events in the future.
</content>
    </entry>
  
    <entry>
      <title>My team's Kanban board</title>
      <link href="http://ninjaferret.github.com/2010/10/07/my-team-s-kanban-board.html"/>
      <updated>2010-10-07T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2010/10/07/my-team-s-kanban-board</id>
      <content type="html">This is the Kanban board that has evolved for my team. 
</content>
    </entry>
  
    <entry>
      <title>An introduction to Kanban</title>
      <link href="http://ninjaferret.github.com/2010/10/07/an-introduction-to-kanban.html"/>
      <updated>2010-10-07T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2010/10/07/an-introduction-to-kanban</id>
      <content type="html">I recently gave a nugget at &lt;a href=&quot;http://nxtgenug.net/&quot;&gt;nxtgen user group&lt;/a&gt; in Cambridge on Kanban and thought that I would just get my &lt;a href=&quot;http://ninjaferret.co.uk/Presentations/Kanban.pptx&quot;&gt;slides on-line&lt;/a&gt; for people to download.

For several years I have been working within an agile team, or a team trying to be agile in a hostile world. We were trying to use SCRUM and achieved a lot but there were times when we actually found that it became difficult because we were not quite managing to do agile methods to their completion as we found it difficult to break away from the traditional waterfall methodologies. Testers were not close to the development so months of development work was being done before testers would even look at it and on the other side of development we were expected to do a significant amount of  analysis and design months in advance of any software being developed.

My team was faced with the question &lt;em&gt;how much time are we going to commit to fixing bugs this iteration?&lt;/em&gt; How can we answer that? Perhaps if we knew what bugs there were going to be found then we would know how we were going to fix them but this is the real world and we guessed. Were the guesses accurate? We found that either we were significantly under or over-committing to the sprint depending on the bugs that were found in the last project. At this point the sprints started to fail and the team became demotivated.

Kanban came to the rescue. 

&lt;h3&gt;What is Kanban?&lt;/h3&gt;

Kanban originated in the 1940&#39;s and 50&#39;s from Toyota where they were developing the &lt;em&gt;lean&lt;/em&gt; processes. They devised a process where each process in the chain was the consumer of the earlier processes, in the system there should be enough parts in the stores to allow the current process to take only what it needs and then a &lt;em&gt;signal&lt;/em&gt; (usually in the form of a card - or &lt;strong&gt;Kanban&lt;/strong&gt; as they are called in Japanese) is sent to the previous process to produce more of the consumed items.

The delivery of the final product to the consumer determines the rate of flow, if demand is low then production slows down (by removing Kanban from the system) and if demand increases more Kanban are pushed into the system to meet demand.  

&lt;h3&gt;Applying Kanban...&lt;/h3&gt;
Kanban does not always have to be applied to the manufacturing industry where it was first conceived, it can be used as a means of managing workflow in a number of different environments and be applied to many processes.

&lt;h4&gt;Value stream&lt;/h4&gt;

The first thing that you really need to do is to start looking at the &lt;em&gt;Value Stream&lt;/em&gt; that you are applying Kanban to, personally I would start with the most basic flow:

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;To Do&lt;/strong&gt; this is the list of tasks (hopefully prioritised) that needs to be done, sometimes I will call this &lt;em&gt;Options&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Doing&lt;/strong&gt; the list of tasks that is currently being worked on, the things that are in progress.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Done&lt;/strong&gt; the task has been completed - it is up to the individual team to determine what they define by done&lt;/li&gt;
&lt;/ol&gt;

In a complicated process then it is necessary to actually break down the &quot;Doing&quot; column down into smaller sections that map the process that a task has to go through in order to be completed. Given the software development domain then we may have:

&lt;ul&gt;
  &lt;li&gt;Analysis&lt;/li&gt;
  &lt;li&gt;Development&lt;/li&gt;
  &lt;li&gt;Test&lt;/li&gt;
 &lt;/ul&gt;

We may say that something is only truly done when it is deployed to the production environment.

&lt;h4&gt;Introduce a pull-based signalling mechanism...&lt;/h4&gt;

So, having mapped our value stream we need to think about a system where the new features are pulled through the system rather than being pushed through. Continuing with the development environment and assuming that we are aiming to deliver features into a system:

&lt;ul&gt;
  &lt;li&gt;The &lt;strong&gt;Testers&lt;/strong&gt; finishes testing a feature, releases it for production deployment. They know that a feature is waiting for them and so they begin testing that feature. This is the signal to the development team that the test team need will soon need a new feature to test.&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;Developers&lt;/strong&gt; were already working on a new feature and deliver it to the test team, they complete the work and it is ready to test. They have received a signal from the Test Team that they can start work on a new feature so they take the feature that is pending; this is the signal to the analysts that a new feature is required.&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;Analysts&lt;/strong&gt; were already preparing the initial analysis for a new feature and make it available when it is ready. They receive a signal from the development team saying that they have no pending work therefore they take the next high priority story and begin their analysis.&lt;/li&gt;
&lt;/ul&gt;

Therefore the flow has changed from people saying &quot;you will deliver this&quot; to simply focussing on delivering at the speed of the slowest component.

&lt;h4&gt;Visualisation&lt;/h4&gt;

When applying Kanban to software processes people tend to create a &lt;strong&gt;Kanban board&lt;/strong&gt;. At the moment I am using a whiteboard:

[caption id=&quot;attachment_147&quot; align=&quot;alignnone&quot; width=&quot;300&quot; caption=&quot;My team\&#39;s Kanban board&quot;]&lt;a href=&quot;http://ninjaferret.co.uk/blog/wp-content/uploads/2010/10/KanbanAtWork_small.png&quot;&gt;&lt;img src=&quot;http://ninjaferret.co.uk/blog/wp-content/uploads/2010/10/KanbanAtWork_small-300x237.png&quot; alt=&quot;This is my team&amp;#039;s Kanban board&quot; title=&quot;My team&amp;#039;s Kanban board&quot; width=&quot;300&quot; height=&quot;237&quot; class=&quot;size-medium wp-image-147&quot; /&gt;&lt;/a&gt;[/caption]

It is broken down into the three basic components:

&lt;ul&gt;
   &lt;li&gt;Options - a prioritised list of items that need to be done (&lt;em&gt;To Do&lt;/em&gt; from above)&lt;/li&gt;
   &lt;li&gt;In Progress - what the team is working on (&lt;em&gt;Doing&lt;/em&gt; from above)&lt;/li&gt;
   &lt;li&gt;Deployed - the work is finally deployed to the production server (&lt;em&gt;Done&lt;/em&gt; from above)&lt;/li&gt;
&lt;/ul&gt;

I would generally advise that you keep the board simple and focus on mapping your value stream. We split the &lt;em&gt;In Progress&lt;/em&gt; column into the three phases of &lt;strong&gt;Analysis&lt;/strong&gt;, &lt;strong&gt;Build&lt;/strong&gt; and &lt;strong&gt;Test&lt;/strong&gt;. In the lower horizontal swim-lane we have provided a number of different sections that representing the flow of data:

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Parked&lt;/strong&gt; - After initial analysis the project is put on hold until approval is reached and once more after the design has been produced. As you can see, we have done a lot analysis and design at the moment.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Awaiting Test&lt;/strong&gt; - Our Test Team are in control about when they receive the new features. We have split this section into two pieces, the first for items in the release currently being tested and the other for items being built for the next release.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Awaiting deployment&lt;/strong&gt; - If/When the test team tell us that a feature has been tested (including UAT) then it is now just waiting for a deployment to be scheduled.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Rejected&lt;/strong&gt; - This is for features that have either been pulled, due to the estimates being large, or for bugs reported where the problem was with the original setup data.&lt;/li&gt;
&lt;/ul&gt; 

This board represents our view of the world, it was not our first attempt and I&#39;m sure that over time it will evolve further.

We have also been experimenting with recording the dates on which the features move from one state to the next, as we record these facts we can build metrics of how long different sized features take to pass through each stage in the system.

&lt;h4&gt;Limiting Work in Progress&lt;/h4&gt;

We use our Kanban board to help us limit the amount of work in progress that our team is doing. With a team of 5 people we limit ourselves to no more than 8 features in progress at once, 4 things in analysis and design with a further 4 things in build. At the moment we do not constrain the work of the Test Team because we deliver work to them in a waterfall manner.

Why do we limit the work in progress? If we do not limit our work in progress then a team can quickly become swamped with requests for designs to be done, bugs raised by the test team, new features and numerous other things. Context switching is expensive and it is often easier to focus on a single thing to completion before starting the next. 

Limiting the total work in progress for the team to less than double the team&#39;s number ensures that no one person can be working on more than two things at once, if one task is blocked then they can work on the other. However, it encourages people to help the other members of the team, if I complete something in &lt;em&gt;build&lt;/em&gt; but nothing is ready for analysis I should help someone finish their analysis, or do some work to aid the test team.


&lt;h4&gt;Identifying inefficient processes&lt;/h4&gt;

So far I have talked about mapping your value streams, visualising your progress with a Kanban board and limiting your work in progress. The Kanban board is flowing and it becomes quickly obvious where the most inefficient processes are:

&lt;ul&gt;
  &lt;li&gt;Testing is swamped - there are over 40 items stacked within the test column&lt;/li&gt;
  &lt;li&gt;We are doing a significant amount of analysis so far ahead of development that we have to re-analyse everything before starting work&lt;/li&gt;
&lt;/ul&gt;

These are things that we already &quot;know&quot; but it helps us to quantify and visualise the nature of the problem. We know now that there are features waiting for almost 5 months from when we finished the development and have not yet gone into testing. We know that testing is swamped and we can see just how much work they have to do. Will this help us make changes? I hope so.

&lt;h4&gt;Experiment&lt;/h4&gt;

The Kanban board has to be built for your specific team, they have to mean something to your team because unless you get buy-in from everyone then it is not going to work. Apply the &lt;em&gt;Inspect and Adapt&lt;/em&gt; principle, try something for a while, review it and then decide whether it was a success or not.

&lt;h3&gt;Summary&lt;/h3&gt;

Kanban is a simple system, it is a signalling system that works with your current processes, gives you the ability to constrain your work to make you more efficient and allows you to visualise the inefficiencies in your system. Please try out Kanban and experiment with it.
</content>
    </entry>
  
    <entry>
      <title>WCF Clients Part 3 - Runtime Generated Proxies</title>
      <link href="http://ninjaferret.github.com/2010/09/07/wcf-clients-part-3-runtime-generated-proxies.html"/>
      <updated>2010-09-07T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2010/09/07/wcf-clients-part-3-runtime-generated-proxies</id>
      <content type="html">In &lt;a href=&quot;http://ninjaferret.co.uk/blog/?p=98&quot;&gt;Part 1&lt;/a&gt; and &lt;a href=&quot;http://blog.ninjaferret.co.uk/?p=109&quot;&gt;Part 2&lt;/a&gt; of my WCF Client series I have described the problems that I have encountered while building clients for WCF services and how I went about solving those problems by hand-crafting my own code. Having identified a standard pattern that I was using for creating a proxy client for a WCF service as seen in &lt;a href=&quot;http://blog.ninjaferret.co.uk/?p=109&quot;&gt;Part 2&lt;/a&gt;.

My earlier posts were focussed around understanding IL and how to use IL to generate classes at runtime to implement an interface with the aim of generating proxy clients to implement the service interfaces. 

I have created a framework that will automatically generate a proxy client for the service interface at runtime. The &lt;a href=&quot;http://github.com/downloads/ninjaferret/NinjaFerret.Wcf/NinjaFerret.Wcf.Client.zip&quot;&gt;assemblies can be downloaded here&lt;/a&gt; and the source code can be found &lt;a href=&quot;http://github.com/ninjaferret/NinjaFerret.Wcf&quot;&gt;on the github repository&lt;/a&gt;  and I am going to talk about this new framework in this post.

&lt;h3&gt;Recap of the scenario I have&lt;/h3&gt;

I have an account service that has been defined previously but now has been changed into a WCF service definition:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
[ServiceContract]
public interface AccountService
{
    [OperationContract]
    [FaultContract(typeof(AccountNotFoundFault))]
    Account Get(int id);

    [OperationContract]
    [FaultContract(typeof(NoMatchingAccountsFault))]
    Account Find(string customerName, AccountType accountType);

    [OperationContract]
    [FaultContract(typeof(InvalidCustomerNameFault))]
    void Create(string customerName, AccountType accountType);

    [OperationContract]
    [FaultContract(typeof(AccountNotFoundFault))]
    void Delete(int accountId);

    [OperationContract]
    [FaultContract(typeof(AccountNotFoundFault))]
    [FaultContract(typeof(InvalidCustomerNameFault))]
    void ChangeAccountDetails(int accountId, string customerName, AccountType accountType);

    [OperationContract]
    [FaultContract(typeof(AccountNotFoundFault))]
    void FreezeAccount(int accountId);
}
&lt;/pre&gt;&lt;/code&gt;

The client would like to have two different interfaces, the first of which is a general bank teller site written in ASP.NET MVC and a second administrator&#39;s application written in WPF that would be installed on the administrator&#39;s PC (at the moment as a means to justify having the service).

Having now defined the service interface as well as the models and even the faults/exceptions within a single assembly that will be referenced by both clients and by the service itself. The original service implementation will stay exactly the same and a new service implementation will be created to wrap the original service but handle the conversion between the original exceptions and the Faults that are to be carried over the wire.

&lt;h3&gt;Translating the exceptions within the service&lt;/h3&gt;

If you take the original service:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
public class InternalAccountService : AccountService
{
    //constructors and member variables here...

    public Account Get(int id)
    {
        var account = _accountRepository.Get(id);
        if (account == null)
            throw new AccountNotFoundException(id);
        return account;
    }

    // All the other methods go here
}
&lt;/pre&gt;&lt;/code&gt; 

Create the &lt;strong&gt;AccountNotFoundFault&lt;/strong&gt; fault making it implement &lt;strong&gt;NinjaFerret.Wcf.Exception.TranslatableFault&lt;/strong&gt; then change the &lt;strong&gt;AccountNotFoundException&lt;/strong&gt; to implement &lt;strong&gt;NinjaFerret.Wcf.Exception.ITranslatableException&lt;/strong&gt;:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
[DataContract]
public class AccountNotFoundFault : TranslatableFault
{
    [DataMember]
    public int AccountId { get; set; }

    public override Exception ToException()
    {
        return new AccountNotFoundException(AccountId);
    }
}

public class AccountNotFoundException : Exception, NinjaFerret.Wcf.Exception.ITranslatableException
{
    public int AccountId { get; private set; }

    public AccountNotFoundException(int accountId) 
       : base(string.Format(&amp;quot;Account {0} could not be found&amp;quot;, accountId)
    {
        AccountId = accountId;
    }

    public Fault ToFaultException()
    {
        return new FaultException&amp;lt;AccountNotFoundFault &amp;gt;(
                    new AccountNotFoundFault {AccountNumber = AccountNumber},
                    &amp;quot;The account does not exist&amp;quot;);
    }
}
&lt;/pre&gt;&lt;/code&gt;

Your service wrapper becomes:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
public class ExposedAccountService : AccountService
{
    // Constructors and member variables

    public Account Get(int id)
    {
        try
        {
            return _internalAccountService.Get(id);
        }
        catch(AccountNotFoundException e)
        {
            throw e.ToFaultException();
        }
    }

    // And other method implementations here
}
&lt;/pre&gt;&lt;/code&gt;

I can then host this service wrapper within IIS, within a windows service or anywhere a WCF service can be hosted.

&lt;h3&gt;What happens to the client?&lt;/h3&gt;

I now have a service that I can communicate with but what do I have to do on the client? Well, that is where the &lt;strong&gt;NinjaFerret.Wcf.Client.ClientFactory&lt;/strong&gt; class comes into play. 

I am assuming that the client has been written against original interface and the current implementation (the &lt;em&gt;InternalAccountService&lt;/em&gt;) was injected in to the constructor (my sample application does this using the &lt;a href=&quot;http://www.castleproject.org/container/index.html&quot;&gt;Castle Windsor&lt;/a&gt; dependency injection framework). Whatever injects the service implementation into the client now needs to call:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
// this will require that a standard endpoint is set up with the name
// AccountService
new ClientFactory&amp;lt;AccountService&amp;gt;().Generate(); 
&lt;/pre&gt;&lt;/code&gt;

Or if you need to call into services that provide the same interface but with different endpoints then each endpoint will need its own name:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
// this will require that a standard endpoint is set up with the specified endpoint name
new ClientFactory&amp;lt;AccountService&amp;gt;().GenerateForEndpoint(string endpointName); 
&lt;/pre&gt;&lt;/code&gt;

The &lt;strong&gt;ClientFactory&lt;/strong&gt; will, at runtime, generate a client for you based on the information provided by the original interface and will use the &lt;strong&gt;NinjaFerret.Wcf.Exception&lt;/strong&gt; faults provided to convert back and throw the original exceptions therefore there is no additional exception handling to be done on the client (if there is a communication exception then the client may need to handle this case but it is a significantly smaller change that would not affect behaviour should we revert to running the service in-line). The generated client handles all exceptions that come across the wire and correctly disposes any failed channels so we do not have to change the clients to dispose of the service. 

What remains is the standard config changes for defining a WCF  endpoint:

[sourcecode language=&quot;xml&quot; gutter=&quot;false&quot;]
&amp;lt;system.serviceModel&amp;gt;
    &amp;lt;client&amp;gt;
      &amp;lt;endpoint address=&amp;quot;http://localhost:8010/AccountService&amp;quot; binding=&amp;quot;basicHttpBinding&amp;quot;
        bindingConfiguration=&amp;quot;&amp;quot; contract=&amp;quot;NinjaFerret.Wcf.Sample.BankManager.Interface.AccountService&amp;quot;
        name=&amp;quot;AccountService&amp;quot; kind=&amp;quot;&amp;quot; endpointConfiguration=&amp;quot;&amp;quot;&amp;gt;
      &amp;lt;/endpoint&amp;gt;
    &amp;lt;/client&amp;gt;
  &amp;lt;/system.serviceModel&amp;gt;
&lt;/pre&gt;&lt;/code&gt;

&lt;h3&gt;So what now?&lt;/h3&gt;

That is up to you... 

1. Download it
2. Try it
3. Give feedback - without feedback (good or bad) I can&#39;t improve it
</content>
    </entry>
  
    <entry>
      <title>WCF Clients Part 2 - IChannelFactory<T></title>
      <link href="http://ninjaferret.github.com/2010/09/04/wcf-clients-part-2-ichannelfactory-t-.html"/>
      <updated>2010-09-04T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2010/09/04/wcf-clients-part-2-ichannelfactory-t-</id>
      <content type="html">I talked &lt;a href=&quot;http://ninjaferret.co.uk/blog/?p=98&quot;&gt;last time&lt;/a&gt; about how I approached building a WCF service and how using &quot;Add Service Reference&quot; was causing me some problems. The next part of my journey into WCF was to look at other ways of managing the client and I came across &lt;strong&gt;IChannelFactory&amp;lt;T&amp;gt;&lt;/strong&gt;.

&lt;h3&gt;Creating a proxy client IChannelFactory&amp;lt;T&amp;gt;...&lt;/h3&gt;

So, to recap, last time I started by defining the following service:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
[ServiceContract]
public interface AccountService
{
    [OperationContract]
    [FaultContract(typeof(AccountNotFoundFault))]
    Account Get(int id);

    [OperationContract]
    [FaultContract(typeof(NoMatchingAccountsFault))]
    Account Find(string customerName, AccountType accountType);

    [OperationContract]
    [FaultContract(typeof(InvalidCustomerNameFault))]
    void Create(string customerName, AccountType accountType);
}
&lt;/pre&gt;&lt;/code&gt;

The next thing is to look at the original client code within an ASP.NET MVC controller:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
public ActionResult AddNewAccount(NewAccountCommand newAccountCommand)
{
    // Validate the command
    ...
    // Create the account using the service
    try
    {
        _service.Create(newAccountCommand.CustomerName, newAccountCommand.AccountType);
     }
    catch (InvalidCustomerNameException e)
    {
        // Handle the error
    }
}
&lt;/pre&gt;&lt;/code&gt;

How do we get to using the channel factory? As my client is already referencing the service assemblies and using the channel factory we can implement the interface:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
var channelFactory = new ChannelFactory&amp;lt;AccountService&amp;gt;();
var proxy = channelFactory.CreateChannel();
&lt;/pre&gt;&lt;/code&gt;

The proxy that is generated can be passed into the ASP.NET MVC controller as the service.

&lt;h3&gt;So where is the problem?&lt;/h3&gt;

The client code above will work... well, it will work in the case where the exceptions are not thrown. When an exception is thrown it will be translated into a &lt;strong&gt;FaultException&lt;/strong&gt; therefore the original exception will not be caught. To fix this we could try:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
public ActionResult AddNewAccount(NewAccountCommand newAccountCommand)
{
    // Validate the command
    ...
    // Create the account using the service
    try
    {
        _service.Create(newAccountCommand.CustomerName, newAccountCommand.AccountType);
     }
    catch (InvalidCustomerNameException e)
    {
        // Handle the error
    }
    catch(FaultException&amp;lt;InvalidCustomerNameFault&amp;gt; fault)
    {
        // Handle the error
    }
}
&lt;/pre&gt;&lt;/code&gt;

But now the code is less clean, for each error I want to handle I would have to now catch either just the equivalent &lt;strong&gt;FaultException&lt;/strong&gt; or both the normal exception or the &lt;strong&gt;FaultException&lt;/strong&gt;.

However, the service is now disposable! I now have to remember to call the &lt;strong&gt;Close()&lt;/strong&gt; or &lt;strong&gt;Dispose()&lt;/strong&gt; methods at some point to release the resources. When do I call it? I have injected this into the controller so the controller cannot dispose it because it does not know whether there are any other references to this object out there. 

While searching around this problem I came across the &lt;a href=&quot;http://old.iserviceoriented.com/blog/post/Indisposable+-+WCF+Gotcha+1.aspx&quot;&gt;Indisposable - WCF Gotcha 1&lt;/a&gt; article on &lt;a href=&quot;http://www.iserviceoriented.com/blog&quot;&gt;serviceoriented.com&lt;/a&gt;. To summarise, a disposable object can be wrapped in a using statement which will automatically call the &lt;strong&gt;Dispose()&lt;/strong&gt; method or you can call the &lt;strong&gt;Dispose()&lt;/strong&gt; method yourself (usually in a finally block). However, the proxy that is generated by &lt;strong&gt;ChannelFactory&lt;/strong&gt; requires that you call &lt;strong&gt;Abort()&lt;/strong&gt; after an exception rather than &lt;strong&gt;Dispose()&lt;/strong&gt;.

The pattern that was recommended by the blog article was that we can wrap most of the error handling within a generic method:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
public static class Service&amp;lt;T&amp;gt;
{
    public static ChannelFactory&amp;lt;T&amp;gt; _channelFactory = new ChannelFactory&amp;lt;T&amp;gt;(&amp;quot;&amp;quot;); 

    public static void Use(UseServiceDelegate&amp;lt;T&amp;gt; codeBlock)
    {
        IClientChannel proxy = (IClientChannel)_channelFactory.CreateChannel();
        bool success = false;
        try
        {
            codeBlock((T)proxy);
            proxy.Close();
            success = true;
        }
        finally
        {
            if (!success)
            {
                proxy.Abort();
            }
        }
    }
}
&lt;/pre&gt;&lt;/code&gt;

But that means that rather than making a standard call to the service we now have to change the client code to:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
public ActionResult AddNewAccount(NewAccountCommand newAccountCommand)
{
    // Validate the command
    ...
    // Create the account using the service
    try
    {
        Service&amp;lt;AccountService&amp;gt;.Use(service =&amp;gt; service.Create(newAccountCommand.CustomerName, newAccountCommand.AccountType);
     }
    catch (InvalidCustomerNameException e)
    {
        // Handle the error
    }
    catch(FaultException&amp;lt;InvalidCustomerNameFault&amp;gt; fault)
    {
        // Handle the error
    }
}
&lt;/pre&gt;&lt;/code&gt;

This makes me a little worried because the changes have just made the system untestable, by calling into this static class I have now bound the controller to requiring a WCF service to test.

&lt;h3&gt;Crafting my own proxy...&lt;/h3&gt;

In order to keep my code testable I used the above pattern within my own hand-crafted proxies:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
public class AccountServiceProxy : IAccountService
{
    public void Create(string customerName, AccountType accountType)
    {
        try
        {
            Service&amp;lt;AccountService&amp;gt;.Use(service =&amp;gt; service.Create(customerName, accountType);
        }
        catch(FaultException&amp;lt;InvalidCustomerNameFault&amp;gt; fault)
        {
            throw new InvalidCustomerNameException(...);
        }
    }

    public Account Get(int id)
    {
        try
        {
            Account account = null;
            Service&amp;lt;AccountService&amp;gt;.Use(service =&amp;gt; account = service.Get(id);
            return account;
        }
        catch(FaultException&amp;lt;AccountNotFoundFault&amp;gt; fault)
        {
            throw new AccountNotFoundException(...);
        }
    } 
}
&lt;/pre&gt;&lt;/code&gt;

This keeps the client code in tact, it only needs to handle the standard exceptions that it always handled rather than adding anything more because of the faults. A much cleaner solution, in my mind.

&lt;h3&gt;So is there still a problem?&lt;/h3&gt;

I think that there is still a problem, I am now having to manually do all of the work that I was hoping that someone else would do for me and this led me to think that I now had a pattern that I was manually applying to all services therefore it should be possible to automate the generation of these proxies; this leads me on nicely to my next post...
</content>
    </entry>
  
    <entry>
      <title>WCF Clients Part 1 - Add Service Reference</title>
      <link href="http://ninjaferret.github.com/2010/08/31/wcf-clients-part-1-add-service-reference.html"/>
      <updated>2010-08-31T00:00:00+01:00</updated>
      <id>http://ninjaferret.github.com/2010/08/31/wcf-clients-part-1-add-service-reference</id>
      <content type="html">&lt;h3&gt;The problem I am trying to solve...&lt;/h3&gt;
Over the past couple of years I have written a number of WCF services, I have hosted them in IIS, in a windows service and even in console applications (OK that was only for test/demo purposes) but every time I have written the service I have also written a client to work with that service; more often the need for the service has arisen as a part of writing some other application that I am trying to write.

Let us assume that I am writing an ASP.NET MVC web site that needs to talk down to a database to retrieve information and this site is a simple site that allows a bank clerk to do very simple banking transactions but the first thing we need to focus on is delivering a system that has very basic account management:
&lt;ul&gt;
	&lt;li&gt;Create an account&lt;/li&gt;
	&lt;li&gt;Find an account using its ID&lt;/li&gt;
	&lt;li&gt;Find an account using the name of the customer and the account type&lt;/li&gt;
	&lt;li&gt;Transfer money between accounts&lt;/li&gt;
	&lt;li&gt;Deposit money to an account&lt;/li&gt;
	&lt;li&gt;Withdraw money from an account&lt;/li&gt;
&lt;/ul&gt;
&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
public interface AccountService
{
    Account Get(int id);

    Account Find(string customerName, AccountType accountType);

    void Create(string customerName, AccountType accountType);
}
&lt;/pre&gt;&lt;/code&gt;

I would create the site but have the controller talk down to an internal service which would abstract away all of the business logic from the user interface, this service would be injected into the controllers to aid testability. Within the service I would expect a rich domain model and repositories as the data access layer. I hook everything up and it works; so far, so good.

However, as well as the ASP.NET site the client would like a group of super users to have a specific WPF application that has the same functionality but also has the ability to do:
&lt;ul&gt;
	&lt;li&gt;Delete a transaction&lt;/li&gt;
	&lt;li&gt;Update account details&lt;/li&gt;
	&lt;li&gt;Freeze an account&lt;/li&gt;
&lt;/ul&gt;
I now need to expose the original service that I created to two different clients and at this point I turn to WCF so I start by amending the interface definition:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
[ServiceContract]
public interface AccountService
{
    [OperationContract]
    [FaultContract(typeof(AccountNotFoundFault))]
    Account Get(int id);

    [OperationContract]
    [FaultContract(typeof(NoMatchingAccountsFault))]
    Account Find(string customerName, AccountType accountType);

    [OperationContract]
    [FaultContract(typeof(InvalidCustomerNameFault))]
    void Create(string customerName, AccountType accountType);
}
&lt;/pre&gt;&lt;/code&gt;


The next thing I need to do is to host this new service somewhere, assuming for now this is IIS I could just plug in the existing service implementation but I would need to change within the service itself what exceptions it is throwing. Personally, I would much prefer to keep the service implementation &quot;as is&quot; and write a wrapper for it that catches the specific exceptions and throws their equivalent faults. 

OK, I&#39;ve written the wrapper service, hosted it and I&#39;m ready to go... well apart from the fact that I haven&#39;t yet done anything to the ASP.NET MVC or WPF clients. 

&lt;h3&gt;Add Service Reference&lt;/h3&gt;

On my client projects, which normally exist within the same solution as the service (as the service was formed from it) I now need to use the &quot;Add Service Reference&quot; wizard. I locate my service and add the reference, luckily, if I have my service contract (i.e. the interface, faults and models) located in their own assembly I am happy to use the option to re-use the models that I am referencing, however it generates a new interface for me. 

&lt;h4&gt;I lose my reference to the original interface...&lt;/h4&gt;

I am forced to change everywhere that referenced my original service interface to point to the client interface that has been generated despite the fact that the original interface is still perfectly usable. For me this is a problem, especially for my development, if I want to check some functionality out in a hurry I want the ability to just run the system and not care whether I have configured the WCF endpoint correctly on the server or the client; I just want to run the the service in-line to test the logic. Whether we use a WCF service or an in-line service on the production environment does not matter to me when I am tracking down a bug within the business logic.

You can edit the automatically generated reference to point back at your original interface (solve&#39;s this problem doesn&#39;t it) but that is extremely hacky and requires someone to know that every time you re-generate the reference you have to do this work and that is storing up problems for whoever is following you to maintain this code.

&lt;h4&gt;I now have to change all of my exception handling...&lt;/h4&gt;

There is now also the problem with the exceptions that the service was originally throwing because all of my catch blocks need to be changed to catch the fault exceptions. They are no longer catching the original exceptions which means that it becomes even more difficult just to replace the WCF service at runtime with an in-line version. There is now a dependency, the client knows that it is talking via WCF and should the client actually care? Should any logic within the client care about how it is communicating with the service?

Again, the work-around is to catch both the original exceptions and the fault exceptions but that is really starting to make your code ugly and any logic is now still very dependent on knowing that we are talking over WCF.

&lt;h4&gt;The client is now disposable...&lt;/h4&gt;

When we talk over WCF a channel is opened to the service by the automatically generated client and this resource needs to be released which is why the generated clients are disposable. All calls now need to be wrapped within a using statement so that we can correctly dispose of the resource. 

But what happens if there is an error? Well, on error dispose is not the method that you should call and a lot of people do not notice this. When an exception is thrown by the service then it is the client&#39;s job to call &lt;strong&gt;Abort()&lt;/strong&gt; on the service to correctly dispose of any resource.

All of this is causing the client code to change significantly for no actual benefit, in my opinion all of this should be handled internally within the generated classes. 

&lt;h3&gt;So what next?&lt;/h3&gt;

I have now finished ranting about the problems of using &quot;Add service reference&quot; and there are people out there who will tell me that using &lt;strong&gt;ChannelFactory&amp;lt;T&amp;gt;&lt;/strong&gt; would solve some of my issues above and to those people I would say that you are partially correct but &lt;strong&gt;ChannelFactory&amp;lt;T&amp;gt;&lt;/strong&gt; is the subject of my next post. 

Once I have finished explaining the problems of &lt;strong&gt;ChannelFactory&amp;lt;T&amp;gt;&lt;/strong&gt; I will follow that with a post showing my solution to this problem as I release an open source WCF client generator to handle the issues above, so stay tuned...
</content>
    </entry>
  
    <entry>
      <title>DDD8</title>
      <link href="http://ninjaferret.github.com/2010/02/01/ddd8.html"/>
      <updated>2010-02-01T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2010/02/01/ddd8</id>
      <content type="html">I believe that I was one of the lucky ones who managed to get a place at &lt;strong&gt;DeveloperDeveloperDeveloper 8&lt;/strong&gt; (&lt;strong&gt;DDD8&lt;/strong&gt;) at the Microsoft Campus in Reading. I say I was lucky because I believe that the 400 available places were snapped up in 12 minutes and another 200 people were put on the reserve list within the next 12 minutes. Sadly, I had to get the first train of the day to get to Reading.

There were a lot of different sessions that I wanted to attend but I could be at only attend one session at a time as I haven&#39;t yet mastered the ability of being in multiple places at once.

&lt;h4&gt;Real World MVC Architectures - Ian Cooper&lt;/h4&gt;


I am relatively new to ASP.NET MVC and have made a number of the mistakes that Ian Cooper was talking about.

Ian explained the &quot;Thin Controller/Fat Model&quot; pattern where the model/domain is where the majority of the work takes place and the controller provides more of the application logic. The controller needs to gather the required data, prepare it for the view then render the appropriate view. More of the business logic needs to be pushed down into the domain, or in the case for my sites into the service layer. However, we don&#39;t want a fat service layer either so the same principle applies, the logic from there needs to be pushed down into the domain from the service.

This certainly has given me some ideas that I need to take back to work and to my home projects

&lt;h4&gt;Hello Document Databases - Neil Robbins&lt;/h4&gt;

I wasn&#39;t sure whether to attend this one or go to the &lt;strong&gt;Multi-tenant ASP.NET MVC Projects (Or 30 very different customers and a single codebase)&lt;/strong&gt; by Rob Ashton but since I&#39;d been to one MVC talk I decided to look at something completely different.

In the end I think I made a good choice as Neil really gave me something to think about for my next projects at home. For a long time now RDBMS has been the &quot;only choice&quot; when people want to store data and to use the same quote Neil did &quot;No one ever got fired for choosing IBM&quot;. However, they aren&#39;t that fault tolerant and don&#39;t scale well for vast distributed systems (think Google).

Document databases are designed to be more fault tolerant, they are distributed so a node can disappear without bringing the entire system down but that is at the price of data consistency. The nodes communicate with each other and diff the data but there is a latency there so should a node go down some data may be lost.

Neil focussed on &lt;a href=&quot;http://couchdb.apache.org/&quot;&gt;Couch DB&lt;/a&gt; which he ran on Linux because it is easier to install (Linux at Microsoft shock!). The API is URL based and RESTful and he demonstrated it using a linux terminal and curl.

&lt;h4&gt;C#4.0 - Jon Skeet&lt;/h4&gt;

Well, what can I say about this talk? Jon is a fantastic presenter and he made it so easy to understand the new features of C# 4.0.

The main purpose of this release is to improve COM interop with a primary focus on getting better interop with Microsoft Office. C# is getting a number of features that were in VB that C# has been desperately needing to help with COM interop such as &lt;strong&gt;Named Arguments&lt;/strong&gt; and &lt;strong&gt;Default Parameters&lt;/strong&gt;.

I found Jon&#39;s explanation of covariance and contravariance extremely easy to follow, one of the best explanations that I have come across and I would say that if the video of his talk is put online I would recommend it just for that.

&lt;h4&gt;Lunchtime - grok talks&lt;/h4&gt;

At lunch I went to the grok talks and listened to topics that ranged from using CodeRush Xpress to trying to introduce Lean and Kanban into the workplace to the death of MSBuild, I&#39;m sure I&#39;m missing one out...

I found the Kanban talk very interesting and I need to read Chris&#39; &lt;a href=&quot;http://www.jobshy.com/shift-in/post/2010/01/05/Personal-Kanban.aspx&quot;&gt;blog&lt;/a&gt; on the subject.

Ben&#39;s talk on using &lt;a href=&quot;http://github.com/derickbailey/Albacore&quot;&gt;albacore&lt;/a&gt; to automate build/deployment processes was very interested and something that I need to read about more.

&lt;h4&gt;Not Everything is an Object - Gary Short&lt;/h4&gt;

I was debating to go to this talk or the entity framework talk but in the end I came to the conclusion that this sounded a little different and therefore a bit more interesting.

Gary began right at the very beginnings of programming and his talk began very much like a &quot;brief history of everything&quot; as he delved into discussions on quantum limitations on chip size and into quantum tunnelling.

I was intrigued by his thoughts that the reasons Object Orientation is failing to solve current world problems is that we are, in essence, trying to model something and we aren&#39;t very good modellers. He also brought us back to the concept that objects hide data whereas data structures present data but no behaviour. In some ways this links very neatly in with some of what Ian Cooper was saying in his talk earlier (about a fatter model).

Back onto the subject matter at hand, objects also have shared state that is not thread safe and as software gets more complex and processors get more cores rather than faster we need to focus on parallel processing (and threading isn&#39;t nice in the OO world).

Gary introduced a JVM based functional language called &quot;Clojure&quot; and gave us a brief introduction to the language and the benefits of a functional language where everything is immutable. I am seriously going to take a look at Clojure or another functional language.

&lt;h4&gt;Automating Testing With Windows Virtual PC - Guy Smith-Ferrier&lt;/h4&gt;

This was one of the talks that I was interested in going to from the start as automating testing through a Virtual PC is something I am about to start looking into.

There are several ways to do most actions, Guy showed us the easy way and the hard way (e.g. auto-login on windows rather than trying to automate login - using a sequence of characters into the keyboard buffer).

This talk has given me lots to think about and ideas about how to automate the deployment and build processes using the simple methods, batch files in the start-up folder of the Virtual PC that do the install and then run the tests. I also like Guy&#39;s idea of using shared hard disks that can be used between different VPC images.

&lt;h4&gt;Final thoughts&lt;/h4&gt;

Firstly, thanks very much to the organisers, speakers, helpers and sponsors for providing a phenomenal free conference. The Microsoft facilities and staff were again fantastic so thanks very much to everyone involved from Microsoft.

I have learned a lot today and have been to talks on topics that I would never have even considered looking at before and I hope to take a look at a lot of what was discussed here over the next few months.

Hopefully I&#39;ll be there next year, perhaps I might even think about doing a grok talk.
</content>
    </entry>
  
    <entry>
      <title>MISL - 5. Lambda Expressions</title>
      <link href="http://ninjaferret.github.com/2010/01/04/misl-5-lambda-expressions.html"/>
      <updated>2010-01-04T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2010/01/04/misl-5-lambda-expressions</id>
      <content type="html">This is the last of the &quot;tutorial&quot; posts on MSIL. There is a massive amount that I have not had time to cover here but this is, as I said in my first post, the story of my journey through MSIL. You will find the source code for this blog post on my &lt;a href=&quot;http://code.assembla.com/NinjaFerretDemos/subversion/nodes/Reflection/LambdaExpressions&quot;&gt;subversion repository&lt;/a&gt;.

For this one last time I want to carry on the Calculator for one final time. This time I want to explore the use of Lambda expressions and to introduce a pattern similar to the one that I will make use of in later posts.

The implementation of the Calculator interface then looks identical for each method:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
// Generated class
public class Calc : Calculator
{
	public int Add(int a, int b)
	{
		int result = 0;
		Evaluator.Evaluate(() =&amp;gt; result = a + b);
		return result;
	}

	...
}

public delegate void CalcFunction(int a, int b);

public static class Evaluator
{
	public static void Evaluate(CalcFunction codeBlock)
	{
		codeBlock();
	}
}
&lt;/pre&gt;&lt;/code&gt;

&lt;h4&gt;Defining the Lambda Expression&lt;/h4&gt;

As always, the first task is to make a test implementation in order use ILDASM to see what has been generated. Looking at the generated type &lt;strong&gt;TestCalculator&lt;/strong&gt; there are four nested types:

&lt;ul&gt;
    &lt;li&gt;&amp;lt;&amp;gt;__DisplayClass1&lt;/li&gt;
	&lt;li&gt;&amp;lt;&amp;gt;__DisplayClass4&lt;/li&gt;
	&lt;li&gt;&amp;lt;&amp;gt;__DisplayClass7&lt;/li&gt;
	&lt;li&gt;&amp;lt;&amp;gt;__DisplayClassa&lt;/li&gt;
&lt;/ul&gt;

These are the internal representation of the lambda expressions. Each lambda expression has it&#39;s own class nested inside the class in which it was defined so it is necessary to generate these nested types using IL. For the most part how a nested type is created is very similar to creating a normal type, it is only how the &lt;strong&gt;TypeBuilder&lt;/strong&gt; is created that is different using the method &lt;strong&gt;typeBuilder.DefineNestedType(string.Format(&quot;&amp;lt;&amp;gt;__DisplayClass{0}&quot;, index, NestedTypeAttributes);&lt;/strong&gt;.

The nested types each have three public fields &lt;strong&gt;a&lt;/strong&gt;, &lt;strong&gt;b&lt;/strong&gt; and &lt;strong&gt;result&lt;/strong&gt;; based on the lambda expression &lt;strong&gt;() =&amp;gt; result = a + b&lt;/strong&gt; it should be easy to see where these fields have been derived from.

Finally, there is the method which again has a special format, e.g. &lt;strong&gt;&amp;lt;Add&amp;gt;b__0&lt;/strong&gt; and &lt;strong&gt;&amp;lt;Subtract&amp;gt;b__3&lt;/strong&gt;, which has again been auto-generated based on the name of the method that the lambda expression was defined in and an incrementing number to permit uniqueness. The IL for these methods is very simple, load the two fields (&lt;strong&gt;a&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt;) add/subtract/divide/multiple and save the result in the &lt;strong&gt;result&lt;/strong&gt; field.

The &lt;strong&gt;DefineLambdaExpression&lt;/strong&gt; method in my example code creates the nested type, defines the default constructor, defines the fields and implements the method. The final act is to then create the nested type using the &lt;strong&gt;TypeBuilder.CreateType()&lt;/strong&gt; method.

&lt;h4&gt;Why can&#39;t I use the generated type yet?&lt;/h4&gt;

Once the nested type has been created it must surely be a usable type? When generating the code that will call this lambda expression I initially expected to be able to use &lt;strong&gt;Type.GetConstructor()&lt;/strong&gt;, &lt;strong&gt;Type.GetMethod()&lt;/strong&gt; and &lt;strong&gt;Type.GetField()&lt;/strong&gt; to get access to the required members of the lambda expression. I was wrong, the first thing I needed to do was to delcare a local of the lambda expression type but I get the exception:

&lt;code&gt;&lt;pre class=&quot;brush:plain&quot;&gt;
System.TypeLoadException: Could not load type &#39;Calculator&#39; from assembly &#39;Calculatorb1d3a726-fa47-47cc-b190-ff5403de4ad7, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null&#39;.
   at System.RuntimeTypeHandle.GetDeclaringType(RuntimeType type)
   at System.RuntimeType.RuntimeTypeCache.GetEnclosingType()
   at System.RuntimeType.get_DeclaringType()
   at System.Reflection.Emit.ModuleBuilder.GetTypeRefNested(Type type, Module refedModule, String strRefedModuleFileName)
   at System.Reflection.Emit.ModuleBuilder.GetTypeTokenWorkerNoLock(Type type, Boolean getGenericDefinition)
   at System.Reflection.Emit.ModuleBuilder.GetTypeTokenInternal(Type type, Boolean getGenericDefinition)
   at System.Reflection.Emit.SignatureHelper.AddOneArgTypeHelperWorker(Type clsArgument, Boolean lastWasGenericInst)
   at System.Reflection.Emit.SignatureHelper.AddArgument(Type argument, Boolean pinned)
   at System.Reflection.Emit.ILGenerator.DeclareLocal(Type localType, Boolean pinned)
   at System.Reflection.Emit.ILGenerator.DeclareLocal(Type localType)
   at LambdaExpressions.Program.DefineMethod(MethodInfo method, TypeBuilder typeBuilder, LambdaExpressionDetails lambdaExpression) in D:\code\dotNet\BlogDemos\Reflection\LambdaExpressions\Program.cs:line 150
   at LambdaExpressions.Program.Main(String[] args) in D:\code\dotNet\BlogDemos\Reflection\LambdaExpressions\Program.cs:line 54
&lt;/pre&gt;&lt;/code&gt;

So, even though the nested type has compiled it cannot be used directly until the containing type has been fully defined, but the containing type is the one that I am still building. Thankfully, &lt;strong&gt;TypeBuilder&lt;/strong&gt; inherits &lt;strong&gt;Type&lt;/strong&gt; so I can simply use the &lt;strong&gt;TypeBuilder&lt;/strong&gt; when declaring the local variable.

The same occurs trying to use the &lt;strong&gt;Type.GetConstructor()&lt;/strong&gt;, &lt;strong&gt;Type.GetMethod()&lt;/strong&gt; or &lt;strong&gt;Type.GetField()&lt;/strong&gt; methods on either the created type or on the &lt;strong&gt;TypeBuilder&lt;/strong&gt;. However, when creating each of these members the appropriate method used returns a &lt;strong&gt;XXXBuilder&lt;/strong&gt; object that inherits from its equivalent and can be used in its place:

&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;ConstructorBuilder&lt;/strong&gt; inherits &lt;strong&gt;ConstructorInfo&lt;/strong&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;MethodBuilder&lt;/strong&gt; inherits &lt;strong&gt;MethodInfo&lt;/strong&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;FieldBuilder&lt;/strong&gt; inherits &lt;strong&gt;FieldInfo&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

Hence, the &lt;strong&gt;DefineLambdaExpression&lt;/strong&gt; method, stores these objects in a &lt;strong&gt;LambdaExpressionDetails&lt;/strong&gt; object that can be passed on to the method where the call into this lambda expression will be made.

&lt;h4&gt;Making the call&lt;/h4&gt;

The creation of the type and the initial definition of the methods of the &lt;strong&gt;ICalculator&lt;/strong&gt; interface should be identical to that in previous posts; it is only the implementation that will differ.

The four methods again look very similar, the only difference between them is the lambda expression that they are using:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
var result = 0;
Evaluator.Evaluate(() =&amp;gt; result = a * b);
return result;
&lt;/pre&gt;&lt;/code&gt;

As before, the test implementation and ILDASM shows how the calling method is constructed. The contents of each method can be distilled to the following (ignoring what has gone before in previous posts):

&lt;ul&gt;
   &lt;li&gt;Declare a &lt;em&gt;lamdba expression&lt;/em&gt; local variable &lt;/li&gt;
   &lt;li&gt;Declare the &lt;em&gt;result&lt;/em&gt; local variable&lt;/li&gt;
   &lt;li&gt;Create a new instance of the lambda expression using the &lt;strong&gt;LambdaExpressionDetails.Constructor&lt;/strong&gt; property&lt;/li&gt;
   &lt;li&gt;Save it to the &lt;em&gt;lambda expression&lt;/em&gt; local variable&lt;/li&gt;
   &lt;li&gt;For each argument:
      &lt;ul&gt;
	     &lt;li&gt;Push the &lt;em&gt;lambda expression&lt;/em&gt; local variable onto the stack&lt;/li&gt;
		 &lt;li&gt;Push the appropriate argument onto the stack&lt;/li&gt;
		 &lt;li&gt;Store the value into the appropriate field on the lambda expression using the &lt;strong&gt;LambdaExpressionDetails.Method&lt;/strong&gt; property&lt;/li&gt;
	  &lt;/ul&gt;
   &lt;/li&gt;
   &lt;li&gt;Load the &lt;em&gt;lambda expression&lt;/em&gt; local variable&lt;/li&gt;
   &lt;li&gt;Load the function pointer to the method of the lambda expression using &lt;/li&gt;
   &lt;li&gt;Create a new instance of the &lt;strong&gt;CalcFunction&lt;/strong&gt; delegate&lt;/li&gt;
   &lt;li&gt;Call the &lt;strong&gt;Evaluator.Evaluate()&lt;/strong&gt; method&lt;/li&gt;
   &lt;li&gt;Load the result field&lt;/li&gt;
   &lt;li&gt;Return the result&lt;/li&gt;
&lt;/ul&gt;

So the IL becomes:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
// Create the lambda expression object
il.Emit(OpCodes.Newobj, lambdaExpressionDetails.Constructor);
il.Emit(OpCodes.Stloc_0);

// Load all of the parameters into the lambda expression&#39;s fields
il.Emit(OpCodes.Ldloc_0);
il.Emit(OpCodes.Ldarg_1);
il.Emit(OpCodes.Stfld, 
   lambdaExpressionDetails.ParameterFields
      .SingleOrDefault(x =&amp;gt; x.Name.Equals(&quot;a&quot;)));
il.Emit(OpCodes.Ldloc_0);
il.Emit(OpCodes.Ldarg_2);
il.Emit(OpCodes.Stfld, 
   lambdaExpressionDetails.ParameterFields
      .SingleOrDefault(x =&amp;gt; x.Name.Equals(&quot;b&quot;)));
il.Emit(OpCodes.Nop);

// Set up the return value
il.Emit(OpCodes.Ldloc_0);
il.Emit(OpCodes.Ldc_I4_0);
il.Emit(OpCodes.Stfld, lambdaExpressionDetails.ResultField);

// Make the call
il.Emit(OpCodes.Ldloc_0);
il.Emit(OpCodes.Ldftn, lambdaExpressionDetails.Method);
var constructorInfo = typeof(EvaluationDelegate)
   .GetConstructor(new[] { typeof(object), typeof(IntPtr) });
il.Emit(OpCodes.Newobj, constructorInfo);
var methodInfo = typeof(Evaluator)
   .GetMethod(&quot;Evaluate&quot;, new[] { typeof(EvaluationDelegate) });
il.Emit(OpCodes.Call, methodInfo);
il.Emit(OpCodes.Ldloc_0);
il.Emit(OpCodes.Ldfld, lambdaExpressionDetails.ResultField);
il.Emit(OpCodes.Stloc_1);
il.Emit(OpCodes.Br_S, label);
il.MarkLabel(label);
il.Emit(OpCodes.Ldloc_1);
il.Emit(OpCodes.Ret);
&lt;/pre&gt;&lt;/code&gt;

&lt;h4&gt;Finally...&lt;/h4&gt;
Today, I have shown that lambda expressions are really nothing special under the hood, from what I can tell there are no features of &lt;strong&gt;MSIL&lt;/strong&gt; that make lambda expressions possible, they are made possible through features of the languages and the compilers that produce &lt;strong&gt;MSIL&lt;/strong&gt;. By investigating the structure of the generated &lt;strong&gt;MSIL&lt;/strong&gt; for this one case I hope to have demonstrated the principles behind dynamically generating your own lambda expressions for a variety of other cases.

That is it for my investigation into &lt;strong&gt;MSIL&lt;/strong&gt; and &lt;strong&gt;Refleciton.Emit&lt;/strong&gt;. My guiding principles throughout these tutorials has been to make an example of what I wish to produce then use &lt;strong&gt;ILDASM&lt;/strong&gt; to break the compiled assembly down into &lt;strong&gt;MSIL&lt;/strong&gt; to gain an understanding of what is produced. I hope that these tutorials will prove useful to anyone trying to make use of &lt;strong&gt;Reflection.Emit&lt;/strong&gt; but I hope that when I have achieved my ultimate goal (a generic WCF Service Client factory) will prove valuable and that will be the topic of my next blog post.
</content>
    </entry>
  
    <entry>
      <title>MSIL - 4. For Loops</title>
      <link href="http://ninjaferret.github.com/2009/12/23/msil-4-for-loops.html"/>
      <updated>2009-12-23T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2009/12/23/msil-4-for-loops</id>
      <content type="html">So for completeness I thought that I would introduce you to basic loops, although I have no need of them in my intended use of &lt;strong&gt;Reflection.Emit&lt;/strong&gt;. I am going to finish off my Calculator implementation by non-optimal implementation of the multiply method, I could make use of the &lt;strong&gt;OpCodes.Mul&lt;/strong&gt; operator in the same way as I have used &lt;strong&gt;OpCodes.Add&lt;/strong&gt; etc. but this would not demonstrate a basic for loop. As part of this I will also demonstrate calling a method of the generated class.

The source code for this can be found in my &lt;a href=&quot;http://code.assembla.com/NinjaFerretDemos/subversion/nodes/Reflection/ForLoop&quot;&gt;subversion repository&lt;/a&gt;.

&lt;h4&gt;Implementing Multiply&lt;/h4&gt;
I can create a loop that will make use of the &lt;strong&gt;Add()&lt;/strong&gt; method to calculate the multiplication:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
public int Multiply(int a, int b)
{
	int result = 0;
	for (int i = a; i &amp;gt; 0; i--)
	{
		result = Add(result, b);
	}
	return result;
}
&lt;/pre&gt;&lt;/code&gt;

So taking a look at ILDASM:

&lt;code&gt;&lt;pre class=&quot;brush: plain&quot;&gt;
.method public hidebysig newslot virtual final
        instance int32  Multiply(int32 a,
                                 int32 b) cil managed
{
  // Code size       36 (0x24)
  .maxstack  3
  .locals init ([0] int32 result,
           [1] int32 i,
           [2] int32 CS$1$0000,
           [3] bool CS$4$0001)
  IL_0000:  nop
  IL_0001:  ldc.i4.0
  IL_0002:  stloc.0
  IL_0003:  ldarg.1
  IL_0004:  stloc.1
  IL_0005:  br.s       IL_0016
  IL_0007:  nop
  IL_0008:  ldarg.0
  IL_0009:  ldloc.0
  IL_000a:  ldarg.2
  IL_000b:  call       instance int32 Inheritance.CalculatorImplementation::Add(int32,
                                                                                int32)
  IL_0010:  stloc.0
  IL_0011:  nop
  IL_0012:  ldloc.1
  IL_0013:  ldc.i4.1
  IL_0014:  sub
  IL_0015:  stloc.1
  IL_0016:  ldloc.1
  IL_0017:  ldc.i4.0
  IL_0018:  cgt
  IL_001a:  stloc.3
  IL_001b:  ldloc.3
  IL_001c:  brtrue.s   IL_0007
  IL_001e:  ldloc.0
  IL_001f:  stloc.2
  IL_0020:  br.s       IL_0022
  IL_0022:  ldloc.2
  IL_0023:  ret
} // end of method CalculatorImplementation::Multiply
&lt;/pre&gt;&lt;/code&gt;

The first thing is that we now have 4 local variables:
&lt;ul&gt;
   &lt;li&gt;&lt;strong&gt;[0] int32 result&lt;/strong&gt; - the integer variable to hold the &lt;em&gt;result&lt;/em&gt;&lt;/li&gt;
   &lt;li&gt;&lt;strong&gt;[1] int32 i&lt;/strong&gt; - the integer variable to hold the &lt;em&gt;loop counter&lt;/em&gt;&lt;/li&gt;
   &lt;li&gt;&lt;strong&gt;[2] int32 CS$1$0000&lt;/strong&gt; - an integer variable, created by the compiler, that holds the &lt;em&gt;return value&lt;/em&gt;&lt;/li&gt;
   &lt;li&gt;&lt;strong&gt;[3] bool CS$4$0001&lt;/strong&gt; - a boolean variable, created by the compiler, that holds the result of the &lt;em&gt;loop condition&lt;/em&gt; &lt;code&gt;for(int i = a; &lt;strong&gt;i &amp;gt; 0&lt;/strong&gt;; i--)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

Initially, the &lt;em&gt;result&lt;/em&gt; variable is assigned the constant 0 (&lt;strong&gt;OpCodes.Ldc_I4_0&lt;/strong&gt;) and the &lt;em&gt;loop counter&lt;/em&gt; is assigned the value of the first parameter (&lt;strong&gt;OpCodes.Ldarg_1&lt;/strong&gt;). The IL then switches execution to the location &lt;strong&gt;IL0016&lt;/strong&gt; that executes the condition and stores the result of that condition into the &lt;em&gt;loop condition&lt;/em&gt; variable. If the &lt;em&gt;loop condition&lt;/em&gt; variable is true execution then returns to location &lt;strong&gt;IL0007&lt;/strong&gt; to execute the main body of the loop. Note the use of the &lt;strong&gt;OpCodes.Cgt - greater than&lt;/strong&gt; code.

In the main body of the loop the &lt;strong&gt;this&lt;/strong&gt; parameter (parameter 0) is pushed onto the stack followed by the current &lt;em&gt;result&lt;/em&gt; variable and the second parameter. The stack is now set-up for a call into the &lt;strong&gt;Add&lt;/strong&gt; method and the result is stored into the &lt;em&gt;result&lt;/em&gt; variable. The &lt;em&gt;loop counter&lt;/em&gt; variable is then decremented and the condition is evaluated once more.

When the &lt;em&gt;loop condition&lt;/em&gt; is false, the &lt;em&gt;result variable&lt;/em&gt; is stored into the &lt;em&gt;return&lt;/em&gt; variable which is then returned to the caller.

So putting this into code:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
private static void DefineMultiplyMethod(TypeBuilder typeBuilder, Type interfaceType)
{
	var interfaceMethod = interfaceType.GetMethod(&amp;quot;Multiply&amp;quot;);
	var addMethod = interfaceType.GetMethod(&amp;quot;Add&amp;quot;);
	var addParameters = addMethod
		.GetParameters()
		.Select(parameter =&amp;gt; parameter.ParameterType)
		.ToArray();
	var methodBuilder = typeBuilder.DefineMethod(&amp;quot;Multiply&amp;quot;,
                             MethodAttributes.Public |
                             MethodAttributes.Virtual |
                             MethodAttributes.Final |
                             MethodAttributes.NewSlot |
                             MethodAttributes.HideBySig);
	methodBuilder.SetReturnType(interfaceMethod.ReturnType);
	var parameters = interfaceMethod
                             .GetParameters()
                             .Select(parameter =&amp;gt; parameter.ParameterType)
                             .ToArray();
	methodBuilder.SetParameters(parameters);
	methodBuilder.InitLocals = true;

	var il = methodBuilder.GetILGenerator();
	var returnLabel = il.DefineLabel();
	var decisionLabel = il.DefineLabel();
	var loopStartLabel = il.DefineLabel();
	il.DeclareLocal(typeof(int));
	il.DeclareLocal(typeof(int));
	il.DeclareLocal(typeof(int));
	il.DeclareLocal(typeof(bool));
	il.Emit(OpCodes.Nop);
	il.Emit(OpCodes.Ldc_I4_0);
	il.Emit(OpCodes.Stloc_0);
	il.Emit(OpCodes.Ldarg_1);
	il.Emit(OpCodes.Stloc_1);
	il.Emit(OpCodes.Br_S, decisionLabel);
	il.MarkLabel(loopStartLabel);
	il.Emit(OpCodes.Nop);
	il.Emit(OpCodes.Ldarg_0);
	il.Emit(OpCodes.Ldloc_0);
	il.Emit(OpCodes.Ldarg_2);
	il.EmitCall(OpCodes.Call, addMethod, addParameters);
	il.Emit(OpCodes.Stloc_0);
	il.Emit(OpCodes.Nop);
	il.Emit(OpCodes.Ldloc_1);
	il.Emit(OpCodes.Ldc_I4_1);
	il.Emit(OpCodes.Sub);
	il.Emit(OpCodes.Stloc_1);
	il.MarkLabel(decisionLabel);
	il.Emit(OpCodes.Ldloc_1);
	il.Emit(OpCodes.Ldc_I4_0);
	il.Emit(OpCodes.Cgt);
	il.Emit(OpCodes.Stloc_3);
	il.Emit(OpCodes.Ldloc_3);
	il.Emit(OpCodes.Brtrue_S, loopStartLabel);
	il.Emit(OpCodes.Ldloc_0);
	il.Emit(OpCodes.Stloc_2);
	il.MarkLabel(returnLabel);
	il.Emit(OpCodes.Ldloc_2);
	il.Emit(OpCodes.Ret);
	typeBuilder.DefineMethodOverride(methodBuilder, interfaceMethod);
}
&lt;/pre&gt;&lt;/code&gt;

I now have a fully functional and fully working calculator class that implements the &lt;strong&gt;Calculator&lt;/strong&gt; interface and has been generated dynamically at runtime.

Next time, I will be looking into lambda expressions...
</content>
    </entry>
  
    <entry>
      <title>MSIL - 3. If Statements...</title>
      <link href="http://ninjaferret.github.com/2009/12/19/msil-3-if-statements-.html"/>
      <updated>2009-12-19T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2009/12/19/msil-3-if-statements-</id>
      <content type="html">For my third post in the MSIL series I want to focus on a simple if statement, how do I represent the code &lt;strong&gt;if (x != 0)&lt;/strong&gt; using IL?

The source code for this can be found in my &lt;a href=&quot;http://code.assembla.com/NinjaFerretDemos/subversion/nodes/Reflection/If&quot;&gt;subversion repository&lt;/a&gt;.

&lt;h4&gt;Implementing division&lt;/h4&gt;

I want to take a look at the &lt;strong&gt;Divide()&lt;/strong&gt; method of the Calculator class:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
public int Divide(int numerator, int denominator)
{
	if (denominator != 0)
	{
		throw new ArgumentException(&amp;quot;denominator cannot be zero&amp;quot;,
		                            &amp;quot;denominator&amp;quot;);
	}
	return numerator/denominator;
}
&lt;/pre&gt;&lt;/code&gt;

This gives me the opportunity to show the implementation of conditional statements and the throwing of exceptions. What would this implementation generate in IL terms for handling the equality?

&lt;code&gt;&lt;pre class=&quot;brush: plain&quot;&gt;
.method public hidebysig newslot virtual final
        instance int32  Divide(int32 numerator,
                               int32 denominator) cil managed
{
  // Code size       37 (0x25)
  .maxstack  3
  .locals init ([0] int32 CS$1$0000,
           [1] bool CS$4$0001)
  IL_0000:  nop
  IL_0001:  ldarg.2
  IL_0002:  ldc.i4.0
  IL_0003:  ceq
  IL_0005:  ldc.i4.0
  IL_0006:  ceq
  IL_0008:  stloc.1
  IL_0009:  ldloc.1
  IL_000a:  brtrue.s   IL_001d
  IL_000c:  nop
  IL_000d:  ldstr      &quot;denominator cannot be zero&quot;
  IL_0012:  ldstr      &quot;denominator&quot;
  IL_0017:  newobj     instance void [mscorlib]System.ArgumentException::.ctor(string,
                                                                               string)
  IL_001c:  throw
  IL_001d:  ldarg.1
  IL_001e:  ldarg.2
  IL_001f:  div
  IL_0020:  stloc.0
  IL_0021:  br.s       IL_0023
  IL_0023:  ldloc.0
  IL_0024:  ret
} // end of method CalculatorImplementation::Divide
&lt;/pre&gt;&lt;/code&gt;

The &lt;strong&gt;if&lt;/strong&gt; statement is executed between &lt;strong&gt;IL_0001&lt;/strong&gt; and &lt;strong&gt;IL000a&lt;/strong&gt;. The first thing is to push onto the stack the demoninator (&lt;strong&gt;ldarg.2&lt;/strong&gt;) and the constant integer zero (&lt;strong&gt;ldc.i4.0&lt;/strong&gt;) then compare them using the equals comparison operator (&lt;strong&gt;ceq&lt;/strong&gt;), which pushes either a &lt;strong&gt;1&lt;/strong&gt; onto the stack if it is true and &lt;strong&gt;0&lt;/strong&gt; if it is false. As I am doing a &lt;strong&gt;not equals&lt;/strong&gt; comparison we then compare the result of the first comparison with &lt;strong&gt;0&lt;/strong&gt; to find out if the two items do not match, the result of this equality is placed back onto the stack.

What do we know now? &lt;strong&gt;brtrue.s   IL_001d&lt;/strong&gt; tells the compiler that if the calculation above (i.e. is the denominator not equal to zero) is true then jump past the exception throwing statements to the calculation part at position &lt;strong&gt;IL001d&lt;/strong&gt; that follows the standard pattern of the Add and Subtract methods described in my first post.


The resulting IL looks like:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
private static void DefineDivideMethod(TypeBuilder typeBuilder, Type interfaceType)
{
	var interfaceSubtractMethod = interfaceType.GetMethod(&amp;quot;Divide&amp;quot;);
	var methodBuilder = typeBuilder.DefineMethod(&amp;quot;Divide&amp;quot;,
             MethodAttributes.Public |
             MethodAttributes.Virtual |
             MethodAttributes.Final |
             MethodAttributes.NewSlot |
             MethodAttributes.HideBySig);
	methodBuilder.SetReturnType(interfaceSubtractMethod.ReturnType);
	var parameters = interfaceSubtractMethod
             .GetParameters()
             .Select(parameter =&amp;gt; parameter.ParameterType)
             .ToArray();
	methodBuilder.SetParameters(parameters);
	methodBuilder.InitLocals = true;

	var il = methodBuilder.GetILGenerator();
	var returnLabel = il.DefineLabel();
	var trueLabel = il.DefineLabel();
	il.DeclareLocal(typeof (int));
	il.DeclareLocal(typeof (bool));
	il.Emit(OpCodes.Nop);
	il.Emit(OpCodes.Ldarg_2);
	il.Emit(OpCodes.Ldc_I4_0);
	il.Emit(OpCodes.Ceq);
	il.Emit(OpCodes.Ldc_I4_0);
	il.Emit(OpCodes.Ceq);
	il.Emit(OpCodes.Stloc_1);
	il.Emit(OpCodes.Ldloc_1);
	il.Emit(OpCodes.Brtrue_S, trueLabel);
	il.Emit(OpCodes.Nop);
	il.Emit(OpCodes.Ldstr, &amp;quot;denominator cannot be zero&amp;quot;);
	il.Emit(OpCodes.Ldstr, &amp;quot;demoninator&amp;quot;);
	il.Emit(OpCodes.Newobj, typeof (ArgumentException).GetConstructor(new[] {typeof (string), typeof (string)}));
	il.Emit(OpCodes.Throw);
	il.MarkLabel(trueLabel);
	il.Emit(OpCodes.Ldarg_1);
	il.Emit(OpCodes.Ldarg_2);
	il.Emit(OpCodes.Div);
	il.Emit(OpCodes.Stloc_0);
	il.Emit(OpCodes.Br_S, returnLabel);
	il.MarkLabel(returnLabel);
	il.Emit(OpCodes.Ldloc_0);
	il.Emit(OpCodes.Ret);
	typeBuilder.DefineMethodOverride(methodBuilder, interfaceSubtractMethod);
}
&lt;/pre&gt;&lt;/code&gt;

So there it is, a simple if statement. The IL is starting to look increasingly complicated and it is becoming very clear that trying to dynamically generate code for anything but simple methods is increasingly difficult. What I will go onto in the very next post is how to do a simple loop to complete the fundamentals of programming.
</content>
    </entry>
  
    <entry>
      <title>MSIL - 2. Implementing Interfaces</title>
      <link href="http://ninjaferret.github.com/2009/12/14/msil-2-implementing-interfaces.html"/>
      <updated>2009-12-14T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2009/12/14/msil-2-implementing-interfaces</id>
      <content type="html">Following on from my previous blog post I intend to show you how to take the basics that I presented and extend them to become much more useful. One of the problems with the previous example was that there was absolutely no way when writing the test code to use the object that had been dynamically created without using even more reflection. The answer to this is to start implementing interfaces so that the compiled code knows the interface for the dynamically generated methods.

Once again, all of the source code can be downloaded from the &lt;a href=&quot;http://code.assembla.com/NinjaFerretDemos/subversion/nodes/Reflection/Interfaces&quot;&gt;here&lt;/a&gt;.

&lt;h4&gt;The interface&lt;/h4&gt;
I am going to continue on with the Calculator theme:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
public interface Calculator
{
	int Add(int a, int b);
	int Subtract(int a, int b);
	int Multiply(int a, int b);
	int Divide(int numerator, int denominator);
}
&lt;/pre&gt;&lt;/code&gt;

&lt;h4&gt;Implementing a method&lt;/h4&gt;
In my last blog post I showed you how to create a method using &lt;strong&gt;Reflection.Emit&lt;/strong&gt;, so staying with the &lt;strong&gt;Add()&lt;/strong&gt; method I will show you how I would create the method based on the interface. The IL inside the method will not change so the primary changes will come from simply defining the method.

&lt;h4&gt;1. Mark the class as implementing the interface&lt;/h4&gt;

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
var interfaceType = typeof(Calculator);
...
typeBuilder.AddInterfaceImplementation(interfaceType);
&lt;/pre&gt;&lt;/code&gt;

&lt;h4&gt;2. Define the method&lt;/h4&gt;
The definition of the method is almost identical to the definition of the method shown in the previous post:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
var interfaceAddMethod = interfaceType.GetMethod(&amp;quot;Add&amp;quot;);

var methodBuilder = typeBuilder.DefineMethod(&amp;quot;Add&amp;quot;, MethodAttributes.Public |
                    MethodAttributes.Virtual |
                    MethodAttributes.Final |
                    MethodAttributes.NewSlot |
                    MethodAttributes.HideBySig);
methodBuilder.SetReturnType(interfaceAddMethod.ReturnType);
var parameters = interfaceAddMethod
    .GetParameters()
    .Select(parameter =&amp;gt; parameter.ParameterType)
    .ToArray();
methodBuilder.SetParameters(parameters);
...
var il = methodBuilder.GetILGenerator();
...
typeBuilder.DefineMethodOverride(methodBuilder, interfaceAddMethod);
&lt;/pre&gt;&lt;/code&gt;

One of the most interesting things that have discovered on my journey through IL I realised that the implementing method is marked as &lt;strong&gt;Virtual&lt;/strong&gt; and &lt;strong&gt;Final&lt;/strong&gt; in order to implement the interface. The attribute &lt;strong&gt;HideBySig&lt;/strong&gt; indicates that this method hides all methods in base classes and interfaces with the same signature and the attribute &lt;strong&gt;NewSlot&lt;/strong&gt; tells the compiler to allocate a new spot in the V-Table.

A lot of work has already been done for us in defining the interface, the parameters and the return type are defined for us. After generating the IL for the method there is one final call to make &lt;strong&gt;typeBuilder.DefineMethodOverride()&lt;/strong&gt; passing in the generated method and the method from the interface that it is overriding.

&lt;h4&gt;3. Defining Multiply and Divide as Not Implemented&lt;/h4&gt;
For now I do not need to implement these methods and I would like to throw a &lt;strong&gt;NotImplementedException&lt;/strong&gt; rather than worrying about writing the IL now. The method bodies become very simple:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
var il = methodBuilder.GetILGenerator();
il.Emit(OpCodes.Ldstr, &amp;quot;The Multiply method has not been implemented yet&amp;quot;);
il.Emit(OpCodes.Newobj, typeof(NotImplementedException).GetConstructor(new [] {typeof(string)}));
il.Emit(OpCodes.Throw);
&lt;/pre&gt;&lt;/code&gt;

It simply pushes the string onto the stack and uses it to create a new exception that is then immediately throws.

&lt;h4&gt;What is next?&lt;/h4&gt;
I have shown you how to implement an interface and the basic mechanisms of creating a new object (in this case an exception). The next stage is to take the calculator implementation further using the &lt;strong&gt;Divide&lt;/strong&gt; method which will take us into writing an &lt;strong&gt;if&lt;/strong&gt; statement to detect if the denominator is 0.
</content>
    </entry>
  
    <entry>
      <title>MSIL - 1. First steps</title>
      <link href="http://ninjaferret.github.com/2009/12/13/msil-1-first-steps.html"/>
      <updated>2009-12-13T00:00:00+00:00</updated>
      <id>http://ninjaferret.github.com/2009/12/13/msil-1-first-steps</id>
      <content type="html">In my first blog on this process I want to start simple, creating a basic calculator class that can Add and Subtract, using &lt;strong&gt;Reflection.Emit&lt;/strong&gt; to give a basic introduction IL and some of the operations that I have encountered. In the subsequent blogs I will then build on this knowledge until I can achieve a specific goal of dynamically generating WCF clients at runtime.

All of the source code for this blog can be found at &lt;a href=&quot;http://code.assembla.com/NinjaFerretDemos/subversion/nodes/Reflection/FirstSteps&quot;&gt;here&lt;/a&gt;.

&lt;h4&gt;What does IL look like?&lt;/h4&gt;
Not knowing IL I decided that the first thing to do was to write the class in C# properly, compile it then use ILDASM to take a look at the resulting IL.

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
public class Calculator
{
   public int Add(int a, int b) { return a + b; }
   public int Subtract(int a, int b) { return a - b; }
}
&lt;/pre&gt;&lt;/code&gt;

Just looking a the Add method for now:

&lt;code&gt;
.method public hidebysig instance int32  Add(int32 a,
                     int32 b) cil managed
{
  // Code size       9 (0x9)
  .maxstack  2
  .locals init ([0] int32 CS$1$0000)
  IL_0000:  nop
  IL_0001:  ldarg.1
  IL_0002:  ldarg.2
  IL_0003:  add
  IL_0004:  stloc.0
  IL_0005:  br.s       IL_0007
  IL_0007:  ldloc.0
  IL_0008:  ret
} // end of method TestCalculator::Add
&lt;/code&gt;

What does this all mean?

&lt;ul&gt;
	&lt;li&gt;&lt;strong&gt;.maxstack 2&lt;/strong&gt; tells the runtime to expect a maximum of 2 items on the stack during the execution of this method.&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;.locals init ([0] int32 CS$1$0000)&lt;/strong&gt; creates a new local variable of type &lt;strong&gt;Int32&lt;/strong&gt; that will hold the result of the sum.&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;IL_0000:  nop&lt;/strong&gt; is an empty operation that does not have any affect on the system.&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;IL_0001:  ldarg.1&lt;/strong&gt; pushes argument 1 onto the stack - note that argument 0 for an instance method is the &lt;strong&gt;this&lt;/strong&gt; parameter.&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;IL_0002:  ldarg.2&lt;/strong&gt; pushes argument 2 onto the stack.&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;IL_0003:  add&lt;/strong&gt; pops the two integers on the stack and adds them together, putting the result onto the stack.&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;IL_0004:  stloc.0&lt;/strong&gt; pops the item on the stack and stores it into the local variable at location 0 (defined above).&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;IL_0005:  br.s       IL_0007&lt;/strong&gt; transfers execution to the specified location. In my opinion it is almost redundant in this case as execution is being transferred to the next statement.&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;IL_0007:  ldloc.0&lt;/strong&gt; loads the value from the local variable and pushes it onto the stack&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;IL_0008:  ret&lt;/strong&gt; returns the value on the stack back to the caller&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Generating this at runtime...&lt;/h4&gt;

I am now ready to begin auto-generating the above IL and I will be using the following basic pattern:
&lt;ol&gt;
	&lt;li&gt;Create a dynamic assembly to hold the new types&lt;/li&gt;

	&lt;li&gt;Create the &lt;strong&gt;TypeBuilder&lt;/strong&gt; to generate the type&lt;/li&gt;
	&lt;li&gt;Create each required method using a &lt;strong&gt;MethodBuilder&lt;/strong&gt;&lt;/li&gt;
	&lt;li&gt;Create the actual type&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;1. Creating the dynamic assembly&lt;/h4&gt;

Before starting to generate types I have to first create an assembly and module in which to hold them:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
var assemblyName = new AssemblyName {Name = &amp;quot;Calculator&amp;quot;};
var assemblyBuilder = Thread.GetDomain().DefineDynamicAssembly(assemblyName,
                            AssemblyBuilderAccess.RunAndSave);
var modBuild = assemblyBuilder.DefineDynamicModule(&amp;quot;CalculatorModule&amp;quot;,
                    string.Format(&amp;quot;{0}.dll&amp;quot;, assemblyName.Name)
&lt;/pre&gt;&lt;/code&gt;

This simply defines a new dynamic assembly the &lt;strong&gt;AssemblyBuilderAccess.RunAndSave&lt;/strong&gt; allows the code to be both executed and saved as an assembly to the file system. It does not automatically save the assembly, I will come to that later, but the ability to save can come in very handy when debugging the generated code. An alternative is to use &lt;strong&gt;AssemblyBuilderAccess.Run&lt;/strong&gt; which will simply allow the assembly to be executed in memory.

(Note: I would normally suffix the assembly name with a GUID or timestamp so that it can be uniquely identified, especially if saving to the file system)

&lt;h4&gt;2. Create the TypeBuilder&lt;/h4&gt;

The &lt;strong&gt;ModuleBuilder.DefineType()&lt;/strong&gt; method is used to create the type builder:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
var typeBuilder = modBuild.DefineType(&amp;quot;Calculator&amp;quot;,
                          TypeAttributes.Public |
                          TypeAttributes.Class |
                          TypeAttributes.AutoLayout |
                          TypeAttributes.AnsiClass |
                          TypeAttributes.BeforeFieldInit);
&lt;/pre&gt;&lt;/code&gt;

The &lt;strong&gt;TypeAttributes&lt;/strong&gt; were simply taken from what ILDASM was showing me from my compiled implementation of this code. We can see that this is now a public class but there are a few interesting attributes here that I had not seen before:

&lt;ul&gt;
	&lt;li&gt;&lt;strong&gt;TypeAttributes.AutoLayout&lt;/strong&gt; specifies that the fields are automatically laid out by the Common Language Runtime.&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;TypeAttributes.AnsiLayout&lt;/strong&gt; tells the underlying runtime that LPSTR is to be interpreted as an ANSI string.&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;TypeAttributes.BeforeFieldInit&lt;/strong&gt; states that calling static methods of the type does not force the system to initialize the type.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;3. Create the methods&lt;/h4&gt;

Methods are created using the &lt;strong&gt;TypeBuilder.DefineMethod()&lt;/strong&gt; method to create a &lt;strong&gt;MethodBuilder&lt;/strong&gt; and then using the &lt;strong&gt;MethodBuilder.GetILGenerator()&lt;/strong&gt; method to retrieve the object that we will use to generate the code:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
var methodBuilder = typeBuilder.DefineMethod(&amp;quot;Add&amp;quot;,
                    MethodAttributes.Public | MethodAttributes.Final);
methodBuilder.SetReturnType(typeof(int));
methodBuilder.SetParameters(new[] {typeof (int), typeof (int)});
methodBuilder.InitLocals = true;

var il = methodBuilder.GetILGenerator();
var label = il.DefineLabel();
il.DeclareLocal(typeof (int));
il.Emit(OpCodes.Ldarg_1);
il.Emit(OpCodes.Ldarg_2);
il.Emit(OpCodes.Add);
il.Emit(OpCodes.Stloc_0);
il.Emit(OpCodes.Br_S, label);
il.MarkLabel(label);
il.Emit(OpCodes.Ldloc_0);
il.Emit(OpCodes.Ret);
&lt;/pre&gt;&lt;/code&gt;

Hopefully, the first few lines are self-explanatory, define the method with the set attributes (I&#39;ll explore method attributes in more detail in a later post) then set the parameters and return type.

On line 5 the &lt;strong&gt;methodBuilder.InitLocals = true&lt;/strong&gt; tells the runtime to automatically initialises local variables to zero.

The following lines are simply emitting the operation codes identified in the IL that I generated above. Where you see &quot;_1&quot;, &quot;_2&quot; etc. suffixes there is a more generic version where you can pass a &lt;strong&gt;short&lt;/strong&gt; as the second parameter to &lt;strong&gt;il.Emit()&lt;/strong&gt; e.g. &lt;strong&gt;il.Emit(OpCodes.Ldarg_1)&lt;/strong&gt; can be written &lt;strong&gt;il.Emit(OpCodes.Ldarg, (short)1)&lt;/strong&gt;.

Lines 8, 14 and 15 show how to transfer the execution to another point. I have defined a label on line 5 then when I emit the break in execution on line 14 to tell the code to jump to that label but the label is not yet associated with a position in the IL, this happens on line 15 where I mark the label&#39;s position.

Now that the Add method has been defined I repeat almost the same IL for the subtract method but use  &lt;strong&gt;OpCodes.Sub&lt;/strong&gt; instead of &lt;strong&gt;OpCodes.Add&lt;/strong&gt;.

&lt;h4&gt;4. Produce the type&lt;/h4&gt;
So all that is left to do is to create the type and test it:

&lt;code&gt;&lt;pre class=&quot;brush: csharp&quot;&gt;
var calculatorType = typeBuilder.CreateType();

var obj = Activator.CreateInstance(calculatorType);

var addMethod = calculatorType.GetMethod(&amp;quot;Add&amp;quot;, new[] {typeof (int), typeof (int)});
Console.WriteLine(&amp;quot;1 + 2 = {0}&amp;quot;, addMethod.Invoke(obj, new object[] {1, 2}));

var subtractMethod = calculatorType.GetMethod(&amp;quot;Subtract&amp;quot;, new[] { typeof(int), typeof(int) });
Console.WriteLine(&amp;quot;10 + 7 = {0}&amp;quot;, subtractMethod.Invoke(obj, new object[] { 10, 7 }));
&lt;/pre&gt;&lt;/code&gt;

I simply call &lt;strong&gt;typeBuilder.CreateType()&lt;/strong&gt; once everything is defined to create the type and begin to use it.

However, the remaining code is very ugly simply because at the time the application is compiled the type has not been created so the compiler does not know what methods this type will have; this leaves us using reflection to call all of the methods.

What would be nicer is if we could have a known interface and automatically generate the implementation using &lt;strong&gt;Reflection.Emit&lt;/strong&gt; and that will be the topic of my next post.

&lt;h4&gt;Finally...&lt;/h4&gt;
If you take anything away from this blog post is that you should not be scared of &lt;strong&gt;Reflection.Emit&lt;/strong&gt;, by creating a real class that is close to the implementation that you want you can identify what operations you need to do the job.

Over the coming blog posts things will get more complicated and more useful, hopefully, as I move towards a real-world use of runtime-class generation.
</content>
    </entry>
  
</feed>